{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyunini/DeepLearning_From_Scratch/blob/main/%EB%B0%91%EC%8B%9C%EB%94%A5ch07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWlk6aeL3_d2"
      },
      "source": [
        "# CNN (합성곱 신경망)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiDf0fK54D-Z"
      },
      "source": [
        "### 1. 전체 구조: 원래는 Affine->Relu였다면 지금은 Conv->Relu->Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK3eLCpfBRHT"
      },
      "source": [
        "### 2. 합성곱/풀링 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-e4MmxUBojA"
      },
      "source": [
        "- 4차원 배열"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NibtMzSL_Ptr",
        "outputId": "f3dbaa28-759c-4681-fbcd-1796a451ba05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 1, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "x = np.random.rand(10, 1, 28, 28)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayW87Y7dBdWV",
        "outputId": "4cc12d76-9486-4470-99f8-29567927ee59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 28, 28)\n",
            "(1, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(x[0].shape)\n",
        "print(x[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8aG-UzsBnHJ",
        "outputId": "f5064626-1f3d-461a-b8f3-b5ac7079ac95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(x[0, 0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JVazRRhDVNN"
      },
      "source": [
        "- 합성곱 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNsqAuKkGlPd"
      },
      "source": [
        "- image to column: 추후 구현 살펴보기\n",
        "https://velog.io/@jadenkim5179/%EB%B0%91%EB%B0%94%EB%8B%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-im2col-%EA%B5%AC%ED%98%84-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0\n",
        "\n",
        "대충은 이해되는데 완벽하게는 안됨.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rwDyogvLBsmx"
      },
      "outputs": [],
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape # 입력 데이터의 형상\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1 # 합성곱 계산 이후 결과\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    # numpy.pad(array, pad_width, mode='constant', **kwargs)\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') #  pad_width는 각 축에 패딩 적용 (위쪽행, 아래행) (왼쪽 열, 오른쪽 열)\n",
        "    '''\n",
        "    col은 특정 데이터(N)에 대해서,\n",
        "    필터의 특정 채널(C)의 특정 원소(filter_h, filter_w)가\n",
        "    곱해지는 값들의 모음(out_h, out_w)를 나타낸다.\n",
        "\n",
        "    예를 들어, col[1, 2, 3, 4, 5, 6]은 0번째 데이터에 대해서,\n",
        "    필터의 2번째 채널의 [3, 4]위치의 원소가\n",
        "    곱해지는 값들의 모음 중 [5, 6]위치에 있는 원소를 나타낸다\n",
        "    '''\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w)) # input data를 리턴값인 col의 2차원으로 나타내기 위해서 선언\n",
        "\n",
        "    for y in range(filter_h): # 이중 for문으로 (y, x)를 통해 (filter_h, filter_w)를 순회\n",
        "        y_max = y + stride*out_h  # (y, x)에서 시작하여 stride를 통해 도달할 수 있는 끝점\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride] # [y : y_max : stride, x : x_max : stride]는 (y, x)에서 시작하여 y방향 또는 x방향으로 각각 stride씩 증가하며 순회\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1) # 축 변경\n",
        "    return col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez1WqLBaDqia",
        "outputId": "837ecadd-c239-44ae-9cb9-49fd7249b00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n"
          ]
        }
      ],
      "source": [
        "x1 = np.random.rand(1, 3, 7, 7) # 데이터수, 채널 수, 높이, 너비\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape) # 채널 3 필터 크기 5, 5 이므로 3*5*5 = 75임"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDa62wwZEPX-",
        "outputId": "d51a288b-989e-4a75-d109-eb7df31139ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ],
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7) # 데이터수, 채널 수, 높이, 너비\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape) # 데이터 수가 10배 늘어났으므로 90이 됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wOl7zQAVK6oy"
      },
      "outputs": [],
      "source": [
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col : 2차원 배열(입력 데이터)\n",
        "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img : 변환된 이미지들\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 합성곱 계층 구현"
      ],
      "metadata": {
        "id": "W-uT3KQU8sB5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIpz9BFXG8Hf"
      },
      "source": [
        "- 합성곱 계층의 forward 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "YOlnvXr8EWKr"
      },
      "outputs": [],
      "source": [
        "class Convolution:\n",
        "  def __init__(self, W, b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape # 커널\n",
        "    N, C, H, W = x.shape # 입력 데이터\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad) # 입력 데이터를 im2col을 이용해서 전개\n",
        "    col_W = self.W.reshape(FN, -1).T # 필터 전개 FN은 개수를 의미하므로 (개수 * 일차원) 배열을 Transpose함\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # transpose는 축의 순서를 바꿔줌 0,1,2,3 순서대로 N, out_h, out_w, -1인데 이를 0,3,1,2순서로 바꿈\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전체 구현"
      ],
      "metadata": {
        "id": "PJlCKN__B3R2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "MPTm07UvIbo3"
      },
      "outputs": [],
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout): # col2im 사용\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 풀링계층 구현"
      ],
      "metadata": {
        "id": "AtsiDymv8ukU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 풀링 계층의 forward 구현"
      ],
      "metadata": {
        "id": "pxc8TDQgBvU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        # (1)\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) # 입력 데이터를 전개\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        # (2)\n",
        "        out = np.max(col, axis=1) # 행별 최댓값을 수한다\n",
        "\n",
        "        # (3)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) # 적절한 모양으로 변환\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9CyHv_A1BdeP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 전체 구현"
      ],
      "metadata": {
        "id": "SM1am_45ByBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2rwcET9iKB5f"
      },
      "outputs": [],
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) # 입력 데이터를 전개\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1) # 행별 최댓값을 수한다\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) # 적절한 모양으로 변환\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 함수들 선언"
      ],
      "metadata": {
        "id": "dbZAgvdF8bsm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1PstyXerKyBO"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로 대책\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hhR3jJG3LCgK"
      },
      "outputs": [],
      "source": [
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 클래스 선언"
      ],
      "metadata": {
        "id": "FzHJIBy4CBzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "21hHuNWJKamR"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN 구현하기\n",
        "\n",
        " - p.250"
      ],
      "metadata": {
        "id": "ETK5VpDh8mk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ldr2EXoXKF1L"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    ex)  network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, # 합성곱 계층의 하이퍼파라미터, 딕셔너리 형태 -> filter_num: FN, C는 input_dim[0], filter_size: (FH, FW)\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num'] # 초기화 인수로 주어진 합성곱 계층의 하이퍼파라미터를 딕셔너리에서 꺼냄\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1] # 첫 번째 차원\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1 # 24 => (24*24)\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2)) # 30*12*12 => 2*2 max pooling의 결과 크기\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size) # 4차원으로 1번째 층의 합성곱 계층의 가중치와 편향 초기화\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2) # 2*2 pooling\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"/content/drive/MyDrive/Colab Notebooks/2023_AI/params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습을 위한 필요 클래스들 정의"
      ],
      "metadata": {
        "id": "xP8PCtmoR7Y2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SaCC3xPPOkBJ"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01},\n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "\n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "\n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "\n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "\n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "\n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "\n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "afXqZ1P-O9F1",
        "outputId": "26f5b80e-7041-4584-b429-5d453c362249"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noptimizer = SGD()\\noptimizer.update(params, grads) 이런식으로 수행\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr = 0.01):\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]\n",
        "\n",
        "'''\n",
        "optimizer = SGD()\n",
        "optimizer.update(params, grads) 이런식으로 수행\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eCwNk7LHPC9X"
      },
      "outputs": [],
      "source": [
        "class Momentum:\n",
        "  def __init__(self, lr=0.01, momentum=0.9):\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)\n",
        "\n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "      params[key] += self.v[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "COHJBKSoPVXO"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val) # 초기화\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key] # 기울기 제곱, h라는 새로운 변수\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 작은 값 더해서 0으로 나누는 것 방지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "i8yODuX2PYi1"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "\n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kgCpNQ6iPJ8h"
      },
      "outputs": [],
      "source": [
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "zAYOwqHbPQWY"
      },
      "outputs": [],
      "source": [
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleConvNet으로 MNIST데이터셋\n",
        "\n",
        "- https://velog.io/@clayryu328/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D-19-%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EA%B5%AC%ED%98%84"
      ],
      "metadata": {
        "id": "u44F5JQvCj31"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwJ7jwJEvhXm",
        "outputId": "3eb201b4-8196-4639-ed1b-eae34b595729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "I3rDmkd0v4I3"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "from mnist import load_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UvWwr_B9Oap-",
        "outputId": "9d7e889d-daec-481d-851f-81dab7512d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.001904467042558659\n",
            "train loss:0.001486169393867818\n",
            "train loss:0.0022431176785195988\n",
            "train loss:0.004613359415028537\n",
            "train loss:0.0013301904339572631\n",
            "train loss:0.0013812237302597858\n",
            "train loss:0.004480443465594299\n",
            "train loss:0.0018665292851041669\n",
            "train loss:0.0016352464842428682\n",
            "train loss:0.005817700807988183\n",
            "train loss:0.000585330622620828\n",
            "train loss:0.0056287721798047394\n",
            "train loss:0.006384532660011463\n",
            "train loss:0.0072492796416735415\n",
            "train loss:0.00038176435616377906\n",
            "train loss:0.00891719315873598\n",
            "train loss:0.0021930588760132883\n",
            "train loss:0.013097351205076187\n",
            "train loss:0.004133456110268179\n",
            "train loss:0.0009864255006363316\n",
            "train loss:0.002640003805583274\n",
            "train loss:0.007322202804999244\n",
            "train loss:0.008330101080465788\n",
            "train loss:0.0002637438983771772\n",
            "train loss:0.016476568102250712\n",
            "train loss:0.0021877620803497267\n",
            "train loss:0.006969950399441952\n",
            "train loss:0.0019694015453759046\n",
            "train loss:0.03660821366303522\n",
            "train loss:0.0016254551079005683\n",
            "train loss:0.0003062500526665034\n",
            "train loss:0.026371614461492193\n",
            "train loss:0.004731888026952972\n",
            "train loss:0.0014416538071004318\n",
            "train loss:0.0021195849112256877\n",
            "train loss:0.0008819287439442499\n",
            "train loss:0.001826655611525554\n",
            "train loss:0.014675702676817481\n",
            "train loss:0.012856965768187132\n",
            "train loss:0.030620684354252452\n",
            "train loss:0.002536690587233376\n",
            "train loss:0.013064679194362066\n",
            "train loss:0.0006596881319453204\n",
            "train loss:0.0066411111109006635\n",
            "train loss:0.04063407611242946\n",
            "train loss:0.041009830474049276\n",
            "train loss:0.003057814190136333\n",
            "train loss:0.000662480560978459\n",
            "train loss:0.007643450835056611\n",
            "train loss:0.005408960703398868\n",
            "train loss:0.0038892052704060797\n",
            "train loss:0.0007769147144159182\n",
            "train loss:0.009479279868717933\n",
            "train loss:0.007507969031222055\n",
            "train loss:0.0024480337439080567\n",
            "train loss:0.03758237056375104\n",
            "train loss:0.012194644030851506\n",
            "train loss:0.003997986284639086\n",
            "train loss:0.0032575689966856058\n",
            "train loss:0.004629458337208226\n",
            "train loss:0.0014315644536642563\n",
            "train loss:0.002711878781017811\n",
            "train loss:0.0054505397922360276\n",
            "train loss:0.0024184916625094267\n",
            "train loss:0.0046381041297644705\n",
            "train loss:0.00878086185292224\n",
            "train loss:0.012186856734493913\n",
            "train loss:0.010684655888544187\n",
            "train loss:0.006363516024426586\n",
            "train loss:0.01700029106302305\n",
            "train loss:0.0031999828076938967\n",
            "train loss:0.007198651469517632\n",
            "train loss:0.0011748659454657447\n",
            "train loss:0.005265983057466248\n",
            "train loss:0.006166505792846317\n",
            "train loss:0.006093642791903189\n",
            "train loss:0.007054614204262164\n",
            "train loss:0.00015186880488099786\n",
            "train loss:0.0070286285306366955\n",
            "train loss:0.0006111138687647405\n",
            "train loss:0.003989831010945608\n",
            "train loss:0.0014094349265476808\n",
            "train loss:0.004622029043688034\n",
            "train loss:0.0014525818529194153\n",
            "train loss:0.00712929224925168\n",
            "train loss:0.0045555253648019885\n",
            "train loss:0.003157787558932757\n",
            "train loss:0.004425074348321133\n",
            "train loss:0.001616210424627109\n",
            "train loss:0.001235923993800711\n",
            "train loss:0.002027147613289204\n",
            "train loss:0.008549400169785833\n",
            "train loss:0.006240392151129012\n",
            "train loss:0.004007354325284287\n",
            "train loss:0.004068559336786248\n",
            "train loss:0.007086368125794198\n",
            "train loss:0.001607549220423907\n",
            "train loss:0.0005203495944254369\n",
            "train loss:0.0007431713757210036\n",
            "train loss:0.003458325959385411\n",
            "train loss:0.0012574917114380052\n",
            "train loss:0.0008336533017024589\n",
            "train loss:0.01194354366311518\n",
            "train loss:0.0005081243102313897\n",
            "train loss:0.011387033059331041\n",
            "train loss:0.002613711393379563\n",
            "train loss:0.0008086122565199122\n",
            "train loss:0.0024580995788928478\n",
            "train loss:0.0031616816318654245\n",
            "train loss:0.0021022123747140922\n",
            "train loss:0.0005349598306059543\n",
            "train loss:0.0004704146875693424\n",
            "train loss:0.0034714843011882644\n",
            "train loss:0.004389703574649932\n",
            "train loss:0.00018540707212291925\n",
            "train loss:0.00400886004151601\n",
            "train loss:0.0005171661417661111\n",
            "train loss:0.0010677851026599998\n",
            "train loss:0.002769862975612122\n",
            "train loss:0.00391043090663864\n",
            "train loss:0.002086664399636631\n",
            "train loss:0.002342063256851785\n",
            "train loss:0.0013428869528317518\n",
            "train loss:0.0007497166555392755\n",
            "train loss:0.0014378967594166612\n",
            "train loss:0.0022789267354165594\n",
            "train loss:0.0005227014397835122\n",
            "train loss:0.007384987205292593\n",
            "train loss:0.000122931984903768\n",
            "train loss:0.0014704050293126447\n",
            "train loss:0.0013563611401309205\n",
            "train loss:0.005425035244408168\n",
            "train loss:0.001970729775117969\n",
            "train loss:0.004704025906518927\n",
            "train loss:0.08992494298679161\n",
            "train loss:0.0005015741798776831\n",
            "train loss:0.0012854854411930678\n",
            "train loss:0.00869544139700355\n",
            "train loss:0.0029538944756866887\n",
            "train loss:0.006793166740490987\n",
            "train loss:0.006929078163197085\n",
            "train loss:0.004586031523807432\n",
            "train loss:0.00027567154940987817\n",
            "train loss:0.002666201763767767\n",
            "train loss:0.0012787355988016045\n",
            "train loss:0.016375187207603784\n",
            "train loss:0.0001525544129388559\n",
            "train loss:0.004731159246893492\n",
            "train loss:0.0032081071872248207\n",
            "train loss:0.027494031005718324\n",
            "train loss:0.0033260079407336084\n",
            "train loss:0.0027744786768917474\n",
            "train loss:0.0022893453286089026\n",
            "train loss:0.005084238384318474\n",
            "train loss:0.006826035520475273\n",
            "train loss:0.002297037371381012\n",
            "train loss:0.007068708350671882\n",
            "train loss:0.009983503301667312\n",
            "train loss:0.01615324402893053\n",
            "train loss:0.0011782607984184615\n",
            "train loss:0.008296616488521468\n",
            "train loss:0.0012120335495149513\n",
            "train loss:0.005935120769354555\n",
            "train loss:0.0011265740810289802\n",
            "train loss:0.0032744318903850777\n",
            "train loss:0.015511360170384897\n",
            "train loss:0.005451964910679258\n",
            "train loss:0.003673090888435085\n",
            "train loss:0.0013718332617225903\n",
            "train loss:0.0009928227883559452\n",
            "train loss:0.003760710722784043\n",
            "train loss:0.0012578039286258239\n",
            "train loss:0.0032632205955111554\n",
            "train loss:0.0013958681950412236\n",
            "train loss:0.004114384839593147\n",
            "train loss:0.008011812260401947\n",
            "train loss:0.004371162799758671\n",
            "train loss:0.007111588363149035\n",
            "train loss:0.022311589386261654\n",
            "train loss:0.011071740430187342\n",
            "train loss:0.010445169793997322\n",
            "train loss:0.0011411510267780476\n",
            "train loss:0.001419979499095792\n",
            "train loss:0.034282408837317535\n",
            "train loss:0.008078598185066621\n",
            "train loss:0.0019964618291977874\n",
            "train loss:0.0018017059300238112\n",
            "train loss:0.01339631433656646\n",
            "train loss:0.0009584152568388683\n",
            "train loss:0.00161226664855657\n",
            "=== epoch:13, train acc:0.995, test acc:0.986 ===\n",
            "train loss:0.0016291396495466332\n",
            "train loss:0.007747972253836171\n",
            "train loss:0.0019616260889692336\n",
            "train loss:0.035372763309624806\n",
            "train loss:0.009800408189057443\n",
            "train loss:0.0011986638580261534\n",
            "train loss:0.0019872393806589657\n",
            "train loss:0.001711405070986059\n",
            "train loss:0.0021371122041872756\n",
            "train loss:0.009947114620389535\n",
            "train loss:0.0071786076652688305\n",
            "train loss:0.0025164665869065626\n",
            "train loss:0.002510885102893376\n",
            "train loss:0.00019255547080032417\n",
            "train loss:0.0006429671170421568\n",
            "train loss:0.011448204010934573\n",
            "train loss:0.01317071304894941\n",
            "train loss:0.013093896729130859\n",
            "train loss:0.00690585697873656\n",
            "train loss:0.00893482822687003\n",
            "train loss:0.0034220889364537203\n",
            "train loss:0.031154783738338358\n",
            "train loss:0.002290069488077769\n",
            "train loss:0.003869292917938757\n",
            "train loss:0.0014754193113574294\n",
            "train loss:0.002080816622971361\n",
            "train loss:0.001870640301418269\n",
            "train loss:0.0014958703318015184\n",
            "train loss:0.010556472547000897\n",
            "train loss:0.012891646939554463\n",
            "train loss:0.0033334646047050343\n",
            "train loss:0.0011016773986573087\n",
            "train loss:0.005706453997368046\n",
            "train loss:0.005934479327334434\n",
            "train loss:0.005334148985341807\n",
            "train loss:0.0008347999936085654\n",
            "train loss:0.006043617372010106\n",
            "train loss:0.0034910291012827947\n",
            "train loss:0.0040897540620678866\n",
            "train loss:0.009413763181487312\n",
            "train loss:0.020281183292437242\n",
            "train loss:0.000673420726850769\n",
            "train loss:0.0011706453319064974\n",
            "train loss:0.0035819686139183403\n",
            "train loss:0.008267387664202097\n",
            "train loss:0.00017944467116899778\n",
            "train loss:0.0030761489310425394\n",
            "train loss:0.0012669760891084604\n",
            "train loss:0.0017015566068689506\n",
            "train loss:0.00048517027964515354\n",
            "train loss:0.001298244100811803\n",
            "train loss:0.003627602858420422\n",
            "train loss:0.0006279747473861483\n",
            "train loss:0.0033989443298408526\n",
            "train loss:0.001760568378274366\n",
            "train loss:0.005650469394936051\n",
            "train loss:0.00019840429316031434\n",
            "train loss:0.002297619903959371\n",
            "train loss:0.0002935389164868955\n",
            "train loss:0.0010967348463113388\n",
            "train loss:0.0007264656971217759\n",
            "train loss:0.0034010313643707346\n",
            "train loss:0.002168214651611358\n",
            "train loss:0.0003925644407625544\n",
            "train loss:0.002034047826512257\n",
            "train loss:0.02322642212735062\n",
            "train loss:0.0293325340044481\n",
            "train loss:0.024797456620877782\n",
            "train loss:0.0026933653507032473\n",
            "train loss:0.0016326130672340267\n",
            "train loss:0.0010391172637522165\n",
            "train loss:0.0008126294637756322\n",
            "train loss:0.015215676244801057\n",
            "train loss:0.00446302395140355\n",
            "train loss:0.0005913198766377\n",
            "train loss:0.022412824061552993\n",
            "train loss:0.0013987956526605241\n",
            "train loss:0.0066510196465827875\n",
            "train loss:0.0019739085468189524\n",
            "train loss:0.0006974893107421231\n",
            "train loss:0.0015219086359193335\n",
            "train loss:0.002576222790591074\n",
            "train loss:0.0016436566865355466\n",
            "train loss:0.0012276748819276776\n",
            "train loss:0.0019205651327570883\n",
            "train loss:0.000582464664632612\n",
            "train loss:0.0003276271000130683\n",
            "train loss:0.000728763576432531\n",
            "train loss:0.008847014173241624\n",
            "train loss:0.022651940017875436\n",
            "train loss:0.003864090829698886\n",
            "train loss:0.014899916271421487\n",
            "train loss:0.0031928881340134436\n",
            "train loss:0.0013314309871266675\n",
            "train loss:0.003034120157029449\n",
            "train loss:0.0010706032242715142\n",
            "train loss:0.0013518276543171561\n",
            "train loss:0.000307022031709687\n",
            "train loss:0.004417360000444474\n",
            "train loss:0.0005669488686065887\n",
            "train loss:0.007053860047546554\n",
            "train loss:0.002160996202444458\n",
            "train loss:0.018294259681712607\n",
            "train loss:0.0008123848389333799\n",
            "train loss:0.00127803041815795\n",
            "train loss:0.0019889355479200304\n",
            "train loss:0.0007485347742222247\n",
            "train loss:0.0010745940370837299\n",
            "train loss:0.004382476821919057\n",
            "train loss:0.004364532025305424\n",
            "train loss:0.007753543691317596\n",
            "train loss:0.002554158556509637\n",
            "train loss:0.0049613477833998195\n",
            "train loss:0.0024078081052672923\n",
            "train loss:0.0018596801362858345\n",
            "train loss:0.0018967641855645648\n",
            "train loss:0.001833992506501409\n",
            "train loss:0.0032077490642705626\n",
            "train loss:0.0019954853514004105\n",
            "train loss:0.002729393202838259\n",
            "train loss:0.013757616043001391\n",
            "train loss:0.004073733065471478\n",
            "train loss:0.0004506869715054731\n",
            "train loss:0.003980148930557284\n",
            "train loss:0.014622680260422651\n",
            "train loss:0.0023400137587721945\n",
            "train loss:0.004676485118160265\n",
            "train loss:0.002162374133045499\n",
            "train loss:0.000535628700254926\n",
            "train loss:0.0012679998467549716\n",
            "train loss:0.004766786719354866\n",
            "train loss:0.005214240227380211\n",
            "train loss:0.003942833442100287\n",
            "train loss:0.026677628218326737\n",
            "train loss:0.005069381153246478\n",
            "train loss:0.006502781172883047\n",
            "train loss:0.0007255796982317175\n",
            "train loss:0.002591910657906786\n",
            "train loss:0.001486328220768006\n",
            "train loss:0.002728587985079091\n",
            "train loss:0.00040996205437356597\n",
            "train loss:0.0037984630788970588\n",
            "train loss:0.002117910187650341\n",
            "train loss:0.0019280568726616906\n",
            "train loss:0.001668930500357907\n",
            "train loss:0.0003083579410121294\n",
            "train loss:0.002679557932726963\n",
            "train loss:0.0016440269783042577\n",
            "train loss:0.0014074043210824802\n",
            "train loss:0.014064672995005913\n",
            "train loss:0.0006963234575249209\n",
            "train loss:0.0019813848358355094\n",
            "train loss:0.0030082938192773153\n",
            "train loss:0.0013351424951498403\n",
            "train loss:0.004582776039736511\n",
            "train loss:0.011662409210685681\n",
            "train loss:0.0023767277246609974\n",
            "train loss:0.003044989887491963\n",
            "train loss:0.0020233426421982586\n",
            "train loss:7.08716868427179e-05\n",
            "train loss:0.0020584793665053376\n",
            "train loss:0.0003042939591617051\n",
            "train loss:0.0013865699513623212\n",
            "train loss:0.012435465400281287\n",
            "train loss:0.0008191063503723884\n",
            "train loss:0.0007134986465697392\n",
            "train loss:0.006128003265693826\n",
            "train loss:0.0002771525925278837\n",
            "train loss:0.0074499601294871175\n",
            "train loss:0.0020772049290889395\n",
            "train loss:0.0004029142851335886\n",
            "train loss:0.00251518574531601\n",
            "train loss:0.0007694318386603509\n",
            "train loss:0.0289078488125601\n",
            "train loss:0.0021323910738801828\n",
            "train loss:0.0019574594359978124\n",
            "train loss:0.009244608520382227\n",
            "train loss:0.0006999374112185004\n",
            "train loss:0.004010775189009761\n",
            "train loss:0.0011926321148193048\n",
            "train loss:0.0317777433334915\n",
            "train loss:0.004874749871995511\n",
            "train loss:0.0004649543535679919\n",
            "train loss:0.002301437680021206\n",
            "train loss:0.004100017415022166\n",
            "train loss:0.003004927535780963\n",
            "train loss:0.019894077924592232\n",
            "train loss:0.002667345238162763\n",
            "train loss:0.0188939924526203\n",
            "train loss:0.003916999708539257\n",
            "train loss:0.003126451883337278\n",
            "train loss:0.00503515277227242\n",
            "train loss:0.0019170250024272272\n",
            "train loss:0.009189635546929933\n",
            "train loss:0.012959448176897441\n",
            "train loss:0.005607795304711946\n",
            "train loss:0.015821844460145847\n",
            "train loss:0.001923082963067657\n",
            "train loss:0.004784978905993512\n",
            "train loss:0.0008489315833136148\n",
            "train loss:0.0012091438184536215\n",
            "train loss:0.003958219084299969\n",
            "train loss:0.005542928533226081\n",
            "train loss:0.02714228442319479\n",
            "train loss:0.0006195205214440532\n",
            "train loss:0.005002704805174002\n",
            "train loss:0.002180275418320357\n",
            "train loss:0.005922305829585906\n",
            "train loss:0.005732502232386963\n",
            "train loss:0.004423831376560937\n",
            "train loss:0.0050083798799695324\n",
            "train loss:0.000167593806790911\n",
            "train loss:0.0007050073389694897\n",
            "train loss:0.004205788879098053\n",
            "train loss:0.0007240504110439808\n",
            "train loss:0.0009583970827999875\n",
            "train loss:0.004991121452623914\n",
            "train loss:0.014324490842835031\n",
            "train loss:0.0028055640410210763\n",
            "train loss:0.004507986009393488\n",
            "train loss:0.0008700740226417618\n",
            "train loss:0.010920714205204613\n",
            "train loss:0.01769822962573743\n",
            "train loss:0.0007178648673152252\n",
            "train loss:0.008710223048495187\n",
            "train loss:0.002353850759154852\n",
            "train loss:0.00023578449382104908\n",
            "train loss:0.03864925707181146\n",
            "train loss:0.010387412533167755\n",
            "train loss:0.003369806867584055\n",
            "train loss:0.005476133645692181\n",
            "train loss:0.004223712351471373\n",
            "train loss:0.0004769714429843706\n",
            "train loss:0.0018201835022083643\n",
            "train loss:0.005641866880551737\n",
            "train loss:0.009164496351801527\n",
            "train loss:0.005956409797217946\n",
            "train loss:0.003285144649581731\n",
            "train loss:0.009446712728047908\n",
            "train loss:0.002714832904740672\n",
            "train loss:0.006526863265924206\n",
            "train loss:0.0034385239657983603\n",
            "train loss:0.0012599812338388982\n",
            "train loss:0.0021619194252762448\n",
            "train loss:0.0021505835819990633\n",
            "train loss:0.003353100509477496\n",
            "train loss:0.018764090095032458\n",
            "train loss:0.0007243846930178123\n",
            "train loss:0.0021753034422200613\n",
            "train loss:0.0014453926326398994\n",
            "train loss:0.007122462776964083\n",
            "train loss:0.0033686992063270906\n",
            "train loss:0.004228906257391921\n",
            "train loss:0.011092326416355627\n",
            "train loss:0.003230222686710413\n",
            "train loss:0.0026361981432410637\n",
            "train loss:0.004475442340041632\n",
            "train loss:0.006905356871481522\n",
            "train loss:0.009232505432276727\n",
            "train loss:0.003286320110785777\n",
            "train loss:0.0006972878065239726\n",
            "train loss:0.006782049241482011\n",
            "train loss:0.00168845621395196\n",
            "train loss:0.011806461969532176\n",
            "train loss:0.003037139490061837\n",
            "train loss:0.0026630987249064825\n",
            "train loss:0.003111116056592763\n",
            "train loss:0.008534431981239494\n",
            "train loss:0.01163687463394648\n",
            "train loss:0.016870333643353396\n",
            "train loss:0.007671022851687608\n",
            "train loss:0.009786351844362705\n",
            "train loss:0.0014730923876462366\n",
            "train loss:0.0015963998861195459\n",
            "train loss:0.001636606847714974\n",
            "train loss:0.010107925676645188\n",
            "train loss:0.04756372168478466\n",
            "train loss:0.007303332706747211\n",
            "train loss:0.001029426884555764\n",
            "train loss:0.008871496409751605\n",
            "train loss:0.001414927343092875\n",
            "train loss:0.0034284000771540963\n",
            "train loss:0.0025207393769368985\n",
            "train loss:0.001115880639189098\n",
            "train loss:0.001796409395877286\n",
            "train loss:0.005452246576162119\n",
            "train loss:0.009115878106502074\n",
            "train loss:0.007867718919794143\n",
            "train loss:0.00409687398290538\n",
            "train loss:0.010893462631778374\n",
            "train loss:0.0076752615499439325\n",
            "train loss:0.01309066232594402\n",
            "train loss:0.0026178659544073392\n",
            "train loss:0.0038306018258564005\n",
            "train loss:0.0024138308579568614\n",
            "train loss:0.0020049714733813367\n",
            "train loss:0.0075229642420215894\n",
            "train loss:0.0005940944312203156\n",
            "train loss:0.007717901971530955\n",
            "train loss:0.010075725215362699\n",
            "train loss:0.00047760626042952744\n",
            "train loss:0.0017666594976216065\n",
            "train loss:0.00127810611865488\n",
            "train loss:0.0007402396120340595\n",
            "train loss:0.0008846296100170408\n",
            "train loss:0.00798595337461244\n",
            "train loss:0.004529791982856386\n",
            "train loss:0.00023971049607571362\n",
            "train loss:0.0031269900164190745\n",
            "train loss:0.014955997732749884\n",
            "train loss:0.00046399223166967394\n",
            "train loss:0.00028781440880210914\n",
            "train loss:0.0033467926354602594\n",
            "train loss:0.004700458077289366\n",
            "train loss:0.0019757313295876845\n",
            "train loss:0.001527391747104496\n",
            "train loss:0.001879217367087253\n",
            "train loss:0.0014807506437409794\n",
            "train loss:0.007043397907418939\n",
            "train loss:0.0018699936348791036\n",
            "train loss:0.0005313294308587394\n",
            "train loss:0.0013051776218141616\n",
            "train loss:0.004214700319644411\n",
            "train loss:0.00012480319913456682\n",
            "train loss:0.00023480289039977026\n",
            "train loss:0.0020102790869906875\n",
            "train loss:0.00872829473152155\n",
            "train loss:0.012563386540569498\n",
            "train loss:0.0013504513187319765\n",
            "train loss:0.0013548212407995868\n",
            "train loss:0.007588054890746059\n",
            "train loss:0.0019778302174849106\n",
            "train loss:0.005301928029064954\n",
            "train loss:0.0018673383274936937\n",
            "train loss:0.011256862248750799\n",
            "train loss:0.007902191846403192\n",
            "train loss:0.0013188482095171795\n",
            "train loss:0.002626731451312483\n",
            "train loss:0.021028024709417056\n",
            "train loss:0.0025900958544826437\n",
            "train loss:0.03445660306047632\n",
            "train loss:0.0013684454907842425\n",
            "train loss:0.0014767019951852777\n",
            "train loss:0.004084302198527663\n",
            "train loss:0.0018057663840363733\n",
            "train loss:0.0032491532746441263\n",
            "train loss:0.005383057735049168\n",
            "train loss:0.002929680102104589\n",
            "train loss:0.001555771052528054\n",
            "train loss:0.003265922562478769\n",
            "train loss:0.019905117998045275\n",
            "train loss:0.0007626588542660004\n",
            "train loss:0.003176014657102747\n",
            "train loss:0.007988721935699184\n",
            "train loss:0.0012556528998922112\n",
            "train loss:0.0007796982667912328\n",
            "train loss:0.0007428324552294878\n",
            "train loss:0.010204649968857693\n",
            "train loss:0.006765490978960234\n",
            "train loss:0.002010112892195658\n",
            "train loss:0.010551075680771514\n",
            "train loss:0.003458051464603097\n",
            "train loss:0.013776467638216777\n",
            "train loss:0.00034190540677141385\n",
            "train loss:0.003936704727847868\n",
            "train loss:0.007404603302814262\n",
            "train loss:0.0005307252799962636\n",
            "train loss:0.006429571972069341\n",
            "train loss:0.008312780285937562\n",
            "train loss:0.001554611274176724\n",
            "train loss:0.0011485029556053293\n",
            "train loss:0.0098178765712191\n",
            "train loss:0.0015887106065582614\n",
            "train loss:0.005302510073781268\n",
            "train loss:0.003619104781543167\n",
            "train loss:0.017576446491432716\n",
            "train loss:0.001771246685498777\n",
            "train loss:0.009432881386502154\n",
            "train loss:0.0006626208773919508\n",
            "train loss:0.006682005138363304\n",
            "train loss:0.00043295380690902487\n",
            "train loss:0.0012069115377041973\n",
            "train loss:0.0014401516320172545\n",
            "train loss:0.00414351106675127\n",
            "train loss:0.005498909421674203\n",
            "train loss:0.001444999183065226\n",
            "train loss:0.003978624387265535\n",
            "train loss:0.001504484557965664\n",
            "train loss:0.0005114399998237652\n",
            "train loss:0.004699605266526461\n",
            "train loss:0.0007421380725332283\n",
            "train loss:0.002116352577928855\n",
            "train loss:0.0030676026030776495\n",
            "train loss:0.05706046249543556\n",
            "train loss:0.0005640247217098439\n",
            "train loss:0.001505488189662009\n",
            "train loss:0.0002921875094151699\n",
            "train loss:0.0010948434391642847\n",
            "train loss:0.005062410900947537\n",
            "train loss:0.0007731150993159097\n",
            "train loss:0.0012136994994547243\n",
            "train loss:0.0033024576531043267\n",
            "train loss:0.0029564249824008465\n",
            "train loss:0.0003059952927574313\n",
            "train loss:0.010729658130503538\n",
            "train loss:0.0007481906195656339\n",
            "train loss:0.0007659934846037134\n",
            "train loss:0.008764405698116894\n",
            "train loss:0.002384527678243736\n",
            "train loss:0.0008043887798143259\n",
            "train loss:0.0003988630979769868\n",
            "train loss:0.008070103590647238\n",
            "train loss:0.019586283590848737\n",
            "train loss:0.004922218621756832\n",
            "train loss:0.0006710643833152561\n",
            "train loss:0.0037764251593667623\n",
            "train loss:0.0020106304219953386\n",
            "train loss:0.0012182322812286863\n",
            "train loss:0.0011000858090627\n",
            "train loss:0.027739131424489645\n",
            "train loss:0.000371719095021317\n",
            "train loss:0.004119415571646327\n",
            "train loss:0.0006833051903469257\n",
            "train loss:0.004821657969536235\n",
            "train loss:0.000843152644147836\n",
            "train loss:0.004033412805952321\n",
            "train loss:0.013276821535645468\n",
            "train loss:0.002967600660827859\n",
            "train loss:0.0010217606202904497\n",
            "train loss:0.008385335002120882\n",
            "train loss:0.001160970395856442\n",
            "train loss:0.0026623693424879286\n",
            "train loss:0.0016564920069790266\n",
            "train loss:0.0042972433177740745\n",
            "train loss:0.002562334545152911\n",
            "train loss:0.0036424309355566915\n",
            "train loss:0.016434296335666967\n",
            "train loss:0.0005077210168207242\n",
            "train loss:0.0008552985123995219\n",
            "train loss:0.0003895603338376199\n",
            "train loss:0.010627869217585064\n",
            "train loss:0.005187834089245605\n",
            "train loss:0.008102146690524175\n",
            "train loss:0.0013653274096010787\n",
            "train loss:0.0026232143470555696\n",
            "train loss:0.008728736448368276\n",
            "train loss:0.007218418208776889\n",
            "train loss:0.003410539631511081\n",
            "train loss:0.004524327516394998\n",
            "train loss:0.0004920583386077343\n",
            "train loss:0.0043420856784658875\n",
            "train loss:0.006572166255557562\n",
            "train loss:0.015996533732784\n",
            "train loss:0.0013907158484595611\n",
            "train loss:0.007786189217694337\n",
            "train loss:0.01736490229631173\n",
            "train loss:0.012674851235840535\n",
            "train loss:0.0012537396813482487\n",
            "train loss:0.0032895612757451527\n",
            "train loss:0.002796764352048039\n",
            "train loss:0.000825127023072595\n",
            "train loss:0.009307723978979124\n",
            "train loss:0.016254414790254645\n",
            "train loss:0.008288600170668362\n",
            "train loss:0.0023602135911473614\n",
            "train loss:0.000990174705695683\n",
            "train loss:0.004674763703361984\n",
            "train loss:0.0015867254264982453\n",
            "train loss:0.002785324197408341\n",
            "train loss:0.005160882133397722\n",
            "train loss:0.00034860450172346965\n",
            "train loss:0.002475375921539984\n",
            "train loss:0.0021812219280974757\n",
            "train loss:0.03083313759150741\n",
            "train loss:0.0026491442895787347\n",
            "train loss:0.0047212439798785125\n",
            "train loss:0.0018979368927918145\n",
            "train loss:0.0008135405195036959\n",
            "train loss:0.006023852394252044\n",
            "train loss:0.00031897228899160495\n",
            "train loss:0.0016092008904431251\n",
            "train loss:0.0030643554683418782\n",
            "train loss:0.006950412685162808\n",
            "train loss:0.004061076871124292\n",
            "train loss:0.0007171264600013647\n",
            "train loss:0.0021805064377904294\n",
            "train loss:0.020065836837494907\n",
            "train loss:0.0023039796794335245\n",
            "train loss:0.007917130399444137\n",
            "train loss:0.0011462636297464095\n",
            "train loss:0.0012094681412199036\n",
            "train loss:0.00032311769772457423\n",
            "train loss:0.0020937014616164216\n",
            "train loss:0.000971386242410971\n",
            "train loss:0.006232083821220879\n",
            "train loss:0.0152205843174369\n",
            "train loss:0.0029689784277329044\n",
            "train loss:0.0012989292219700778\n",
            "train loss:0.0010644744857779378\n",
            "train loss:0.0017332329226600884\n",
            "train loss:0.0008602794446552635\n",
            "train loss:0.0038459578649268746\n",
            "train loss:0.0018920306763826875\n",
            "train loss:0.0016783400008652104\n",
            "train loss:0.006192842449115424\n",
            "train loss:0.007065865970613569\n",
            "train loss:0.0010042956712089579\n",
            "train loss:0.006007328012422208\n",
            "train loss:0.0025587755081015284\n",
            "train loss:0.006392089956023317\n",
            "train loss:0.0011405826481393991\n",
            "train loss:0.03168115843439622\n",
            "train loss:0.0008642473075344475\n",
            "train loss:0.00013469123303485803\n",
            "train loss:0.001368653049662818\n",
            "train loss:0.0042204007215228675\n",
            "train loss:0.005160494362681577\n",
            "train loss:0.004953611435161471\n",
            "train loss:0.0018405156241263524\n",
            "train loss:0.0010772712579091212\n",
            "train loss:0.005226830520165586\n",
            "train loss:0.0010956812358878488\n",
            "train loss:0.0008994193936299179\n",
            "train loss:0.04155239716990379\n",
            "train loss:0.0011550312350024734\n",
            "train loss:0.004038924148386696\n",
            "train loss:0.0009856738711088362\n",
            "train loss:0.0037455468862998766\n",
            "train loss:0.00839369331151072\n",
            "train loss:0.001693429135376854\n",
            "train loss:0.0027101649656309373\n",
            "train loss:0.0006942946233833226\n",
            "train loss:0.0025114780732425966\n",
            "train loss:0.002381257030496076\n",
            "train loss:0.007018995608219693\n",
            "train loss:0.0066296566853101865\n",
            "train loss:0.0021245835969576497\n",
            "train loss:0.006539407160814915\n",
            "train loss:0.0031414687883397512\n",
            "train loss:0.011285866110865033\n",
            "train loss:0.002329063386804851\n",
            "train loss:0.00566215098272775\n",
            "train loss:0.0036439620438098634\n",
            "train loss:0.0003813945491666418\n",
            "train loss:0.005540091429925616\n",
            "train loss:0.004096144946390885\n",
            "train loss:0.0008088137384155905\n",
            "train loss:0.005418809329732976\n",
            "train loss:0.00038163492398824334\n",
            "train loss:0.002502732740562914\n",
            "train loss:0.0034379742727277084\n",
            "train loss:0.006839871454922237\n",
            "train loss:0.00130620995616913\n",
            "train loss:0.001986405611215048\n",
            "train loss:0.0018307400324325887\n",
            "train loss:0.0007112108151642795\n",
            "train loss:0.0075124545924398975\n",
            "train loss:0.0018999384821471487\n",
            "train loss:0.006321903778030544\n",
            "train loss:0.0004197502749561441\n",
            "train loss:0.0008033301477872014\n",
            "train loss:0.024091673630634358\n",
            "train loss:0.00039797456080987797\n",
            "train loss:0.0016130058283295265\n",
            "train loss:0.0034053939833371266\n",
            "train loss:0.001493870214151074\n",
            "train loss:0.001180392056401278\n",
            "train loss:0.0001813154022766073\n",
            "train loss:0.003347973344745796\n",
            "train loss:0.012468802725571502\n",
            "train loss:0.0015196344578601365\n",
            "train loss:0.0006462894288508593\n",
            "train loss:0.0009041363951940579\n",
            "train loss:0.0008873341565956239\n",
            "train loss:0.0010514112578375832\n",
            "train loss:0.0012445591944403938\n",
            "train loss:0.0022537959933265827\n",
            "train loss:0.00043451300312232123\n",
            "train loss:0.0005886818439606751\n",
            "train loss:0.006330131880556672\n",
            "train loss:0.0027678166366567666\n",
            "train loss:0.002951697097413689\n",
            "train loss:0.004675791793292019\n",
            "train loss:0.0023684933892814566\n",
            "train loss:0.0005140666809725254\n",
            "train loss:0.004864421147989722\n",
            "train loss:0.0008914402790873437\n",
            "train loss:0.001665602969064399\n",
            "train loss:0.0006560053679435433\n",
            "train loss:0.0013188122999491763\n",
            "train loss:0.0022310865148771996\n",
            "train loss:0.0017270913241104278\n",
            "train loss:0.0010153561606656215\n",
            "train loss:0.0024629503403578706\n",
            "train loss:0.00042671308484083353\n",
            "train loss:0.0009496239025376995\n",
            "train loss:0.0007093335906208755\n",
            "train loss:0.001593738977957069\n",
            "train loss:0.0003828151342002089\n",
            "train loss:0.0002919678826624599\n",
            "=== epoch:14, train acc:0.999, test acc:0.987 ===\n",
            "train loss:0.0007403768808932314\n",
            "train loss:0.0007855897676432366\n",
            "train loss:0.0054613802468282645\n",
            "train loss:0.0032111014630826036\n",
            "train loss:0.006474416831286944\n",
            "train loss:0.0041160761925284626\n",
            "train loss:0.000769876460394662\n",
            "train loss:0.014230430015155776\n",
            "train loss:0.0002273397246745041\n",
            "train loss:0.0011126553666899396\n",
            "train loss:0.001415503724798874\n",
            "train loss:0.001056539707806787\n",
            "train loss:0.01844132339437483\n",
            "train loss:0.004782526185459112\n",
            "train loss:0.0020354328611058184\n",
            "train loss:0.00016921609377802818\n",
            "train loss:0.0013241768134570372\n",
            "train loss:0.0008863227969799996\n",
            "train loss:0.0004967543248675108\n",
            "train loss:0.00036410247848405993\n",
            "train loss:0.0012200469402092182\n",
            "train loss:0.01001135492670922\n",
            "train loss:0.0013601634070998195\n",
            "train loss:0.0017800089960521895\n",
            "train loss:0.010555759610075882\n",
            "train loss:0.001397330898339182\n",
            "train loss:0.002726171584112991\n",
            "train loss:0.011608206292870014\n",
            "train loss:0.0007295099319673682\n",
            "train loss:0.005514418948554536\n",
            "train loss:0.002101054939030334\n",
            "train loss:0.005403463333450626\n",
            "train loss:0.0013102195481433001\n",
            "train loss:0.006383267434346707\n",
            "train loss:0.0004257656224274672\n",
            "train loss:0.0015011244211844718\n",
            "train loss:0.008775510689822932\n",
            "train loss:0.009133885296257017\n",
            "train loss:0.0005978806256877337\n",
            "train loss:0.007339667364280373\n",
            "train loss:0.0020385169278238423\n",
            "train loss:0.0015335311037967347\n",
            "train loss:0.00023276257256433765\n",
            "train loss:0.001367419112663951\n",
            "train loss:0.008139147259190101\n",
            "train loss:0.0010095554640638543\n",
            "train loss:0.005634362747335228\n",
            "train loss:0.0010019956410320353\n",
            "train loss:0.007637688282853793\n",
            "train loss:0.003355488436619383\n",
            "train loss:0.0010101058114614532\n",
            "train loss:0.00028712931601742193\n",
            "train loss:0.0022992948301335226\n",
            "train loss:0.00011068907526282658\n",
            "train loss:0.00759082417887329\n",
            "train loss:0.002150144521463694\n",
            "train loss:0.004402953243082989\n",
            "train loss:0.0011675707485649326\n",
            "train loss:0.005170879610664335\n",
            "train loss:0.0011437069164885513\n",
            "train loss:0.0010977139695930644\n",
            "train loss:0.004466700512354061\n",
            "train loss:0.0019219330795676476\n",
            "train loss:0.010899123358280548\n",
            "train loss:0.0004439311642034248\n",
            "train loss:0.004852714430655842\n",
            "train loss:0.0012492255598252496\n",
            "train loss:0.000282241649696645\n",
            "train loss:0.010694785190507395\n",
            "train loss:0.0011500225909598575\n",
            "train loss:0.004503379420398942\n",
            "train loss:0.0006502230182957474\n",
            "train loss:0.002797346925588427\n",
            "train loss:0.000760494391663297\n",
            "train loss:0.0009117849775474162\n",
            "train loss:0.0013538091865587351\n",
            "train loss:0.007838656576074798\n",
            "train loss:0.002115981069428825\n",
            "train loss:0.0024620287162981096\n",
            "train loss:0.0005994044074937671\n",
            "train loss:0.004765053584510678\n",
            "train loss:0.0010935114431838964\n",
            "train loss:0.002101591461793016\n",
            "train loss:0.004114505583870061\n",
            "train loss:0.001094947079282268\n",
            "train loss:0.0094168144261899\n",
            "train loss:0.0010576410884271465\n",
            "train loss:0.001339953042288762\n",
            "train loss:0.0028572942813548602\n",
            "train loss:0.004179781026118565\n",
            "train loss:0.01800319991794586\n",
            "train loss:0.0002279946539062518\n",
            "train loss:0.00550293027007836\n",
            "train loss:0.0023059300613079985\n",
            "train loss:0.004570734393058304\n",
            "train loss:0.004108688209004254\n",
            "train loss:0.003117435574925841\n",
            "train loss:0.012983436314690644\n",
            "train loss:0.0026114608159423574\n",
            "train loss:0.007852274714346298\n",
            "train loss:0.001130007567560729\n",
            "train loss:0.0025839806083786084\n",
            "train loss:0.0008251801554543108\n",
            "train loss:0.014341372411407339\n",
            "train loss:0.0021147468328176016\n",
            "train loss:0.0009527418715002947\n",
            "train loss:0.0024383289903790926\n",
            "train loss:0.001515460962774162\n",
            "train loss:0.0002484299228696875\n",
            "train loss:0.003193640782113453\n",
            "train loss:0.003952317641851424\n",
            "train loss:0.0051166359746250105\n",
            "train loss:0.0012827248037956607\n",
            "train loss:0.04147054278663871\n",
            "train loss:0.0023894439126274742\n",
            "train loss:0.0007929152342684224\n",
            "train loss:0.000947791274101578\n",
            "train loss:0.0007802159286511692\n",
            "train loss:0.022383031993176487\n",
            "train loss:0.002203929084633021\n",
            "train loss:0.0017642913588741668\n",
            "train loss:0.009336391738856026\n",
            "train loss:0.0001484426689727586\n",
            "train loss:0.001166701962463641\n",
            "train loss:0.0002451817626894474\n",
            "train loss:0.0006764688471108731\n",
            "train loss:0.0034414869793203285\n",
            "train loss:0.037464485393901174\n",
            "train loss:0.0002466917992531991\n",
            "train loss:0.002082076662564203\n",
            "train loss:0.018142843078620376\n",
            "train loss:0.00043437986986952595\n",
            "train loss:0.0018397527612669326\n",
            "train loss:0.0033903613108649928\n",
            "train loss:0.005317210238742224\n",
            "train loss:0.0019492440666101602\n",
            "train loss:0.005368110524129363\n",
            "train loss:0.00021919525773868645\n",
            "train loss:0.00041140866313665994\n",
            "train loss:0.003582326045552763\n",
            "train loss:0.0003747743762190433\n",
            "train loss:0.004480943558459558\n",
            "train loss:0.016306850957031292\n",
            "train loss:0.0006912386872545517\n",
            "train loss:0.0023737388975402563\n",
            "train loss:0.005544798149484092\n",
            "train loss:0.0006612295341782727\n",
            "train loss:0.0005423087173263306\n",
            "train loss:0.007455362918747439\n",
            "train loss:0.0024630540247634006\n",
            "train loss:0.002052767935445995\n",
            "train loss:0.007928040546305946\n",
            "train loss:0.0017061783566684847\n",
            "train loss:0.0037415598465521875\n",
            "train loss:0.004110828364289654\n",
            "train loss:0.003793417298852411\n",
            "train loss:0.007214015648513003\n",
            "train loss:0.004281344211979147\n",
            "train loss:0.015548986619098447\n",
            "train loss:0.0006913773064885439\n",
            "train loss:0.0011766597476155674\n",
            "train loss:0.00036855356233918475\n",
            "train loss:0.07142685144563575\n",
            "train loss:0.007203585224901934\n",
            "train loss:0.004264922247027418\n",
            "train loss:0.002665788808365331\n",
            "train loss:0.0024161780714300612\n",
            "train loss:0.010627575363599063\n",
            "train loss:0.00044666421458947547\n",
            "train loss:0.006348406297045198\n",
            "train loss:0.0011731994086806468\n",
            "train loss:0.028245861802323458\n",
            "train loss:0.004100708030205673\n",
            "train loss:0.0005473604629184484\n",
            "train loss:0.007013535055921129\n",
            "train loss:0.00976785517880192\n",
            "train loss:0.0013296177984473733\n",
            "train loss:0.002966007157815642\n",
            "train loss:0.01625755080544363\n",
            "train loss:0.0015802927733209226\n",
            "train loss:0.0024260330037244016\n",
            "train loss:0.000454800025806434\n",
            "train loss:0.003631472769331091\n",
            "train loss:0.001055661459011364\n",
            "train loss:0.001535590909434663\n",
            "train loss:0.013108473713962851\n",
            "train loss:0.001628968276958775\n",
            "train loss:0.003972168574114822\n",
            "train loss:0.008315389035556016\n",
            "train loss:0.006051856648276992\n",
            "train loss:0.001782375138285693\n",
            "train loss:0.0015731573589432449\n",
            "train loss:0.0009757239103335676\n",
            "train loss:0.001469824483672543\n",
            "train loss:0.006523534142216344\n",
            "train loss:0.002500856531466643\n",
            "train loss:0.0016220805412091432\n",
            "train loss:0.007492748626343864\n",
            "train loss:0.009917405942413852\n",
            "train loss:0.00452317365517266\n",
            "train loss:0.005876465808075922\n",
            "train loss:0.007025707432402214\n",
            "train loss:0.0009180261116168016\n",
            "train loss:0.0035792222886431102\n",
            "train loss:0.001608482253419405\n",
            "train loss:0.0023648279757157264\n",
            "train loss:0.0007728881023688296\n",
            "train loss:0.006551819129016211\n",
            "train loss:0.0027211366053102827\n",
            "train loss:0.005607062917282156\n",
            "train loss:0.0034585023986503953\n",
            "train loss:0.02352102157212919\n",
            "train loss:0.003188309267453688\n",
            "train loss:0.001468878965395577\n",
            "train loss:0.005323433848177562\n",
            "train loss:0.0006006127113994564\n",
            "train loss:0.0028003878124235925\n",
            "train loss:0.0010612761718679393\n",
            "train loss:0.0034454327952232306\n",
            "train loss:0.0021502840038841262\n",
            "train loss:0.006146469785655142\n",
            "train loss:0.006711231889930106\n",
            "train loss:0.0009199976241765877\n",
            "train loss:0.004158866062867027\n",
            "train loss:0.0030153046730736787\n",
            "train loss:0.0028293395221238742\n",
            "train loss:0.009490222955092524\n",
            "train loss:0.0023025214091385382\n",
            "train loss:0.0007340715709921095\n",
            "train loss:0.0007491574998414077\n",
            "train loss:0.0435249415638314\n",
            "train loss:0.0033235717150286977\n",
            "train loss:0.004005507820548441\n",
            "train loss:0.0014318901150138705\n",
            "train loss:0.01929133004932126\n",
            "train loss:0.0020956885475820757\n",
            "train loss:0.0008337637116754302\n",
            "train loss:0.00018476851100561155\n",
            "train loss:0.0006274301021727169\n",
            "train loss:0.0004834980693098856\n",
            "train loss:0.0009467241768101706\n",
            "train loss:0.006202030095375669\n",
            "train loss:0.001135514488651973\n",
            "train loss:0.003249524097412857\n",
            "train loss:0.002267658262350906\n",
            "train loss:0.0009225003912119161\n",
            "train loss:0.0036990278054043605\n",
            "train loss:0.00017382683818915608\n",
            "train loss:0.004812167660382934\n",
            "train loss:0.007926621833905356\n",
            "train loss:0.002223943709352411\n",
            "train loss:0.0023369377830290607\n",
            "train loss:0.0037041637190768912\n",
            "train loss:0.001955889466520043\n",
            "train loss:0.0010442284057894328\n",
            "train loss:0.0035809713583408653\n",
            "train loss:0.004929211046199847\n",
            "train loss:0.0018915798358555339\n",
            "train loss:0.004108624062953844\n",
            "train loss:0.0021830135893530383\n",
            "train loss:0.0013401523674543955\n",
            "train loss:0.0005573904189014009\n",
            "train loss:0.005390821921801103\n",
            "train loss:0.004073576463355247\n",
            "train loss:0.0006609122720205219\n",
            "train loss:0.0013265264283013208\n",
            "train loss:0.0008807910679131767\n",
            "train loss:0.0037121343274366166\n",
            "train loss:0.0004959112332643353\n",
            "train loss:0.00012793739883290613\n",
            "train loss:0.008729878858441899\n",
            "train loss:0.00039680433272113407\n",
            "train loss:0.0008087416670737159\n",
            "train loss:0.00047916410750897977\n",
            "train loss:0.0006891240522365996\n",
            "train loss:0.0030177263790957566\n",
            "train loss:0.00013082571471330356\n",
            "train loss:0.0011504153497921628\n",
            "train loss:0.0008823666082882875\n",
            "train loss:0.00016178469504348833\n",
            "train loss:0.00033753830708591065\n",
            "train loss:0.012862233537266543\n",
            "train loss:0.0011913449606287972\n",
            "train loss:0.0014421180966493665\n",
            "train loss:0.0029221952628435644\n",
            "train loss:0.002172847585880196\n",
            "train loss:0.0015484977744677662\n",
            "train loss:0.011887540304891045\n",
            "train loss:0.000647120115489106\n",
            "train loss:0.0004262839919402658\n",
            "train loss:0.0030375623192220235\n",
            "train loss:0.006094450140995129\n",
            "train loss:0.005934285240604406\n",
            "train loss:0.005308524078364828\n",
            "train loss:0.0008232276300075743\n",
            "train loss:0.0002136325557741761\n",
            "train loss:0.0013682461407827392\n",
            "train loss:0.0021230399624705422\n",
            "train loss:0.00025102155351418623\n",
            "train loss:0.003407041199708469\n",
            "train loss:0.0012610546634232833\n",
            "train loss:0.0020564858944920757\n",
            "train loss:0.001089073039023533\n",
            "train loss:0.0005851803014346501\n",
            "train loss:0.0006439542977855588\n",
            "train loss:0.02120023396355405\n",
            "train loss:0.002653847479170785\n",
            "train loss:0.0009719849836812854\n",
            "train loss:0.09241521922117485\n",
            "train loss:0.004237867588969452\n",
            "train loss:0.0006027280508864259\n",
            "train loss:0.0015504407440723041\n",
            "train loss:0.00022185694478088592\n",
            "train loss:0.0004598922171723271\n",
            "train loss:0.00042895703781841246\n",
            "train loss:0.0062123781142793925\n",
            "train loss:0.0012172922850501745\n",
            "train loss:0.00041876756463391494\n",
            "train loss:0.00709525196240095\n",
            "train loss:0.0021561580312767937\n",
            "train loss:0.011970877398948592\n",
            "train loss:0.0018687485064115038\n",
            "train loss:0.0022628983206926614\n",
            "train loss:0.002469476666573849\n",
            "train loss:0.0033435520901281424\n",
            "train loss:0.0033849700432689533\n",
            "train loss:0.010842306404290437\n",
            "train loss:0.001972041958252615\n",
            "train loss:0.030753371986962198\n",
            "train loss:0.002248099900621666\n",
            "train loss:0.003117743674799701\n",
            "train loss:0.0008276337896489014\n",
            "train loss:0.005077348672497329\n",
            "train loss:0.0011383269627615713\n",
            "train loss:0.0010823046433770522\n",
            "train loss:0.006787807354425082\n",
            "train loss:0.0022805312314845244\n",
            "train loss:0.0002795489875215429\n",
            "train loss:0.004528505670493255\n",
            "train loss:0.00018141488275963004\n",
            "train loss:0.004417344154060711\n",
            "train loss:0.002055645860172957\n",
            "train loss:0.004850045764595862\n",
            "train loss:0.0022812363059093437\n",
            "train loss:0.0031461537384376353\n",
            "train loss:0.0056518569609808715\n",
            "train loss:0.004371151439452383\n",
            "train loss:0.0007560958331737833\n",
            "train loss:0.0008672966839556496\n",
            "train loss:0.0009459443177434033\n",
            "train loss:0.0023515351565970106\n",
            "train loss:0.030267164562131663\n",
            "train loss:0.002058724236356186\n",
            "train loss:0.0015535587631663856\n",
            "train loss:0.0030995116483649634\n",
            "train loss:0.028742735946476503\n",
            "train loss:0.00644179651364151\n",
            "train loss:0.004088161812760229\n",
            "train loss:0.0012856579678546336\n",
            "train loss:0.0004883677196876741\n",
            "train loss:0.0018862729143163872\n",
            "train loss:0.0040807268485209885\n",
            "train loss:0.0010232409325639886\n",
            "train loss:0.002180299621935705\n",
            "train loss:0.0016048091964603033\n",
            "train loss:0.003800574776453009\n",
            "train loss:0.005017707369443657\n",
            "train loss:0.00169745465234934\n",
            "train loss:0.0010327168959223258\n",
            "train loss:0.0016870446581828307\n",
            "train loss:0.005076958948942695\n",
            "train loss:0.012591352834849714\n",
            "train loss:0.0003476102695817413\n",
            "train loss:0.019513114802089344\n",
            "train loss:0.019229013309506774\n",
            "train loss:0.004216629999289839\n",
            "train loss:0.006781132519866411\n",
            "train loss:0.0009109866989609684\n",
            "train loss:0.0010051318977350183\n",
            "train loss:0.0007230186544605996\n",
            "train loss:0.0014569122593758876\n",
            "train loss:0.002604409657691294\n",
            "train loss:0.002377665065382269\n",
            "train loss:0.0020612742620202238\n",
            "train loss:0.0006845360737248369\n",
            "train loss:0.0007167179394298845\n",
            "train loss:0.004512019053057161\n",
            "train loss:0.0036076154441187446\n",
            "train loss:0.0021622486619973947\n",
            "train loss:0.0018620229025265475\n",
            "train loss:0.00970609086277483\n",
            "train loss:0.012316959490129202\n",
            "train loss:0.00041687908520168874\n",
            "train loss:0.0009825897902197788\n",
            "train loss:0.003254196402093723\n",
            "train loss:0.0038206661459413353\n",
            "train loss:0.00454247779450046\n",
            "train loss:0.001776519835092068\n",
            "train loss:0.0006949300232511007\n",
            "train loss:0.010136173884752946\n",
            "train loss:0.0017295538411986114\n",
            "train loss:0.005082723649023195\n",
            "train loss:0.009457419057335568\n",
            "train loss:0.0012341437761074297\n",
            "train loss:0.04760156440338013\n",
            "train loss:0.0016000598189794598\n",
            "train loss:0.0003507429722247001\n",
            "train loss:0.0020274411492426227\n",
            "train loss:0.009899544255587537\n",
            "train loss:0.0019530507874058365\n",
            "train loss:0.0001532082514212009\n",
            "train loss:0.0012708372508356167\n",
            "train loss:0.005001446783663199\n",
            "train loss:0.002907574794493146\n",
            "train loss:0.002556758692217755\n",
            "train loss:0.0025540422241523263\n",
            "train loss:0.017174456575774707\n",
            "train loss:0.0007779525187820753\n",
            "train loss:0.02000305734026358\n",
            "train loss:0.002628349861403592\n",
            "train loss:0.0017110917782241877\n",
            "train loss:0.0006093926748497848\n",
            "train loss:0.009787325207666403\n",
            "train loss:0.01635662554701653\n",
            "train loss:0.001537466196825538\n",
            "train loss:0.0011714917798695324\n",
            "train loss:0.004447055731327739\n",
            "train loss:0.0012281172970160185\n",
            "train loss:0.003088541108697789\n",
            "train loss:0.0028136460471272517\n",
            "train loss:0.0012335453250605494\n",
            "train loss:0.000786968317027216\n",
            "train loss:0.0033057402274727843\n",
            "train loss:0.0008960065914145442\n",
            "train loss:0.0013760068875881997\n",
            "train loss:0.0023165748372107314\n",
            "train loss:0.00436436376372264\n",
            "train loss:0.000535427176810634\n",
            "train loss:0.0004626255690903264\n",
            "train loss:0.0014561474043419456\n",
            "train loss:0.004114738096547345\n",
            "train loss:0.00010926316731346507\n",
            "train loss:0.00034050102115962613\n",
            "train loss:0.0012576875428725223\n",
            "train loss:0.004691370303497078\n",
            "train loss:0.0026256341528152418\n",
            "train loss:0.0002563062642661469\n",
            "train loss:0.001402179825844793\n",
            "train loss:0.003045330570244859\n",
            "train loss:0.0007819288688559307\n",
            "train loss:0.004383688190163432\n",
            "train loss:0.004668707962681704\n",
            "train loss:0.000647543572177338\n",
            "train loss:0.00390385728539089\n",
            "train loss:0.0013121287579623264\n",
            "train loss:0.003469004240056421\n",
            "train loss:0.00022323790546023082\n",
            "train loss:0.011907894501762422\n",
            "train loss:0.004966828447008786\n",
            "train loss:0.0004228291914829006\n",
            "train loss:0.003791393182583512\n",
            "train loss:0.005780073659602132\n",
            "train loss:0.0015385420662633912\n",
            "train loss:0.0036952956827082345\n",
            "train loss:0.0016895621430846647\n",
            "train loss:0.0004350424979245243\n",
            "train loss:0.006421209589556729\n",
            "train loss:0.02765396011935971\n",
            "train loss:0.012936770100919981\n",
            "train loss:0.004940934579949381\n",
            "train loss:0.03336908618964926\n",
            "train loss:0.0007871282699646803\n",
            "train loss:0.0008149493021747096\n",
            "train loss:0.0005611075434051869\n",
            "train loss:0.0037604581410078104\n",
            "train loss:0.0015741164211612237\n",
            "train loss:0.0005380701160824514\n",
            "train loss:0.004818673965779761\n",
            "train loss:0.004546840569084514\n",
            "train loss:0.0012661656707880656\n",
            "train loss:0.0016519768280183317\n",
            "train loss:0.0002815587301474749\n",
            "train loss:0.001024140393870321\n",
            "train loss:0.0033333024678905343\n",
            "train loss:0.001958589009192977\n",
            "train loss:0.004120064540761394\n",
            "train loss:0.0011670600857553392\n",
            "train loss:0.0003739173406341665\n",
            "train loss:0.00484695335461555\n",
            "train loss:0.0012883041760926317\n",
            "train loss:0.0039819049937023536\n",
            "train loss:0.0016344290558330552\n",
            "train loss:0.0009193751167666702\n",
            "train loss:0.0010841367763511514\n",
            "train loss:0.0028459275564606213\n",
            "train loss:0.0006376646397077463\n",
            "train loss:0.002324461637090297\n",
            "train loss:0.0021104435125333092\n",
            "train loss:0.0025593288361415345\n",
            "train loss:0.0018860106737529864\n",
            "train loss:0.04336980697938145\n",
            "train loss:0.0005608113539928609\n",
            "train loss:0.0002752017766848566\n",
            "train loss:0.027446486596925185\n",
            "train loss:0.009439573652984581\n",
            "train loss:0.0008418294718474132\n",
            "train loss:0.0015698625121846068\n",
            "train loss:0.0014246998104211408\n",
            "train loss:0.00155850170906548\n",
            "train loss:0.005071816687180144\n",
            "train loss:0.004338948343322075\n",
            "train loss:0.004605397126519859\n",
            "train loss:0.001235973592026956\n",
            "train loss:0.0009702803074366236\n",
            "train loss:0.0003431451409469385\n",
            "train loss:0.009590846142635813\n",
            "train loss:0.0036332733396098756\n",
            "train loss:0.002033010641341068\n",
            "train loss:0.00040730459329880114\n",
            "train loss:0.0012855960520262917\n",
            "train loss:0.006043735188699503\n",
            "train loss:0.0032562325292720983\n",
            "train loss:0.0008597875609566121\n",
            "train loss:0.006793124791070456\n",
            "train loss:0.00042125079706690035\n",
            "train loss:0.002446444869033013\n",
            "train loss:0.000883557359544326\n",
            "train loss:0.003414106756980385\n",
            "train loss:0.0019708491451257797\n",
            "train loss:0.001152700065829045\n",
            "train loss:0.004252594781367596\n",
            "train loss:0.0015406805745931062\n",
            "train loss:0.0011461359174931294\n",
            "train loss:0.001511796783922645\n",
            "train loss:0.00836293567257975\n",
            "train loss:0.00031208308068512265\n",
            "train loss:0.003808834986128286\n",
            "train loss:0.0007720249184966618\n",
            "train loss:0.00596830999442216\n",
            "train loss:0.0032535878143057106\n",
            "train loss:0.0002317042328375498\n",
            "train loss:0.004677777914002494\n",
            "train loss:0.002373636519021649\n",
            "train loss:0.0012245675659461718\n",
            "train loss:0.003523398478871187\n",
            "train loss:0.027065357572234397\n",
            "train loss:0.0002283487998093222\n",
            "train loss:0.0059640431449644975\n",
            "train loss:0.00401190732562649\n",
            "train loss:0.006833147539871969\n",
            "train loss:0.0012796286785831225\n",
            "train loss:0.0038216912189568135\n",
            "train loss:0.0004692743973865175\n",
            "train loss:0.003523081987033585\n",
            "train loss:0.003189354971181266\n",
            "train loss:0.0021298025034042147\n",
            "train loss:0.00021518885676657064\n",
            "train loss:0.0018683160550886113\n",
            "train loss:0.0004951278001374741\n",
            "train loss:0.0022027668203845\n",
            "train loss:0.003425286232880829\n",
            "train loss:0.01826902644174303\n",
            "train loss:0.0003102118540090271\n",
            "train loss:0.0018053164007541833\n",
            "train loss:0.0006896250256311826\n",
            "train loss:0.0007427491336137628\n",
            "train loss:0.0019545806044201854\n",
            "train loss:0.02507458390780569\n",
            "train loss:0.0030463697415776263\n",
            "train loss:0.0034067401340264225\n",
            "train loss:0.0009489818090860894\n",
            "train loss:0.00030557803373711264\n",
            "train loss:0.002716397488158451\n",
            "train loss:4.0228455977006006e-05\n",
            "train loss:0.005426545520413866\n",
            "train loss:0.0007930769081906425\n",
            "train loss:0.0055810312682430165\n",
            "train loss:0.0009444442466894773\n",
            "train loss:0.005865988883340124\n",
            "train loss:0.0014950957700826926\n",
            "train loss:0.0053577887805653455\n",
            "train loss:0.0033304852313678275\n",
            "train loss:0.0014990575495276801\n",
            "train loss:0.0021124904871959575\n",
            "train loss:0.00016788235607849368\n",
            "train loss:0.0015865453765690743\n",
            "train loss:0.0002884771575851737\n",
            "train loss:0.004397936790581999\n",
            "train loss:0.002012882233899431\n",
            "train loss:0.002564422347186944\n",
            "train loss:0.0008530877410897038\n",
            "train loss:0.019877539591528735\n",
            "train loss:0.0005573246047825053\n",
            "train loss:0.0022247932221428858\n",
            "train loss:0.001028188797854909\n",
            "train loss:0.002656332071907951\n",
            "train loss:0.004511485334809125\n",
            "train loss:0.0012609759984008586\n",
            "train loss:0.02112850936362571\n",
            "train loss:0.004174315415575458\n",
            "=== epoch:15, train acc:0.997, test acc:0.99 ===\n",
            "train loss:0.0034933288796785283\n",
            "train loss:0.004764701895535504\n",
            "train loss:0.0037138187485924257\n",
            "train loss:0.0038455413036960194\n",
            "train loss:0.0036597561783693255\n",
            "train loss:0.005189606559484504\n",
            "train loss:0.0003612251307770948\n",
            "train loss:0.0011715167819705008\n",
            "train loss:0.0006493224922066179\n",
            "train loss:0.004957614801729266\n",
            "train loss:0.0002687499609426423\n",
            "train loss:0.0031532549099246993\n",
            "train loss:0.00012922268023168615\n",
            "train loss:0.004905942135569291\n",
            "train loss:0.005642698075139063\n",
            "train loss:0.011240077263260286\n",
            "train loss:0.0016901384736975151\n",
            "train loss:0.005551942319333803\n",
            "train loss:0.00016683911664584273\n",
            "train loss:0.0018541463187181065\n",
            "train loss:0.004570887683456438\n",
            "train loss:0.009777398497403228\n",
            "train loss:0.0025258500393657885\n",
            "train loss:0.0003535797828995934\n",
            "train loss:0.00015013526426238384\n",
            "train loss:0.003359105161205615\n",
            "train loss:0.004203368801933922\n",
            "train loss:0.0018539496565125743\n",
            "train loss:0.0039802517586265\n",
            "train loss:0.012063511145389396\n",
            "train loss:0.0008943057012944096\n",
            "train loss:0.0017738812739542611\n",
            "train loss:0.003811044449889307\n",
            "train loss:0.001617630019683282\n",
            "train loss:0.007424177776134765\n",
            "train loss:0.0003196053228108305\n",
            "train loss:0.013225780597170113\n",
            "train loss:0.006831171640629289\n",
            "train loss:0.0004693588412472638\n",
            "train loss:0.0013224460849024914\n",
            "train loss:0.0021466654640676956\n",
            "train loss:0.0011586665933362103\n",
            "train loss:0.004028715213066031\n",
            "train loss:0.0060388375758356925\n",
            "train loss:0.004117196460949626\n",
            "train loss:0.002486271547199545\n",
            "train loss:0.006354082065718649\n",
            "train loss:8.882066057841885e-05\n",
            "train loss:0.00032362920265493887\n",
            "train loss:0.0013854539266291916\n",
            "train loss:0.001113150958558365\n",
            "train loss:0.0003667914438862298\n",
            "train loss:0.0038367889240535926\n",
            "train loss:0.002966920189122226\n",
            "train loss:0.0005841285537676146\n",
            "train loss:0.0010278712003363017\n",
            "train loss:0.002296177654241956\n",
            "train loss:0.0010195003070735606\n",
            "train loss:7.686556781147232e-05\n",
            "train loss:0.010184351702659777\n",
            "train loss:0.0005450089897386452\n",
            "train loss:0.011833793786817325\n",
            "train loss:0.003731784582765228\n",
            "train loss:0.005412977581220837\n",
            "train loss:0.0019302970651858232\n",
            "train loss:0.0007147984070792989\n",
            "train loss:0.010467224389454584\n",
            "train loss:0.0033642821988732734\n",
            "train loss:0.005884297740447133\n",
            "train loss:0.007276784715310993\n",
            "train loss:0.043315721967557425\n",
            "train loss:0.0010444820557978774\n",
            "train loss:0.00012122394396962107\n",
            "train loss:0.0025723649666175847\n",
            "train loss:0.0006314702429074531\n",
            "train loss:0.01267728709212935\n",
            "train loss:0.0040723182906636434\n",
            "train loss:0.004306018422129739\n",
            "train loss:0.0007103821464287024\n",
            "train loss:0.006931227560846785\n",
            "train loss:0.0018513027524092143\n",
            "train loss:0.0016481977515966548\n",
            "train loss:0.0777231316257832\n",
            "train loss:0.002111062015812081\n",
            "train loss:0.000979368298163142\n",
            "train loss:0.0005036828276183556\n",
            "train loss:0.00264687658691593\n",
            "train loss:0.0015208526468791248\n",
            "train loss:0.012105652797952969\n",
            "train loss:0.004579033458225124\n",
            "train loss:0.0016448443531691707\n",
            "train loss:0.0013975085273770452\n",
            "train loss:0.022024565051604945\n",
            "train loss:0.014214519002982628\n",
            "train loss:0.0042752233319964155\n",
            "train loss:0.002843875847029508\n",
            "train loss:0.014311609568721349\n",
            "train loss:0.002203043638837412\n",
            "train loss:0.004830947350085738\n",
            "train loss:0.0022422946635848235\n",
            "train loss:0.001717423800055507\n",
            "train loss:0.0013659835674542187\n",
            "train loss:0.005040936967912336\n",
            "train loss:0.024461928678865544\n",
            "train loss:0.008404316358769995\n",
            "train loss:0.0013225054636753447\n",
            "train loss:0.003369027866698501\n",
            "train loss:0.013398122256329739\n",
            "train loss:0.010498446322945746\n",
            "train loss:0.01967893625596242\n",
            "train loss:0.0026023444555569215\n",
            "train loss:0.02655197441354871\n",
            "train loss:0.02176410234164484\n",
            "train loss:0.003692289481449463\n",
            "train loss:0.0013811745060024376\n",
            "train loss:0.003558761544508315\n",
            "train loss:0.009871972234242153\n",
            "train loss:0.006072661986767103\n",
            "train loss:0.011697989585457808\n",
            "train loss:0.0006885965571001127\n",
            "train loss:0.0004361985357222284\n",
            "train loss:0.0096852615742708\n",
            "train loss:0.015313631818721734\n",
            "train loss:0.003628283614298819\n",
            "train loss:0.0006821405632673698\n",
            "train loss:0.005929448211973066\n",
            "train loss:0.0013531147882116094\n",
            "train loss:0.0008209697642575221\n",
            "train loss:0.0037949262258969653\n",
            "train loss:0.0022150737210569747\n",
            "train loss:0.015490306255987305\n",
            "train loss:0.0021387807330583986\n",
            "train loss:0.0002996519577966129\n",
            "train loss:0.002242905812947549\n",
            "train loss:0.013891029149344229\n",
            "train loss:0.0013355294235769006\n",
            "train loss:0.0010153027378747132\n",
            "train loss:0.0020432585886574718\n",
            "train loss:0.014838769717625027\n",
            "train loss:0.0006133970219630689\n",
            "train loss:0.0021623057928526353\n",
            "train loss:0.0006366961667409471\n",
            "train loss:0.000881978578706416\n",
            "train loss:0.002378185189152158\n",
            "train loss:0.0004099201645213411\n",
            "train loss:0.024431598785729192\n",
            "train loss:0.0009717618003063372\n",
            "train loss:0.0002284619773793872\n",
            "train loss:0.0007553133306903062\n",
            "train loss:0.0004312800941282637\n",
            "train loss:0.032892144438182246\n",
            "train loss:0.004496940641163559\n",
            "train loss:0.00048794784194267435\n",
            "train loss:0.004919098290920657\n",
            "train loss:0.002899430851973589\n",
            "train loss:0.0019800574030775803\n",
            "train loss:0.005312994863771151\n",
            "train loss:0.006593247633306899\n",
            "train loss:0.001448820496988652\n",
            "train loss:0.004178275694878996\n",
            "train loss:0.0018416314459547664\n",
            "train loss:0.00274076906799081\n",
            "train loss:0.0051974644898830705\n",
            "train loss:0.00860071047781951\n",
            "train loss:0.001336824319185502\n",
            "train loss:0.003197856807333062\n",
            "train loss:0.0013523396154030115\n",
            "train loss:0.00622150985786921\n",
            "train loss:0.002716799084538532\n",
            "train loss:0.0004808883374837139\n",
            "train loss:0.00441664971490841\n",
            "train loss:0.0033003817566647853\n",
            "train loss:0.0034848418214626004\n",
            "train loss:0.00202202525730523\n",
            "train loss:0.0015802940133873484\n",
            "train loss:0.0067866339764737086\n",
            "train loss:0.006032983971107618\n",
            "train loss:0.0012349551671668746\n",
            "train loss:0.004902888745923594\n",
            "train loss:0.01546817052990213\n",
            "train loss:0.0015398388341287261\n",
            "train loss:0.0009036002781702235\n",
            "train loss:0.0019251846572595555\n",
            "train loss:0.002025412724159113\n",
            "train loss:0.014579094322333461\n",
            "train loss:0.0008571841043468442\n",
            "train loss:0.0005863380253867664\n",
            "train loss:0.001772081634500264\n",
            "train loss:0.005666257037596974\n",
            "train loss:0.002965700391264164\n",
            "train loss:0.018333091567664418\n",
            "train loss:0.013553510489309934\n",
            "train loss:0.02119859387430891\n",
            "train loss:0.00102162802932046\n",
            "train loss:0.00033269426425750577\n",
            "train loss:0.0018318051251213616\n",
            "train loss:0.00019928942874096738\n",
            "train loss:0.0047116588702348795\n",
            "train loss:0.0018091206829776829\n",
            "train loss:0.000968837652089423\n",
            "train loss:0.001847719082623822\n",
            "train loss:0.017213539447776503\n",
            "train loss:0.007366802586440947\n",
            "train loss:0.004978108791784006\n",
            "train loss:0.006622017390573808\n",
            "train loss:0.0008060605543480851\n",
            "train loss:0.001185756610275356\n",
            "train loss:0.0023939483388990697\n",
            "train loss:0.00017466583145610938\n",
            "train loss:0.0038857194985540745\n",
            "train loss:0.018201676882258468\n",
            "train loss:0.0004119080261334659\n",
            "train loss:0.000957113020106606\n",
            "train loss:0.00012000901344114922\n",
            "train loss:0.0027966755335096295\n",
            "train loss:0.0004903251984630818\n",
            "train loss:0.002121085031732315\n",
            "train loss:0.00046718311228711464\n",
            "train loss:0.0002796783632814723\n",
            "train loss:0.0021946155162247214\n",
            "train loss:0.0010924520411544276\n",
            "train loss:0.0038156326067274138\n",
            "train loss:0.0012953605254608079\n",
            "train loss:0.005744104618152165\n",
            "train loss:0.007503791785096816\n",
            "train loss:0.0015243126364454422\n",
            "train loss:0.0009735848687298094\n",
            "train loss:0.0015219708968541113\n",
            "train loss:0.0011723134328746618\n",
            "train loss:0.004549451269336604\n",
            "train loss:0.004929916468455008\n",
            "train loss:0.0008889753610526462\n",
            "train loss:0.0007597954088785579\n",
            "train loss:0.0006090766939146077\n",
            "train loss:0.012307069488146137\n",
            "train loss:0.000566953906583776\n",
            "train loss:0.0035553738976446246\n",
            "train loss:0.0006066398918441446\n",
            "train loss:0.01034633966333955\n",
            "train loss:0.006150862101536571\n",
            "train loss:0.00034344800835174724\n",
            "train loss:0.0017652377990936335\n",
            "train loss:0.0012370701725746155\n",
            "train loss:0.000527843728112636\n",
            "train loss:0.0009970911495520908\n",
            "train loss:0.020627060009743448\n",
            "train loss:0.002666035940714061\n",
            "train loss:0.006133116988218329\n",
            "train loss:0.0035093042526601285\n",
            "train loss:0.008617674233987422\n",
            "train loss:0.015577018342111346\n",
            "train loss:0.007664633993983894\n",
            "train loss:0.0010674296446059302\n",
            "train loss:0.0005899949226859997\n",
            "train loss:0.0006983805812652313\n",
            "train loss:0.0017616173104383812\n",
            "train loss:0.006372806802384825\n",
            "train loss:0.0001424324576597199\n",
            "train loss:0.0014743638465821163\n",
            "train loss:0.0007659839376834754\n",
            "train loss:0.0049047714443186405\n",
            "train loss:0.0002535724840756349\n",
            "train loss:0.00029327412818925863\n",
            "train loss:0.005345844667337183\n",
            "train loss:0.001130594345515949\n",
            "train loss:0.0035692931824269835\n",
            "train loss:0.002132403722820077\n",
            "train loss:0.001872344487844161\n",
            "train loss:0.008762925111938859\n",
            "train loss:0.002314629390860848\n",
            "train loss:0.0008571040457135567\n",
            "train loss:0.004269250925466748\n",
            "train loss:0.0010042404014906183\n",
            "train loss:0.0022408676108912314\n",
            "train loss:0.0005914554978483201\n",
            "train loss:0.0012967222602709013\n",
            "train loss:6.780142594216368e-05\n",
            "train loss:0.003959123651665527\n",
            "train loss:0.000597138918821125\n",
            "train loss:0.0011056471697870579\n",
            "train loss:0.000576791266467317\n",
            "train loss:0.0012772221437421535\n",
            "train loss:0.0036886858977459307\n",
            "train loss:0.0004215805703018848\n",
            "train loss:0.01983336820596138\n",
            "train loss:0.0022725144176791797\n",
            "train loss:0.00036481254894572585\n",
            "train loss:0.03054688732586971\n",
            "train loss:0.010435693909433864\n",
            "train loss:0.0010842157945803509\n",
            "train loss:0.0003780649440601824\n",
            "train loss:0.0015146500467386615\n",
            "train loss:0.0010380733248994472\n",
            "train loss:0.0038036796999014063\n",
            "train loss:0.004392409179735514\n",
            "train loss:0.0031684202349396865\n",
            "train loss:0.004519126459420228\n",
            "train loss:0.05016560952859051\n",
            "train loss:0.0027882070529108207\n",
            "train loss:0.007912786795273961\n",
            "train loss:0.0022922389793737052\n",
            "train loss:0.0013561756154727529\n",
            "train loss:0.0005628049744925925\n",
            "train loss:0.004900249011610052\n",
            "train loss:0.00013997933466361817\n",
            "train loss:0.002181324398171563\n",
            "train loss:0.0027222008306052982\n",
            "train loss:0.0003209414080976055\n",
            "train loss:0.0018392060164415152\n",
            "train loss:0.0020408296433897686\n",
            "train loss:0.00028394069901699934\n",
            "train loss:0.006797702149726633\n",
            "train loss:0.001993421524268994\n",
            "train loss:0.0011835923249856168\n",
            "train loss:0.0033669117318168673\n",
            "train loss:0.000883393187559079\n",
            "train loss:0.00027357528062219537\n",
            "train loss:0.001290412950075745\n",
            "train loss:0.0006618699616837598\n",
            "train loss:0.001142714204939956\n",
            "train loss:0.011740037862509902\n",
            "train loss:0.0015126645775042655\n",
            "train loss:0.0010395777063093295\n",
            "train loss:0.0007562844125030682\n",
            "train loss:0.0006696630277419105\n",
            "train loss:0.0004712700208280162\n",
            "train loss:0.002035462499945358\n",
            "train loss:0.0022976304598476566\n",
            "train loss:0.003920387755099085\n",
            "train loss:0.004909389118222851\n",
            "train loss:0.0026790568381143322\n",
            "train loss:0.0003803290879553709\n",
            "train loss:0.0011990347625401028\n",
            "train loss:0.002133535079662114\n",
            "train loss:0.0006452837436177018\n",
            "train loss:0.0009131431542688654\n",
            "train loss:0.0005512152997459813\n",
            "train loss:0.0009896033600961115\n",
            "train loss:0.002935620305847309\n",
            "train loss:0.000427964352827929\n",
            "train loss:0.004069266220730265\n",
            "train loss:0.002868512715196828\n",
            "train loss:0.0016224840897755553\n",
            "train loss:0.0035676229196028274\n",
            "train loss:0.012566285516203115\n",
            "train loss:0.000696351360153762\n",
            "train loss:0.0002610565872019044\n",
            "train loss:0.0015821528029778228\n",
            "train loss:0.0008131054620886124\n",
            "train loss:0.0004906457929740806\n",
            "train loss:0.0018562023731191056\n",
            "train loss:0.002004359519787204\n",
            "train loss:0.002437866074374159\n",
            "train loss:0.0027041498199869697\n",
            "train loss:0.0005265364653838062\n",
            "train loss:0.0005389708868029137\n",
            "train loss:0.0005958659724371978\n",
            "train loss:0.0015558011309480492\n",
            "train loss:0.0016371120961233454\n",
            "train loss:0.0016701352462944136\n",
            "train loss:0.0044821055631055085\n",
            "train loss:0.0002462506909702856\n",
            "train loss:0.005932557000257984\n",
            "train loss:0.0050757867454045355\n",
            "train loss:0.00152507368582825\n",
            "train loss:0.0012808421482303387\n",
            "train loss:0.0009849804145907332\n",
            "train loss:0.0018778465797529517\n",
            "train loss:0.0019531091754201502\n",
            "train loss:0.0005537563102769068\n",
            "train loss:0.00131678306455947\n",
            "train loss:0.0007558316183370427\n",
            "train loss:0.0002382778071441206\n",
            "train loss:0.0009712015243014131\n",
            "train loss:0.0036128730491898076\n",
            "train loss:0.0006879577812256474\n",
            "train loss:0.01756494727466587\n",
            "train loss:0.009370696828678814\n",
            "train loss:0.0014433144091226696\n",
            "train loss:0.015484415493747083\n",
            "train loss:0.00347193177959596\n",
            "train loss:0.0009600707965942444\n",
            "train loss:0.0035691596016605574\n",
            "train loss:0.0003916101467628774\n",
            "train loss:0.023402362765034827\n",
            "train loss:0.004690558715793792\n",
            "train loss:0.0019294845261687266\n",
            "train loss:0.0009453463746130697\n",
            "train loss:0.0003412278834371748\n",
            "train loss:0.0005693823025668566\n",
            "train loss:0.002417734469000553\n",
            "train loss:0.0008425026519289858\n",
            "train loss:0.00295504275367545\n",
            "train loss:0.018347810980022146\n",
            "train loss:0.023345213554498673\n",
            "train loss:0.002963958351865325\n",
            "train loss:0.002432754683223686\n",
            "train loss:0.00045095017927612296\n",
            "train loss:0.0012511134796689008\n",
            "train loss:0.000665379468325158\n",
            "train loss:0.014643301209091249\n",
            "train loss:0.000434099361218145\n",
            "train loss:0.0018850363453388691\n",
            "train loss:0.000734333541589467\n",
            "train loss:0.001754602372409457\n",
            "train loss:0.0028514241963535063\n",
            "train loss:0.001216960070234961\n",
            "train loss:0.0009606672465337404\n",
            "train loss:0.00035862504266410587\n",
            "train loss:0.003904477594733469\n",
            "train loss:0.002360233893783195\n",
            "train loss:0.0005081756034358001\n",
            "train loss:0.005047840811570271\n",
            "train loss:0.00753790745235777\n",
            "train loss:0.002313482005732479\n",
            "train loss:0.0007457365732236406\n",
            "train loss:0.001422623871271387\n",
            "train loss:0.0007272397811162425\n",
            "train loss:3.630990689126794e-05\n",
            "train loss:0.001565726965447327\n",
            "train loss:0.0025971374966496153\n",
            "train loss:0.0007418864401249411\n",
            "train loss:7.526055509825342e-05\n",
            "train loss:0.0005519374239269442\n",
            "train loss:0.0017844135075675885\n",
            "train loss:0.0018684814853897169\n",
            "train loss:0.0011361576540777863\n",
            "train loss:0.0010551914730024571\n",
            "train loss:0.00023490080428167378\n",
            "train loss:0.0012885700581620483\n",
            "train loss:0.002648337713376517\n",
            "train loss:0.001611640853134359\n",
            "train loss:0.0038124684147355903\n",
            "train loss:0.0022199624730048456\n",
            "train loss:0.004518693050652205\n",
            "train loss:0.007280785651735066\n",
            "train loss:0.009091437857547147\n",
            "train loss:0.002100583267567259\n",
            "train loss:0.001304193272487336\n",
            "train loss:0.01567649487792534\n",
            "train loss:0.0032043394606009025\n",
            "train loss:0.0010732154728864218\n",
            "train loss:0.0004284114943703258\n",
            "train loss:0.0016475960125233577\n",
            "train loss:0.004959219167229576\n",
            "train loss:0.0025312844479692107\n",
            "train loss:0.0022685951934113186\n",
            "train loss:0.0014998621793250247\n",
            "train loss:0.001082652223194966\n",
            "train loss:0.002873700704015531\n",
            "train loss:0.0002624315253065605\n",
            "train loss:0.0003713263164115048\n",
            "train loss:0.00017459504142497195\n",
            "train loss:3.962791847039752e-05\n",
            "train loss:0.08828537191932156\n",
            "train loss:0.0008472975354196043\n",
            "train loss:0.0005656759786920674\n",
            "train loss:0.0014907527220414185\n",
            "train loss:0.003639734751626518\n",
            "train loss:0.00107787282481617\n",
            "train loss:0.001154913597984755\n",
            "train loss:0.0014423862602350973\n",
            "train loss:0.0033017328669722274\n",
            "train loss:0.0013227227854642615\n",
            "train loss:0.0010764153072102902\n",
            "train loss:0.0049628289978502745\n",
            "train loss:0.0004992939033496142\n",
            "train loss:0.0014365272309235589\n",
            "train loss:0.001302728894995156\n",
            "train loss:0.001078155667444349\n",
            "train loss:0.0011668210361644431\n",
            "train loss:0.0023560369660176783\n",
            "train loss:0.002521161515323964\n",
            "train loss:0.0004476393685497698\n",
            "train loss:0.002890844001037894\n",
            "train loss:0.005299718765827624\n",
            "train loss:0.0011248696109715719\n",
            "train loss:0.0007943922956048735\n",
            "train loss:0.001591198863252159\n",
            "train loss:0.00034962331758481643\n",
            "train loss:0.006328610826099496\n",
            "train loss:0.006034226068149894\n",
            "train loss:0.00046920827450043246\n",
            "train loss:0.0013965530580238026\n",
            "train loss:0.00035126641606278626\n",
            "train loss:0.0007644540578256439\n",
            "train loss:0.0003284628222482457\n",
            "train loss:0.003255925589101406\n",
            "train loss:0.0031405755487951244\n",
            "train loss:0.0030876889652420307\n",
            "train loss:0.0020703822456265434\n",
            "train loss:0.0036058474253004417\n",
            "train loss:0.005199766781157319\n",
            "train loss:0.004982674713252835\n",
            "train loss:0.00047012034588739845\n",
            "train loss:0.0009644688129498254\n",
            "train loss:0.0003598123978308351\n",
            "train loss:0.002812315368508563\n",
            "train loss:0.002079114836896442\n",
            "train loss:0.0004713085761088859\n",
            "train loss:0.0005966168914785599\n",
            "train loss:0.00014229888991005004\n",
            "train loss:0.005628622854913607\n",
            "train loss:0.00010031536265612958\n",
            "train loss:0.00010390696082991554\n",
            "train loss:0.00935738572395536\n",
            "train loss:0.025226254495363004\n",
            "train loss:0.00036853674374432615\n",
            "train loss:0.00023106548926034317\n",
            "train loss:0.0015320950253888636\n",
            "train loss:0.0025894469683064698\n",
            "train loss:0.013108098500412559\n",
            "train loss:8.614886120605352e-05\n",
            "train loss:0.0014343757312557126\n",
            "train loss:0.0013745405694038303\n",
            "train loss:0.006966320653182054\n",
            "train loss:0.0008243762043805819\n",
            "train loss:0.0011515188638408667\n",
            "train loss:0.00023418045565320002\n",
            "train loss:0.00013006649425432087\n",
            "train loss:0.006608310522877605\n",
            "train loss:0.0008654934618005078\n",
            "train loss:0.001855645270663319\n",
            "train loss:0.001254097986515623\n",
            "train loss:0.0003791787269334858\n",
            "train loss:0.001111070601008424\n",
            "train loss:0.0010958024576166211\n",
            "train loss:0.003204792440899021\n",
            "train loss:0.002144773334763402\n",
            "train loss:0.004371315470158451\n",
            "train loss:0.0002675219349972528\n",
            "train loss:0.001936957636386776\n",
            "train loss:0.007255683402567607\n",
            "train loss:0.0042170475355912795\n",
            "train loss:0.0010657603607054653\n",
            "train loss:0.0036313770169430987\n",
            "train loss:0.0006392482379243191\n",
            "train loss:0.0035934566502825766\n",
            "train loss:0.003811645002762881\n",
            "train loss:0.012780812301702325\n",
            "train loss:0.0007690589777741491\n",
            "train loss:0.00017365798798993029\n",
            "train loss:0.00410065471214979\n",
            "train loss:0.0074574325069779205\n",
            "train loss:0.0041361813460833695\n",
            "train loss:0.028714031885631167\n",
            "train loss:0.0056039755523704495\n",
            "train loss:0.01751807948826113\n",
            "train loss:0.0031192830299867484\n",
            "train loss:0.000351944266460829\n",
            "train loss:0.0029228268087900977\n",
            "train loss:0.0027004353204921755\n",
            "train loss:0.0046968528434086585\n",
            "train loss:0.011810045591607705\n",
            "train loss:0.005009692425920619\n",
            "train loss:0.009541124638720089\n",
            "train loss:0.0008041906620932941\n",
            "train loss:0.0010373633353667395\n",
            "train loss:0.00973480686802819\n",
            "train loss:0.000802015379284127\n",
            "train loss:0.003440202954904881\n",
            "train loss:0.001587165826444822\n",
            "train loss:0.0008242909098159349\n",
            "train loss:0.005208180445313798\n",
            "train loss:0.003625337205220399\n",
            "train loss:0.012339205119656336\n",
            "train loss:0.002348279526356944\n",
            "train loss:0.03504106957680689\n",
            "train loss:0.0005441964251151446\n",
            "train loss:0.007268627241153766\n",
            "train loss:0.005572248368091788\n",
            "train loss:0.006765545308224389\n",
            "train loss:0.0045915419780886224\n",
            "train loss:0.0005036984931534694\n",
            "train loss:0.0033368674726297594\n",
            "train loss:0.0011572381416721097\n",
            "train loss:0.006337826128433835\n",
            "train loss:0.0037096670831235495\n",
            "train loss:0.00497939890292343\n",
            "train loss:0.0031773931933032407\n",
            "train loss:0.013593214320808835\n",
            "train loss:0.0028056515362766426\n",
            "train loss:0.003366897649956343\n",
            "train loss:0.0031364630471256887\n",
            "train loss:0.0013996654662593031\n",
            "train loss:0.0008734823495140987\n",
            "train loss:0.03680260726226771\n",
            "train loss:0.0005111174641323784\n",
            "train loss:0.0011605999323753278\n",
            "train loss:0.005580153367822237\n",
            "train loss:0.0017718382680698495\n",
            "train loss:0.010620881658603113\n",
            "train loss:0.0011284418088510703\n",
            "train loss:0.0003168415481170412\n",
            "train loss:0.004616772907813062\n",
            "train loss:0.0011415498017533557\n",
            "train loss:0.0002732800344511389\n",
            "train loss:0.0009676458617804617\n",
            "train loss:0.010146789752472763\n",
            "train loss:0.0016998380444916198\n",
            "=== epoch:16, train acc:0.998, test acc:0.986 ===\n",
            "train loss:0.001525916136873203\n",
            "train loss:0.0023418942110080476\n",
            "train loss:0.0023695056611472364\n",
            "train loss:0.00317086759301063\n",
            "train loss:0.005534721972830899\n",
            "train loss:0.0051481585999280785\n",
            "train loss:0.0018442623382674413\n",
            "train loss:0.0016430700490705225\n",
            "train loss:0.007488020503372165\n",
            "train loss:0.0004956674600354779\n",
            "train loss:0.002971159888356272\n",
            "train loss:0.0036293009847877455\n",
            "train loss:0.00110175498496024\n",
            "train loss:0.0019376586959666277\n",
            "train loss:0.0009784366423617472\n",
            "train loss:0.002330495121174783\n",
            "train loss:0.0008554652952504185\n",
            "train loss:0.0002861763336048864\n",
            "train loss:0.0011778523378683336\n",
            "train loss:0.006899592778349796\n",
            "train loss:0.0003688038332198889\n",
            "train loss:0.0012628846416769988\n",
            "train loss:0.0017113430795946765\n",
            "train loss:0.002368635393271711\n",
            "train loss:0.0024265662324630135\n",
            "train loss:0.00030349431279538786\n",
            "train loss:0.0010686285592706047\n",
            "train loss:0.0036339448468958365\n",
            "train loss:0.0005489088242288939\n",
            "train loss:0.0017094077894316197\n",
            "train loss:0.0015301105970096048\n",
            "train loss:0.002941032923521884\n",
            "train loss:0.002181197208913867\n",
            "train loss:0.0007772398468193687\n",
            "train loss:0.0016389291430560556\n",
            "train loss:0.008168441180145926\n",
            "train loss:0.0017399140201409884\n",
            "train loss:0.0002079892447545776\n",
            "train loss:0.0001282313884373967\n",
            "train loss:0.00270019728766094\n",
            "train loss:0.003386348329949099\n",
            "train loss:0.0006039853526790046\n",
            "train loss:0.00033840451247887975\n",
            "train loss:0.002678799761084885\n",
            "train loss:0.0010747450701728933\n",
            "train loss:0.0008386859369901489\n",
            "train loss:0.001156088261298618\n",
            "train loss:0.0020335204782419955\n",
            "train loss:0.0014137054714281022\n",
            "train loss:0.0029152559720728053\n",
            "train loss:0.0147601243516771\n",
            "train loss:0.00022469725245688758\n",
            "train loss:0.0011642450927392889\n",
            "train loss:0.0012273519076478733\n",
            "train loss:0.0001554397363456197\n",
            "train loss:0.0009544542576881073\n",
            "train loss:0.0007323629553182603\n",
            "train loss:0.0013897245440925227\n",
            "train loss:0.0008795541737857632\n",
            "train loss:0.00211985805769454\n",
            "train loss:0.00047349229205237436\n",
            "train loss:0.0008809734509218565\n",
            "train loss:0.0014720320809094992\n",
            "train loss:0.019050630926396455\n",
            "train loss:0.004390045630332016\n",
            "train loss:0.001172463931916113\n",
            "train loss:0.0006625454097398174\n",
            "train loss:0.00043299201374755446\n",
            "train loss:0.0007325824780743211\n",
            "train loss:0.00039453097832698326\n",
            "train loss:0.0015280269658826577\n",
            "train loss:0.0029720640874727946\n",
            "train loss:0.002738533790992745\n",
            "train loss:0.00010449688457701877\n",
            "train loss:0.000413006089874918\n",
            "train loss:0.0002818482415091946\n",
            "train loss:0.005571184538996155\n",
            "train loss:0.0005695152637919302\n",
            "train loss:0.0016378966689162514\n",
            "train loss:0.0023031809202270826\n",
            "train loss:0.0004039984660217985\n",
            "train loss:0.0028753247441270936\n",
            "train loss:0.0020476812597757794\n",
            "train loss:0.0004820346083582239\n",
            "train loss:0.00034597941739160235\n",
            "train loss:0.0008633507963396108\n",
            "train loss:0.0010370128405746287\n",
            "train loss:0.001512269918008089\n",
            "train loss:0.002129755321411149\n",
            "train loss:0.0016999977671263028\n",
            "train loss:0.00032551976531536677\n",
            "train loss:0.0009431550765299726\n",
            "train loss:0.0017999867123493463\n",
            "train loss:0.006095256552607429\n",
            "train loss:0.0006085390560251073\n",
            "train loss:0.001258500248151251\n",
            "train loss:0.00011005508867616005\n",
            "train loss:4.9056773597135053e-05\n",
            "train loss:0.0006615159200369571\n",
            "train loss:0.0004481874843735142\n",
            "train loss:0.0001179964681574472\n",
            "train loss:0.002584317856375941\n",
            "train loss:9.713717629127551e-05\n",
            "train loss:0.0004239923407169703\n",
            "train loss:0.0025656996545270153\n",
            "train loss:0.0005932855933969146\n",
            "train loss:0.0027194020272148624\n",
            "train loss:0.0019763763626355797\n",
            "train loss:0.0003713390320337918\n",
            "train loss:0.0017288415956428153\n",
            "train loss:0.003775598626912015\n",
            "train loss:0.0001332495061400221\n",
            "train loss:0.001498576656693463\n",
            "train loss:0.004934172126197834\n",
            "train loss:0.0003784771134700859\n",
            "train loss:0.0028411613147996816\n",
            "train loss:0.0019687005522905886\n",
            "train loss:0.00026530530978600915\n",
            "train loss:0.004437726051769712\n",
            "train loss:0.00013029768472259335\n",
            "train loss:0.00046478001248661074\n",
            "train loss:0.00047071713763339594\n",
            "train loss:0.00045265440852475974\n",
            "train loss:0.003525506740944074\n",
            "train loss:0.001268834024843667\n",
            "train loss:0.0008392066211072856\n",
            "train loss:0.004851339651658024\n",
            "train loss:0.0005852523736344908\n",
            "train loss:0.00035401380198686404\n",
            "train loss:0.0004577584946370198\n",
            "train loss:0.002577318940953206\n",
            "train loss:0.0006208009375453516\n",
            "train loss:0.001729844418607869\n",
            "train loss:0.0022049799045243787\n",
            "train loss:0.00397823861175939\n",
            "train loss:0.00033525900296541895\n",
            "train loss:0.004347447022612582\n",
            "train loss:0.0012179176073230467\n",
            "train loss:0.0007912684020719335\n",
            "train loss:0.012157705436622243\n",
            "train loss:0.0010889479679828825\n",
            "train loss:0.001862696692539034\n",
            "train loss:0.0010151562545050513\n",
            "train loss:0.006048598546856944\n",
            "train loss:0.006400473624472512\n",
            "train loss:0.008230318379867058\n",
            "train loss:0.0010659923633451922\n",
            "train loss:0.00018582869233563137\n",
            "train loss:0.001305379864358581\n",
            "train loss:0.0028180093028877884\n",
            "train loss:0.00045628642510437523\n",
            "train loss:0.0006954133220598331\n",
            "train loss:0.003022129270196447\n",
            "train loss:0.004814608052764829\n",
            "train loss:0.001300802005214275\n",
            "train loss:0.00943489092449226\n",
            "train loss:0.0016912585399023825\n",
            "train loss:0.0024538447631855297\n",
            "train loss:0.0024946688071564894\n",
            "train loss:0.003910363117812839\n",
            "train loss:0.0012930455997722224\n",
            "train loss:0.000737922475339763\n",
            "train loss:0.0071820591460250004\n",
            "train loss:8.125218339186003e-05\n",
            "train loss:0.0003217069304560038\n",
            "train loss:0.0016989067731871489\n",
            "train loss:0.003102488943031055\n",
            "train loss:0.00012667565594871448\n",
            "train loss:0.0012189464806692902\n",
            "train loss:0.00021519611484348364\n",
            "train loss:0.0007569698822630926\n",
            "train loss:0.001554153944200032\n",
            "train loss:0.00016737279542844321\n",
            "train loss:0.0005967499030149533\n",
            "train loss:0.0003709649381611954\n",
            "train loss:0.010894433620770926\n",
            "train loss:0.0010990894457840632\n",
            "train loss:0.0012625287617796797\n",
            "train loss:0.003535217619892811\n",
            "train loss:0.00022324880030835512\n",
            "train loss:0.002834904719870224\n",
            "train loss:0.003729670754897307\n",
            "train loss:0.000343473532554423\n",
            "train loss:0.0004893356609944223\n",
            "train loss:0.0010524993551391298\n",
            "train loss:0.0004391864155440096\n",
            "train loss:0.0038155064248085293\n",
            "train loss:4.500336539823694e-05\n",
            "train loss:0.0001975085702030303\n",
            "train loss:0.00018428914057969686\n",
            "train loss:0.0003206684874586855\n",
            "train loss:0.0013610332221474933\n",
            "train loss:0.0017028155129014799\n",
            "train loss:0.000629145697798124\n",
            "train loss:0.008963613782778606\n",
            "train loss:0.003174556568118404\n",
            "train loss:0.00012650451162040283\n",
            "train loss:0.002693041759887741\n",
            "train loss:0.002294799984419491\n",
            "train loss:0.0005342341059945251\n",
            "train loss:0.00012261503760125148\n",
            "train loss:0.00031236391715047803\n",
            "train loss:0.0008642992195852785\n",
            "train loss:0.0005486337041309749\n",
            "train loss:0.00011356980063271691\n",
            "train loss:0.002475443224166629\n",
            "train loss:0.0036325774446566653\n",
            "train loss:0.00032025192877207365\n",
            "train loss:0.00037092933192588574\n",
            "train loss:0.001476498722831024\n",
            "train loss:0.004137036505378704\n",
            "train loss:0.0022114381225347302\n",
            "train loss:0.0005045137391627674\n",
            "train loss:0.0029812789399701395\n",
            "train loss:0.0017209051434629826\n",
            "train loss:0.0005683069024861654\n",
            "train loss:0.0001472723994122654\n",
            "train loss:0.00022817707098606518\n",
            "train loss:0.0019105485271462387\n",
            "train loss:0.00013566679893216072\n",
            "train loss:0.003022760113678848\n",
            "train loss:0.0007340653631080019\n",
            "train loss:0.0004236045844705067\n",
            "train loss:0.00036147230162183726\n",
            "train loss:0.0016434251295472223\n",
            "train loss:0.0014837547195455589\n",
            "train loss:0.00024928726282061217\n",
            "train loss:0.0016379585461334656\n",
            "train loss:0.0002476012326174821\n",
            "train loss:0.0007081325696888623\n",
            "train loss:0.0005673431092066028\n",
            "train loss:0.0005368372731346347\n",
            "train loss:0.00044379767925413726\n",
            "train loss:0.011205211368456418\n",
            "train loss:0.0009606244065030939\n",
            "train loss:0.0024912707038450554\n",
            "train loss:0.0006043078526191497\n",
            "train loss:0.00055880852198542\n",
            "train loss:0.0010311974233466331\n",
            "train loss:0.002842669977683513\n",
            "train loss:0.0013311769786718262\n",
            "train loss:0.0002864259693182266\n",
            "train loss:0.0011210999327900839\n",
            "train loss:0.00010526285571930554\n",
            "train loss:0.000977427928412839\n",
            "train loss:0.0004209839487326156\n",
            "train loss:0.0010468475331975983\n",
            "train loss:0.000817006609814157\n",
            "train loss:0.0004410625545563002\n",
            "train loss:0.0010006185310467992\n",
            "train loss:0.0009037258793183849\n",
            "train loss:0.0033987661482364788\n",
            "train loss:0.00025246261876706624\n",
            "train loss:0.0005144637766913268\n",
            "train loss:0.0007131504279251231\n",
            "train loss:0.0019187024790834329\n",
            "train loss:0.013846786965060042\n",
            "train loss:0.0008801715793215337\n",
            "train loss:0.0003194654060948859\n",
            "train loss:0.0028700637166826156\n",
            "train loss:0.002630355220938353\n",
            "train loss:0.004568459840303509\n",
            "train loss:0.0013779294529141072\n",
            "train loss:0.033901433713522354\n",
            "train loss:0.0005059696927777745\n",
            "train loss:0.004128312144674682\n",
            "train loss:0.0006250297093011848\n",
            "train loss:0.004265142824302961\n",
            "train loss:0.0034052330270111703\n",
            "train loss:0.005886117470087454\n",
            "train loss:0.001348246161769072\n",
            "train loss:0.00432393058982845\n",
            "train loss:0.0009624599759759967\n",
            "train loss:0.001175353198095248\n",
            "train loss:0.0001264209283348572\n",
            "train loss:0.0009663237774270017\n",
            "train loss:0.0012015394731502574\n",
            "train loss:0.012147560925584435\n",
            "train loss:0.02173712264044899\n",
            "train loss:0.0005592435579023482\n",
            "train loss:0.001319600967807481\n",
            "train loss:0.004808017815754942\n",
            "train loss:0.0002408886655652528\n",
            "train loss:0.006249125750881727\n",
            "train loss:0.0007164896690681606\n",
            "train loss:0.00026566792428188223\n",
            "train loss:0.0014253653229055253\n",
            "train loss:0.001967007663469823\n",
            "train loss:0.0071598783068974005\n",
            "train loss:0.0020275394957705957\n",
            "train loss:0.0003554501292120056\n",
            "train loss:0.0008002555900803953\n",
            "train loss:0.00013459246771930377\n",
            "train loss:0.0008731739538073861\n",
            "train loss:0.00017264255544431257\n",
            "train loss:0.006116104841664051\n",
            "train loss:0.0007340806671808433\n",
            "train loss:0.0036304563628913994\n",
            "train loss:0.0014004616879770126\n",
            "train loss:0.0017963412723688054\n",
            "train loss:0.014986294025072965\n",
            "train loss:0.0023997911534885995\n",
            "train loss:0.0010552515171745268\n",
            "train loss:9.098809073403609e-05\n",
            "train loss:0.0034184258606068446\n",
            "train loss:0.0035461228718396665\n",
            "train loss:0.0008574314454969386\n",
            "train loss:0.00026025885355591445\n",
            "train loss:0.0009682891035204805\n",
            "train loss:0.0020596809559260427\n",
            "train loss:0.002062546570976568\n",
            "train loss:0.0013480967356123033\n",
            "train loss:0.0001352292218796749\n",
            "train loss:0.00536396831127607\n",
            "train loss:0.001010034988918168\n",
            "train loss:0.011425880275514489\n",
            "train loss:0.0008905970676565977\n",
            "train loss:0.004501144267674681\n",
            "train loss:0.00867728927088136\n",
            "train loss:0.0016583681891386849\n",
            "train loss:0.0012797085841466838\n",
            "train loss:9.810600738242563e-05\n",
            "train loss:0.0009004141754107736\n",
            "train loss:0.0001941394705010526\n",
            "train loss:0.001982910289846148\n",
            "train loss:0.005196796379325116\n",
            "train loss:0.000358942858408785\n",
            "train loss:0.0007489429807100277\n",
            "train loss:0.00040473050258814326\n",
            "train loss:0.0012077815201637868\n",
            "train loss:0.0003140025571795599\n",
            "train loss:0.011562953434932821\n",
            "train loss:0.0007196555062249526\n",
            "train loss:0.0005048511323694541\n",
            "train loss:0.002549004345376186\n",
            "train loss:0.0008366736305561395\n",
            "train loss:0.0014194925879910285\n",
            "train loss:0.0018897463335456868\n",
            "train loss:0.0006485118769696659\n",
            "train loss:0.00041798135205923256\n",
            "train loss:0.013589513095402737\n",
            "train loss:0.002117986487030145\n",
            "train loss:0.006300343908154133\n",
            "train loss:0.0007674042803866337\n",
            "train loss:0.00034900313880441893\n",
            "train loss:0.0019515410038746738\n",
            "train loss:0.001881309549383089\n",
            "train loss:0.003649467604623537\n",
            "train loss:0.000667843512657389\n",
            "train loss:0.007106008344296164\n",
            "train loss:0.0008815392403217372\n",
            "train loss:0.006525948694755327\n",
            "train loss:0.00025760759109477895\n",
            "train loss:0.003473943003985258\n",
            "train loss:0.002290876513863042\n",
            "train loss:0.00010512134590737255\n",
            "train loss:0.0011144223992974323\n",
            "train loss:9.426253330782836e-05\n",
            "train loss:0.0016050758083134451\n",
            "train loss:0.0003741824069470972\n",
            "train loss:0.00272346893433194\n",
            "train loss:0.0015806244264776948\n",
            "train loss:0.0014416696826489853\n",
            "train loss:0.000600992057544218\n",
            "train loss:0.002247541408470716\n",
            "train loss:0.0031109892866739153\n",
            "train loss:0.0017737343866278683\n",
            "train loss:0.0002074577366043625\n",
            "train loss:0.0025896974381897806\n",
            "train loss:0.0001682335410598357\n",
            "train loss:0.0012644471097465117\n",
            "train loss:0.00013223960043418255\n",
            "train loss:7.63591927037684e-05\n",
            "train loss:0.0011000855295402573\n",
            "train loss:0.004976154095103961\n",
            "train loss:0.0012337021676491673\n",
            "train loss:0.0001710336008344832\n",
            "train loss:0.0004785233852863389\n",
            "train loss:0.0005858432481335643\n",
            "train loss:0.010596315413731175\n",
            "train loss:0.002342442397917288\n",
            "train loss:0.007077504153898609\n",
            "train loss:0.0003048065136355133\n",
            "train loss:0.0003796235590269008\n",
            "train loss:0.003562800171753245\n",
            "train loss:0.0006272168896069134\n",
            "train loss:0.0017635932043477667\n",
            "train loss:0.002431360594756901\n",
            "train loss:0.0005269131659378209\n",
            "train loss:0.0004467892389418276\n",
            "train loss:0.005198212776949112\n",
            "train loss:0.005085637614482505\n",
            "train loss:0.0014364848338769478\n",
            "train loss:0.0013201932785466268\n",
            "train loss:0.00031619997547151034\n",
            "train loss:6.743328451450141e-05\n",
            "train loss:0.0019824367020894727\n",
            "train loss:0.0008615241916335656\n",
            "train loss:0.00011565026080253825\n",
            "train loss:0.001228464372079399\n",
            "train loss:0.0027488091280825936\n",
            "train loss:0.00020299379877648506\n",
            "train loss:9.33616000943791e-05\n",
            "train loss:0.00010396491672418995\n",
            "train loss:0.0012356327625304426\n",
            "train loss:0.002440278454729251\n",
            "train loss:0.001042108591025991\n",
            "train loss:0.0019692509737976598\n",
            "train loss:0.0016977402654573708\n",
            "train loss:0.0007371472439316783\n",
            "train loss:0.00018282242069353734\n",
            "train loss:0.005065089153721327\n",
            "train loss:0.00025207182783757847\n",
            "train loss:0.00012107381124351098\n",
            "train loss:0.00020627536134500775\n",
            "train loss:0.0013900502240537088\n",
            "train loss:0.0015719553279885073\n",
            "train loss:0.0027513166119315143\n",
            "train loss:0.00069663351314478\n",
            "train loss:0.0007526816249578202\n",
            "train loss:0.0008255374196300838\n",
            "train loss:0.002074623236760569\n",
            "train loss:0.0019263410302794287\n",
            "train loss:0.00012354920821791736\n",
            "train loss:0.003832605096290046\n",
            "train loss:0.0007413520853869085\n",
            "train loss:0.00029615363677429487\n",
            "train loss:0.0020870586116335517\n",
            "train loss:0.0002721961417019519\n",
            "train loss:0.0018884519193131213\n",
            "train loss:0.00017640467697395634\n",
            "train loss:0.0025017448203440056\n",
            "train loss:0.0018584698333679553\n",
            "train loss:0.0007377307530982068\n",
            "train loss:0.0001523844040147888\n",
            "train loss:0.000759463417844106\n",
            "train loss:0.0029288892331350807\n",
            "train loss:0.0020465677549579083\n",
            "train loss:0.013487879625788743\n",
            "train loss:0.0005552219027167621\n",
            "train loss:0.0010773162655276123\n",
            "train loss:0.008470655101561073\n",
            "train loss:0.0013575247206493004\n",
            "train loss:0.0015924764440239715\n",
            "train loss:0.0005921733327737082\n",
            "train loss:0.004978930596646937\n",
            "train loss:7.339654096371568e-05\n",
            "train loss:0.00019646403489015243\n",
            "train loss:0.001286798841346371\n",
            "train loss:0.0012652532242305645\n",
            "train loss:0.0019211966875820083\n",
            "train loss:0.004142061636571957\n",
            "train loss:0.0003536633631255802\n",
            "train loss:0.0008100622200104898\n",
            "train loss:0.0017241699483187794\n",
            "train loss:0.0015874905612883014\n",
            "train loss:0.00023025637513450153\n",
            "train loss:0.0004632457557468037\n",
            "train loss:0.0013327158422415835\n",
            "train loss:0.0033520556251881322\n",
            "train loss:0.0017457562750069688\n",
            "train loss:0.0036512127455022203\n",
            "train loss:0.005848775107971296\n",
            "train loss:0.0017752393545895306\n",
            "train loss:0.0036385630853536257\n",
            "train loss:0.0007511568600786564\n",
            "train loss:0.0012756376095976608\n",
            "train loss:0.0012852990799235298\n",
            "train loss:0.004245774268500799\n",
            "train loss:0.003986718381575934\n",
            "train loss:0.004251846162620816\n",
            "train loss:2.3595719282692015e-05\n",
            "train loss:0.0004587157423186205\n",
            "train loss:0.00011087262550962258\n",
            "train loss:0.004196453890063099\n",
            "train loss:0.00027273105048136834\n",
            "train loss:0.0008766667852334837\n",
            "train loss:0.011354869931672128\n",
            "train loss:0.005031851525829446\n",
            "train loss:0.0012649285923221926\n",
            "train loss:0.0008153979639364198\n",
            "train loss:0.0006336533097612384\n",
            "train loss:0.000799634645471868\n",
            "train loss:0.001013727975269259\n",
            "train loss:0.002540843621199308\n",
            "train loss:0.009065862417317863\n",
            "train loss:0.0012669686398341423\n",
            "train loss:0.00018827232159132374\n",
            "train loss:0.0051621138157815615\n",
            "train loss:0.0010597555394640653\n",
            "train loss:0.00565924343305294\n",
            "train loss:0.0018361549686690985\n",
            "train loss:0.001016138377212816\n",
            "train loss:0.0012671055483657937\n",
            "train loss:0.0009020090294115928\n",
            "train loss:0.0011709200505698697\n",
            "train loss:0.0029588379838709137\n",
            "train loss:0.00013439861692613004\n",
            "train loss:0.005653261719296491\n",
            "train loss:0.004205190933646041\n",
            "train loss:0.00047881542576004295\n",
            "train loss:0.007649196525067379\n",
            "train loss:2.12766769943992e-05\n",
            "train loss:0.0009902459630497945\n",
            "train loss:0.000487392388902088\n",
            "train loss:0.00016793516768133052\n",
            "train loss:0.0002673566146174792\n",
            "train loss:0.0004906910852602222\n",
            "train loss:0.0013322133715384069\n",
            "train loss:0.0004900613733575875\n",
            "train loss:0.0003140811407163468\n",
            "train loss:0.0013820107983488453\n",
            "train loss:0.0015291533852854191\n",
            "train loss:0.002112413022749213\n",
            "train loss:0.0009996439833107236\n",
            "train loss:0.0015080749902769896\n",
            "train loss:0.002543404239145932\n",
            "train loss:0.00012214052286867592\n",
            "train loss:0.0009418184487384982\n",
            "train loss:0.0032640555149273686\n",
            "train loss:0.0004697913807635652\n",
            "train loss:0.003762006407404547\n",
            "train loss:0.00022037547899775948\n",
            "train loss:0.001098129524381957\n",
            "train loss:0.001068254272074383\n",
            "train loss:0.003924706886378565\n",
            "train loss:0.006680007563626022\n",
            "train loss:0.0010579310555412142\n",
            "train loss:0.0018628951674362637\n",
            "train loss:6.176684882294152e-05\n",
            "train loss:0.0003054953195472294\n",
            "train loss:0.00439348885293165\n",
            "train loss:8.921931071858033e-05\n",
            "train loss:0.001973973645706071\n",
            "train loss:0.002340374855538619\n",
            "train loss:0.00047288225305708054\n",
            "train loss:0.00048024556695984216\n",
            "train loss:0.00044824903999430893\n",
            "train loss:0.00381161940798537\n",
            "train loss:0.00290533954090051\n",
            "train loss:0.0019213752648834968\n",
            "train loss:0.0015071553251950198\n",
            "train loss:0.010236770296145878\n",
            "train loss:0.0019378641178895222\n",
            "train loss:0.002972661002025801\n",
            "train loss:0.0024070925553693104\n",
            "train loss:0.0015258895040255092\n",
            "train loss:0.0006244066257651009\n",
            "train loss:0.0029432167528744297\n",
            "train loss:0.0025734321747715085\n",
            "train loss:0.0003525952129168674\n",
            "train loss:0.005935206054970276\n",
            "train loss:0.0005858769378305632\n",
            "train loss:0.0032781885193241926\n",
            "train loss:0.0023229681780668225\n",
            "train loss:0.0039012474677325958\n",
            "train loss:4.0711687170103165e-05\n",
            "train loss:0.003001280169889797\n",
            "train loss:0.0005689899185904001\n",
            "train loss:0.0065786912980539914\n",
            "train loss:0.0010624985753488886\n",
            "train loss:0.0032848722663764785\n",
            "train loss:0.012528101519242583\n",
            "train loss:0.0015023152965325315\n",
            "train loss:0.0024771240365575272\n",
            "train loss:0.0001448401683084521\n",
            "train loss:0.001008267876693443\n",
            "train loss:0.000903612473869228\n",
            "train loss:0.0010300444449333108\n",
            "train loss:0.0011795642820121813\n",
            "train loss:0.0020608396246663894\n",
            "train loss:0.00038210064500911666\n",
            "train loss:0.0007147791694439203\n",
            "train loss:0.0013087513266499442\n",
            "train loss:0.0008010710419398925\n",
            "train loss:0.008270448959832383\n",
            "train loss:0.0036044540899825782\n",
            "train loss:0.00832716755182652\n",
            "train loss:4.751421299559382e-05\n",
            "train loss:0.011132208926749366\n",
            "train loss:0.0001801032772404631\n",
            "train loss:0.038567464470931456\n",
            "train loss:0.00024121259897486703\n",
            "train loss:0.010433036908459392\n",
            "train loss:0.0030291252068564826\n",
            "train loss:0.000418733688284219\n",
            "train loss:0.00011551729923110212\n",
            "train loss:0.0011336784608995166\n",
            "train loss:0.0016555192162421625\n",
            "train loss:0.0009695628102976879\n",
            "train loss:0.0004453422443167893\n",
            "train loss:0.0008658052398219407\n",
            "train loss:0.017836856521774195\n",
            "train loss:0.0033841308737993657\n",
            "train loss:0.0006877411243123137\n",
            "train loss:0.0017633776053295814\n",
            "train loss:0.0016245899960412727\n",
            "train loss:0.002790001192831342\n",
            "train loss:0.0011380972668063245\n",
            "train loss:4.509576993934343e-05\n",
            "=== epoch:17, train acc:0.997, test acc:0.987 ===\n",
            "train loss:0.001496055783645514\n",
            "train loss:0.001222135486676531\n",
            "train loss:0.001731809359142794\n",
            "train loss:0.00591675450237184\n",
            "train loss:0.007241320989796859\n",
            "train loss:0.0030045390489257806\n",
            "train loss:0.00023075941423224045\n",
            "train loss:0.0011542850725687027\n",
            "train loss:0.0005000090871893914\n",
            "train loss:0.008083754040353356\n",
            "train loss:0.01511957507993126\n",
            "train loss:0.0016690547006887307\n",
            "train loss:0.011444137946528312\n",
            "train loss:0.004316015486295342\n",
            "train loss:0.013385607333372962\n",
            "train loss:0.0003399785681186989\n",
            "train loss:0.006524484501225004\n",
            "train loss:0.0005150027303758463\n",
            "train loss:0.00448003024708179\n",
            "train loss:0.002013340070919964\n",
            "train loss:0.00029658714614201295\n",
            "train loss:0.0012180513763279856\n",
            "train loss:0.002554778079221009\n",
            "train loss:0.0006169259686335216\n",
            "train loss:0.000635114976678725\n",
            "train loss:0.0008651735317014927\n",
            "train loss:0.003817662954227381\n",
            "train loss:0.0006261470441056646\n",
            "train loss:0.0023279919311244048\n",
            "train loss:8.410837315010045e-05\n",
            "train loss:0.01093713565523395\n",
            "train loss:0.00536087813717896\n",
            "train loss:0.003785640679459405\n",
            "train loss:0.0007763779203242541\n",
            "train loss:0.001536717096081141\n",
            "train loss:0.005146863064314798\n",
            "train loss:0.0008334930530262074\n",
            "train loss:0.004891610134212517\n",
            "train loss:0.004827918735606038\n",
            "train loss:0.001529892544831548\n",
            "train loss:0.004778382820707995\n",
            "train loss:0.001168013093839348\n",
            "train loss:0.0003854503338368892\n",
            "train loss:0.0006565281114977957\n",
            "train loss:0.0004969643405055441\n",
            "train loss:0.0033766374773940887\n",
            "train loss:0.0015703441137073844\n",
            "train loss:0.018007358993448633\n",
            "train loss:0.005145455043652544\n",
            "train loss:0.0061683401583711725\n",
            "train loss:0.00020301741603882237\n",
            "train loss:0.0012429628186247654\n",
            "train loss:0.0008532452649024528\n",
            "train loss:0.0013614375946979255\n",
            "train loss:0.005982067025103091\n",
            "train loss:0.002165116478075385\n",
            "train loss:0.0008076370872718748\n",
            "train loss:0.0119753321174815\n",
            "train loss:0.016266650714032983\n",
            "train loss:0.0006950105355828861\n",
            "train loss:0.007818821224140014\n",
            "train loss:0.010142795918924282\n",
            "train loss:0.002829586795912107\n",
            "train loss:0.003754264049230617\n",
            "train loss:0.0010302761923246433\n",
            "train loss:0.0028149381965910125\n",
            "train loss:0.0009597664573758825\n",
            "train loss:0.0005028634149662237\n",
            "train loss:0.004378953731225977\n",
            "train loss:0.0030136123151483633\n",
            "train loss:0.01819708786907948\n",
            "train loss:0.00032695671356023644\n",
            "train loss:0.007137008124581517\n",
            "train loss:0.002140434247789502\n",
            "train loss:0.0018236032299687934\n",
            "train loss:0.0007883972271639353\n",
            "train loss:0.0017687557385352966\n",
            "train loss:0.031131059170103562\n",
            "train loss:0.0011394596835214356\n",
            "train loss:0.0020330060539490145\n",
            "train loss:0.00038160375761494723\n",
            "train loss:0.0002717529128713199\n",
            "train loss:0.0045813601986493865\n",
            "train loss:0.006198692259176439\n",
            "train loss:0.005887578160639632\n",
            "train loss:0.00352642647977615\n",
            "train loss:0.008739043320455612\n",
            "train loss:0.014801095253184429\n",
            "train loss:0.0007241666842799873\n",
            "train loss:0.0010355499856913816\n",
            "train loss:0.002184597044744536\n",
            "train loss:0.009474454464895823\n",
            "train loss:0.002836389084165396\n",
            "train loss:5.3697914073804486e-05\n",
            "train loss:0.001024802698593675\n",
            "train loss:0.004251674824505345\n",
            "train loss:0.000494239342971247\n",
            "train loss:0.011226843346920956\n",
            "train loss:0.009104435065787769\n",
            "train loss:0.000406711729034961\n",
            "train loss:0.003936124107564038\n",
            "train loss:0.001223693874890555\n",
            "train loss:8.43414147010572e-05\n",
            "train loss:0.000367223497313436\n",
            "train loss:0.0010797298305762085\n",
            "train loss:0.0011960478275210814\n",
            "train loss:0.003522242502707869\n",
            "train loss:0.0020677695296963785\n",
            "train loss:9.58975912442896e-05\n",
            "train loss:0.0020598110728991094\n",
            "train loss:0.00040475083201128024\n",
            "train loss:0.0004226795806199398\n",
            "train loss:0.0020942354806314347\n",
            "train loss:0.0052371510757581495\n",
            "train loss:0.0016400489267869704\n",
            "train loss:0.0008524971955914386\n",
            "train loss:0.0006192161073288922\n",
            "train loss:0.009308897670382154\n",
            "train loss:0.004278091798615031\n",
            "train loss:0.0008175072965300286\n",
            "train loss:0.004062076922046024\n",
            "train loss:0.005250754710011551\n",
            "train loss:0.004328494708288061\n",
            "train loss:0.00141512722677179\n",
            "train loss:0.0021907184931111937\n",
            "train loss:0.0006336838563615343\n",
            "train loss:0.0008473469411281177\n",
            "train loss:0.0008606480591963197\n",
            "train loss:0.004265271645185443\n",
            "train loss:0.00027637615144208034\n",
            "train loss:0.028811702632113946\n",
            "train loss:0.003896467884051842\n",
            "train loss:0.003286752722392043\n",
            "train loss:0.0057513193763682425\n",
            "train loss:0.00632066478806999\n",
            "train loss:0.0012829584207730313\n",
            "train loss:0.00021083214698910328\n",
            "train loss:0.0020083675665044072\n",
            "train loss:0.01589477883777738\n",
            "train loss:0.012604116129660187\n",
            "train loss:0.0007147283180887795\n",
            "train loss:0.003948245584573005\n",
            "train loss:0.0035508120563348664\n",
            "train loss:0.00026550180579440073\n",
            "train loss:0.000510476537446996\n",
            "train loss:0.0008335873923587535\n",
            "train loss:0.007437842963765881\n",
            "train loss:0.0022490510744978885\n",
            "train loss:0.012995707197038874\n",
            "train loss:0.0018907068550969664\n",
            "train loss:0.008781903794442915\n",
            "train loss:0.0003015623129178135\n",
            "train loss:0.0022101406840422254\n",
            "train loss:0.004006533215001041\n",
            "train loss:0.016680953060296943\n",
            "train loss:0.004161790720024692\n",
            "train loss:0.0013825771969852333\n",
            "train loss:0.00025322469848478414\n",
            "train loss:0.001572942860238276\n",
            "train loss:0.00034565620331242116\n",
            "train loss:0.00017948197098353172\n",
            "train loss:0.00024599919428654654\n",
            "train loss:0.0012983296756927255\n",
            "train loss:0.0020233047701554738\n",
            "train loss:0.000580041805536643\n",
            "train loss:0.0005516595238443861\n",
            "train loss:0.00030294988163946516\n",
            "train loss:0.004515514439322409\n",
            "train loss:0.022944986268572106\n",
            "train loss:0.003020710732506892\n",
            "train loss:0.0033840263563941404\n",
            "train loss:0.00141418374696555\n",
            "train loss:8.427286524544754e-05\n",
            "train loss:0.0001932305748250835\n",
            "train loss:0.00028417995128441617\n",
            "train loss:0.0023829096801323405\n",
            "train loss:0.00041786851696623865\n",
            "train loss:0.004101805300339545\n",
            "train loss:0.00450908824390799\n",
            "train loss:0.0002960368292176689\n",
            "train loss:2.9985718189790375e-05\n",
            "train loss:0.005259236460527089\n",
            "train loss:0.0011685873673225825\n",
            "train loss:0.0006193605367183702\n",
            "train loss:0.0022158853505488928\n",
            "train loss:0.00027200771697236234\n",
            "train loss:0.00302788967383298\n",
            "train loss:0.0026375661977363117\n",
            "train loss:0.0032427630179889056\n",
            "train loss:0.0009040899967331966\n",
            "train loss:0.0007388850187731589\n",
            "train loss:0.0018151285500145908\n",
            "train loss:0.0004912897047088124\n",
            "train loss:0.00037699882195927346\n",
            "train loss:0.000498814753931377\n",
            "train loss:0.0007758452511452215\n",
            "train loss:0.004027120502314393\n",
            "train loss:0.0011963427097442143\n",
            "train loss:0.000470149577728906\n",
            "train loss:0.0024769006087729745\n",
            "train loss:0.0014892073802941649\n",
            "train loss:0.0009197554513988362\n",
            "train loss:0.0016255584839675748\n",
            "train loss:0.0004409127502547189\n",
            "train loss:0.0012626509176418346\n",
            "train loss:0.001288992372802292\n",
            "train loss:0.0018320456124251549\n",
            "train loss:0.015635859147717902\n",
            "train loss:0.015576312663261356\n",
            "train loss:0.00019182699730109474\n",
            "train loss:0.001592566181572234\n",
            "train loss:0.0006566550711463434\n",
            "train loss:0.0030015459722606687\n",
            "train loss:0.0002963452123587166\n",
            "train loss:0.004966626672919589\n",
            "train loss:0.0012934537600307388\n",
            "train loss:0.004318947105453944\n",
            "train loss:0.011425526386506036\n",
            "train loss:0.00020198906410274925\n",
            "train loss:0.0008275341029612898\n",
            "train loss:0.00011715609599415787\n",
            "train loss:0.0018418202658506571\n",
            "train loss:0.0003258759770440212\n",
            "train loss:0.0015136422956625142\n",
            "train loss:0.0005436276117106883\n",
            "train loss:0.00027546996566330716\n",
            "train loss:0.0010162883252532557\n",
            "train loss:0.00020779662035388634\n",
            "train loss:0.001794843258934855\n",
            "train loss:0.0017321694146421331\n",
            "train loss:0.0013826504483818232\n",
            "train loss:0.0012402516939642024\n",
            "train loss:0.0005211977292137716\n",
            "train loss:0.0015425099662287483\n",
            "train loss:0.0004642751276997699\n",
            "train loss:0.0024061597138758852\n",
            "train loss:0.000261375629563595\n",
            "train loss:0.0006175478639091034\n",
            "train loss:0.0016119542838255701\n",
            "train loss:0.00042811240163232616\n",
            "train loss:0.0007518204117087282\n",
            "train loss:7.906544123911591e-05\n",
            "train loss:0.0011925412547292129\n",
            "train loss:0.0008335424500878866\n",
            "train loss:0.0007911433752032622\n",
            "train loss:0.001607169196842366\n",
            "train loss:0.0017499140187841472\n",
            "train loss:0.0002620904227201642\n",
            "train loss:0.0005206903786947365\n",
            "train loss:0.00015423747985740402\n",
            "train loss:0.00037143266437088984\n",
            "train loss:0.0008747399790338647\n",
            "train loss:0.00014813638332710773\n",
            "train loss:0.0037150519644285657\n",
            "train loss:0.0045252728082656405\n",
            "train loss:0.0006368133860140437\n",
            "train loss:0.0007455656875122969\n",
            "train loss:0.0003010006124807629\n",
            "train loss:0.0005961329216446126\n",
            "train loss:0.00022814847080175775\n",
            "train loss:0.0004048561840464244\n",
            "train loss:0.0003289324138434203\n",
            "train loss:0.0011647286167370387\n",
            "train loss:0.000323270261384233\n",
            "train loss:5.513991924596802e-05\n",
            "train loss:0.001994525121514156\n",
            "train loss:0.0023090736270539674\n",
            "train loss:6.191766448995225e-05\n",
            "train loss:0.0015799502758791534\n",
            "train loss:0.0007765976909760239\n",
            "train loss:0.001984671334461134\n",
            "train loss:0.000565738600921733\n",
            "train loss:0.002876068668313656\n",
            "train loss:0.0021961502032445753\n",
            "train loss:9.561514701163918e-05\n",
            "train loss:0.00033481930705928293\n",
            "train loss:0.00029774781443320784\n",
            "train loss:3.0388430116716774e-05\n",
            "train loss:0.000779232820221958\n",
            "train loss:0.00023567743839293204\n",
            "train loss:0.00041171603784283153\n",
            "train loss:2.50066508426445e-05\n",
            "train loss:0.0004122538237820836\n",
            "train loss:0.0021500231800691758\n",
            "train loss:0.0012820847987787699\n",
            "train loss:0.000994128204608079\n",
            "train loss:0.00013058977781646506\n",
            "train loss:6.780655644476951e-05\n",
            "train loss:0.007609745471365837\n",
            "train loss:0.00048271985071101996\n",
            "train loss:0.0011708743935634105\n",
            "train loss:0.002823300792696611\n",
            "train loss:0.0024642737386713002\n",
            "train loss:0.0003311146317151679\n",
            "train loss:0.0002649246525416313\n",
            "train loss:0.00030169741211948326\n",
            "train loss:0.00034137255705496297\n",
            "train loss:0.0019079332738563479\n",
            "train loss:0.0008423452051655667\n",
            "train loss:0.004139458237804633\n",
            "train loss:9.067351152135003e-05\n",
            "train loss:0.001297208618237192\n",
            "train loss:0.0031409615947405023\n",
            "train loss:9.027965484388099e-05\n",
            "train loss:0.0005492752553295293\n",
            "train loss:7.623798940159622e-05\n",
            "train loss:0.003527342668435964\n",
            "train loss:0.004794321406054788\n",
            "train loss:0.002256354785340946\n",
            "train loss:0.002426694637096108\n",
            "train loss:0.0006392551520264387\n",
            "train loss:0.001926921839309038\n",
            "train loss:0.0002238317309925272\n",
            "train loss:0.0003278505212867158\n",
            "train loss:0.004383214869607462\n",
            "train loss:0.0022117062449076856\n",
            "train loss:0.001364253783334373\n",
            "train loss:0.0034891717920806998\n",
            "train loss:0.0004586602451101186\n",
            "train loss:5.636225364950832e-05\n",
            "train loss:0.002416182604760876\n",
            "train loss:0.002119972191242096\n",
            "train loss:0.0003119912748270277\n",
            "train loss:0.0006387022284878019\n",
            "train loss:0.0009004245113732394\n",
            "train loss:0.0002676768476062097\n",
            "train loss:0.001203458295778702\n",
            "train loss:0.0003481848587301533\n",
            "train loss:0.0001392035584642344\n",
            "train loss:0.0016437203980917932\n",
            "train loss:0.0004876635960921833\n",
            "train loss:0.0013713600719263115\n",
            "train loss:0.0002092606867646296\n",
            "train loss:0.0019554735940355305\n",
            "train loss:0.00011391117007836907\n",
            "train loss:0.000171590432125347\n",
            "train loss:0.00021681258187004345\n",
            "train loss:0.00021514422617699818\n",
            "train loss:0.0017895076500528534\n",
            "train loss:0.0018508992994213545\n",
            "train loss:0.00037240190706573786\n",
            "train loss:0.00017039892852958036\n",
            "train loss:0.0017162907446968021\n",
            "train loss:0.00025981652570314135\n",
            "train loss:0.00020822313773663717\n",
            "train loss:0.00025529528045913687\n",
            "train loss:0.00015807146166663076\n",
            "train loss:0.0011966520503675477\n",
            "train loss:0.0005847246667831771\n",
            "train loss:0.00046711875846459867\n",
            "train loss:0.0008164838012121719\n",
            "train loss:0.0018875191640418253\n",
            "train loss:0.0009438338215339684\n",
            "train loss:0.0020046184768758614\n",
            "train loss:0.000669757570609166\n",
            "train loss:0.0007549399496139277\n",
            "train loss:0.0045764161751908165\n",
            "train loss:0.0002811287645937089\n",
            "train loss:0.0026278615330050403\n",
            "train loss:0.003203215696223684\n",
            "train loss:0.0002464656972341232\n",
            "train loss:0.0056187595980105205\n",
            "train loss:0.00018608769359958525\n",
            "train loss:0.0014644519565080146\n",
            "train loss:0.0018372423287412506\n",
            "train loss:0.001435397716811034\n",
            "train loss:0.0009191201532916214\n",
            "train loss:0.001023377992981807\n",
            "train loss:0.0004543924714315333\n",
            "train loss:0.0003458550046171822\n",
            "train loss:0.000588131661155004\n",
            "train loss:0.0002827503854437967\n",
            "train loss:0.004847230311394965\n",
            "train loss:0.0018414966046231284\n",
            "train loss:0.00042073172804161526\n",
            "train loss:0.00026075543911004926\n",
            "train loss:0.020804211697499145\n",
            "train loss:0.0037295017032661455\n",
            "train loss:0.0027766531043378834\n",
            "train loss:0.0003053531333812304\n",
            "train loss:0.002125932718459938\n",
            "train loss:0.0014899867529259041\n",
            "train loss:0.005287690313958897\n",
            "train loss:0.0017661696633808534\n",
            "train loss:0.0011147830260561315\n",
            "train loss:0.0009064959195547365\n",
            "train loss:0.00022178156126635194\n",
            "train loss:0.0027444011331551666\n",
            "train loss:0.0022916757199471687\n",
            "train loss:0.0005027257916318015\n",
            "train loss:5.466301687269354e-05\n",
            "train loss:0.009456722190431805\n",
            "train loss:0.0018440312600141373\n",
            "train loss:0.0017733072168626674\n",
            "train loss:0.001011806993654378\n",
            "train loss:0.0006043382487183984\n",
            "train loss:0.0001005481291080524\n",
            "train loss:0.01655167131083457\n",
            "train loss:0.00044617080832067246\n",
            "train loss:0.004826895260780912\n",
            "train loss:0.012039269671248777\n",
            "train loss:0.00021383399661680346\n",
            "train loss:0.0004286229378528866\n",
            "train loss:0.001282652256154656\n",
            "train loss:0.00044731136790749574\n",
            "train loss:0.0033482767284479186\n",
            "train loss:0.0038075202371526946\n",
            "train loss:0.0007900457806167058\n",
            "train loss:0.002932677026129578\n",
            "train loss:0.0004609637538257089\n",
            "train loss:0.0009826124395731477\n",
            "train loss:0.0009287588032421565\n",
            "train loss:9.052722620001031e-05\n",
            "train loss:0.0006470875209997659\n",
            "train loss:0.0018102796856652053\n",
            "train loss:0.023540119710484277\n",
            "train loss:0.0003295715831727812\n",
            "train loss:0.0044350546363722786\n",
            "train loss:0.00036371688247728096\n",
            "train loss:0.0034681028908254263\n",
            "train loss:0.0007908046086770563\n",
            "train loss:0.004424562330300746\n",
            "train loss:0.0012440541315693117\n",
            "train loss:0.00638641022776591\n",
            "train loss:0.00028680899236496575\n",
            "train loss:0.003353551273842363\n",
            "train loss:0.000518953830031314\n",
            "train loss:0.005064216958666836\n",
            "train loss:0.0002661990805592057\n",
            "train loss:0.0018824361970846953\n",
            "train loss:0.0070483004441139405\n",
            "train loss:0.00044879496008541164\n",
            "train loss:0.0009044419781433468\n",
            "train loss:0.0007055512311878836\n",
            "train loss:0.0008402359204030334\n",
            "train loss:0.003702182645184335\n",
            "train loss:0.00223213670524671\n",
            "train loss:0.0034602734309818976\n",
            "train loss:0.004300298721695253\n",
            "train loss:0.0005760689665352688\n",
            "train loss:0.004601301273005826\n",
            "train loss:0.0003480786584189938\n",
            "train loss:0.014726854294882021\n",
            "train loss:0.000851289190528293\n",
            "train loss:7.463946497727558e-05\n",
            "train loss:0.0010387713953270942\n",
            "train loss:0.002514104652402431\n",
            "train loss:0.0012577665438345492\n",
            "train loss:0.0003984067699701706\n",
            "train loss:0.0013136502680430138\n",
            "train loss:0.001378552528563233\n",
            "train loss:0.0017028470633716737\n",
            "train loss:0.0004505864791398288\n",
            "train loss:0.00011859589487905866\n",
            "train loss:0.00038046064841514905\n",
            "train loss:0.00018864399904195987\n",
            "train loss:0.0011131471629774545\n",
            "train loss:0.002221294202288643\n",
            "train loss:0.0008910500750698581\n",
            "train loss:0.004460609624127272\n",
            "train loss:0.0001612476130240311\n",
            "train loss:0.003808925961958591\n",
            "train loss:0.00038809065096238757\n",
            "train loss:0.0018455531241939032\n",
            "train loss:0.0024016236296312827\n",
            "train loss:0.0018661853622179512\n",
            "train loss:0.002760395686088006\n",
            "train loss:0.002450035440319158\n",
            "train loss:0.0015034161354244757\n",
            "train loss:0.01687814697076435\n",
            "train loss:0.0016985065600831687\n",
            "train loss:0.00569224759069399\n",
            "train loss:0.0011899503817077803\n",
            "train loss:0.00032592254116667595\n",
            "train loss:0.002813742052780996\n",
            "train loss:0.00013259629062754627\n",
            "train loss:0.0029803260811736274\n",
            "train loss:0.001408856020863585\n",
            "train loss:0.003786369023975734\n",
            "train loss:0.0003357783107892824\n",
            "train loss:0.007348551386646487\n",
            "train loss:0.007786333379956537\n",
            "train loss:0.0041899516010172584\n",
            "train loss:0.00034479481570675994\n",
            "train loss:0.0009012125717565581\n",
            "train loss:0.0017661006938414518\n",
            "train loss:0.003369189804070172\n",
            "train loss:0.004808318520324781\n",
            "train loss:0.013379250677527772\n",
            "train loss:0.004897044141272779\n",
            "train loss:0.00288106161828391\n",
            "train loss:0.002268346703042859\n",
            "train loss:0.012143123031880566\n",
            "train loss:0.0002338748359661438\n",
            "train loss:0.0018691890587282252\n",
            "train loss:0.002758017209505848\n",
            "train loss:0.0008900891092982439\n",
            "train loss:0.002639793005812417\n",
            "train loss:0.004139572724356754\n",
            "train loss:0.0008365137168522957\n",
            "train loss:0.0036008267414819857\n",
            "train loss:0.0001070986810328989\n",
            "train loss:0.00026425580592430545\n",
            "train loss:0.0014267599137834882\n",
            "train loss:0.001092841747577861\n",
            "train loss:0.002868960548803139\n",
            "train loss:7.615703969122966e-05\n",
            "train loss:0.006062340444431173\n",
            "train loss:0.0012008879913983027\n",
            "train loss:0.023719493949644867\n",
            "train loss:0.000995399371837775\n",
            "train loss:0.0006787447485718782\n",
            "train loss:0.0032930352703300987\n",
            "train loss:0.00036075871023601993\n",
            "train loss:0.0025246788124613936\n",
            "train loss:0.0015517937553446615\n",
            "train loss:0.035874563277739\n",
            "train loss:4.051198475061376e-05\n",
            "train loss:0.0005357244572810777\n",
            "train loss:0.00034458404996064873\n",
            "train loss:0.00028804245544019117\n",
            "train loss:0.0013255528017811244\n",
            "train loss:0.007061694985649755\n",
            "train loss:0.0004732724766970806\n",
            "train loss:0.002570216915993207\n",
            "train loss:0.0025365915287979906\n",
            "train loss:0.004615981654924016\n",
            "train loss:0.0067765477627051785\n",
            "train loss:0.003969747025466773\n",
            "train loss:0.00215728974270186\n",
            "train loss:0.00048523237052839415\n",
            "train loss:0.0024674042105343975\n",
            "train loss:0.003094225115258219\n",
            "train loss:0.0002979812883816157\n",
            "train loss:0.002657248807426256\n",
            "train loss:0.002326181644419226\n",
            "train loss:0.001391770550289824\n",
            "train loss:0.0010705658420761517\n",
            "train loss:0.0024258633138916834\n",
            "train loss:0.00769801204943056\n",
            "train loss:0.00823946026010155\n",
            "train loss:0.0031236085803805413\n",
            "train loss:0.015294323530927877\n",
            "train loss:0.019688142707321027\n",
            "train loss:0.0013234200353939452\n",
            "train loss:0.0021314183090372645\n",
            "train loss:0.0001952027282398194\n",
            "train loss:0.0021668519166488124\n",
            "train loss:0.002183596195635165\n",
            "train loss:0.008067904183246759\n",
            "train loss:0.002027667261218828\n",
            "train loss:0.00403600612074573\n",
            "train loss:0.0025930165098454992\n",
            "train loss:0.00011619666520774383\n",
            "train loss:0.001597265412667973\n",
            "train loss:0.002521910738108533\n",
            "train loss:0.002653844378485517\n",
            "train loss:0.004835725315744306\n",
            "train loss:0.0034610723314283985\n",
            "train loss:0.0055112396659742794\n",
            "train loss:8.328062776679364e-05\n",
            "train loss:0.001144567783216964\n",
            "train loss:0.0002663136948604616\n",
            "train loss:0.003356850167667486\n",
            "train loss:0.0001850139328052794\n",
            "train loss:5.0925120350494854e-05\n",
            "train loss:9.965057367584966e-05\n",
            "train loss:0.00058365715884124\n",
            "train loss:0.0004755065242470263\n",
            "train loss:0.0074692589850119916\n",
            "train loss:0.0023211945163955595\n",
            "train loss:0.0026098929164140128\n",
            "train loss:3.309684668264074e-05\n",
            "train loss:0.000358676172785501\n",
            "train loss:0.0023560853110393336\n",
            "train loss:4.709185709893839e-05\n",
            "train loss:0.0004067713845560284\n",
            "train loss:2.1462412263345995e-05\n",
            "train loss:0.002898764976694618\n",
            "train loss:0.002495797510711904\n",
            "train loss:0.0004404200961328488\n",
            "train loss:0.0002153043016562596\n",
            "train loss:0.00016864731063115975\n",
            "train loss:0.0002648947714074887\n",
            "train loss:0.000679929900601311\n",
            "train loss:0.0015735508024298267\n",
            "train loss:0.0026047146044243007\n",
            "train loss:0.002246833688754367\n",
            "train loss:0.00027666521360708205\n",
            "train loss:0.0005542144212352396\n",
            "train loss:0.0001177500103956443\n",
            "train loss:0.0018453627758596128\n",
            "train loss:0.008225883061319833\n",
            "train loss:0.00018036903859068847\n",
            "train loss:0.002501998698493475\n",
            "train loss:0.0005831953757710073\n",
            "train loss:0.0002909764510164723\n",
            "train loss:0.00105879576979405\n",
            "train loss:0.0009489104860659473\n",
            "train loss:0.0019933054169823743\n",
            "=== epoch:18, train acc:1.0, test acc:0.981 ===\n",
            "train loss:0.0009202501226416812\n",
            "train loss:0.0018490905637599418\n",
            "train loss:0.0005568561994344992\n",
            "train loss:0.0017681609695392217\n",
            "train loss:0.003057450661045773\n",
            "train loss:0.0012105571258445808\n",
            "train loss:0.0008635881333878535\n",
            "train loss:0.004565083951235922\n",
            "train loss:0.0007305284451998404\n",
            "train loss:0.0014026688067880892\n",
            "train loss:0.0012705332629089963\n",
            "train loss:0.002338855506393217\n",
            "train loss:0.000894549391576824\n",
            "train loss:0.00019480594326915643\n",
            "train loss:0.002474523248728177\n",
            "train loss:0.0005109975866814698\n",
            "train loss:0.004770368802344172\n",
            "train loss:0.006074302562623401\n",
            "train loss:0.0001840314711819372\n",
            "train loss:0.0003879035031304521\n",
            "train loss:0.0003760774428536798\n",
            "train loss:0.0023869571006892867\n",
            "train loss:0.002064772116746404\n",
            "train loss:0.002341198187810743\n",
            "train loss:0.0007040068483065829\n",
            "train loss:0.0015615587727140676\n",
            "train loss:0.002634130154137921\n",
            "train loss:0.0021235388246157723\n",
            "train loss:0.0016327914383875294\n",
            "train loss:0.0014205511618843563\n",
            "train loss:0.0011578502157788735\n",
            "train loss:0.001230653473710472\n",
            "train loss:0.00015204113379990063\n",
            "train loss:0.0007540637823607821\n",
            "train loss:0.0003257541864638172\n",
            "train loss:0.0015634291410649937\n",
            "train loss:0.0004326018158514977\n",
            "train loss:0.0011092503919291517\n",
            "train loss:0.001067253278733681\n",
            "train loss:0.00044476716123954243\n",
            "train loss:0.0017425795317602686\n",
            "train loss:0.001019191303843014\n",
            "train loss:0.0019540758113737225\n",
            "train loss:0.0025523143041788314\n",
            "train loss:0.0027784227786594173\n",
            "train loss:0.00041111620206830507\n",
            "train loss:0.0005972899945729367\n",
            "train loss:0.0006846165125693239\n",
            "train loss:3.728849861822794e-05\n",
            "train loss:0.0014120269365997201\n",
            "train loss:0.0006861786726012214\n",
            "train loss:0.0031836691134274225\n",
            "train loss:0.000947117798933899\n",
            "train loss:0.001596413937027435\n",
            "train loss:9.285424401005996e-05\n",
            "train loss:0.011680005622786871\n",
            "train loss:0.0024461744047689898\n",
            "train loss:0.0011039014728147544\n",
            "train loss:0.00017775114577333633\n",
            "train loss:0.001974413518473375\n",
            "train loss:7.413026412826103e-05\n",
            "train loss:0.00024466935170111234\n",
            "train loss:0.0012027210903984159\n",
            "train loss:0.009758034071621395\n",
            "train loss:9.649397971131873e-05\n",
            "train loss:0.004438394280109237\n",
            "train loss:0.0007571163225082805\n",
            "train loss:0.0024316983326623334\n",
            "train loss:0.0011399783642881919\n",
            "train loss:0.00023808771809654495\n",
            "train loss:0.0008878488784267378\n",
            "train loss:0.0010500873615690014\n",
            "train loss:0.000809223937524966\n",
            "train loss:0.0024873083633360003\n",
            "train loss:0.004149290216890199\n",
            "train loss:0.00034533312790013045\n",
            "train loss:0.00017473523371760633\n",
            "train loss:0.004406695013097515\n",
            "train loss:0.0021943036011442037\n",
            "train loss:0.00017625482327723597\n",
            "train loss:0.014263377304962497\n",
            "train loss:0.001445973009271082\n",
            "train loss:0.00041263791881748703\n",
            "train loss:0.0016832560648000158\n",
            "train loss:0.001114635600999498\n",
            "train loss:0.00212926771577682\n",
            "train loss:0.0005928282702152706\n",
            "train loss:0.00024391204157305998\n",
            "train loss:0.001606476114011412\n",
            "train loss:0.001745303522272117\n",
            "train loss:0.00014298426421056456\n",
            "train loss:0.002587645514905538\n",
            "train loss:0.0017842962735562199\n",
            "train loss:0.0006042440448348085\n",
            "train loss:0.0018082245813844286\n",
            "train loss:0.0006322400624858125\n",
            "train loss:0.002585347454629659\n",
            "train loss:0.0007744352820667132\n",
            "train loss:0.0011843748497720153\n",
            "train loss:0.0005962010083207104\n",
            "train loss:0.0019286349141881024\n",
            "train loss:0.00016377204236012272\n",
            "train loss:0.00011962282580377241\n",
            "train loss:0.0030027906047800246\n",
            "train loss:0.0008573071808668322\n",
            "train loss:0.0028682194224878966\n",
            "train loss:0.0028117636920298246\n",
            "train loss:0.002951022202950626\n",
            "train loss:0.047466000304581014\n",
            "train loss:0.0005502189174364222\n",
            "train loss:2.6317690600049436e-05\n",
            "train loss:0.013751885299281768\n",
            "train loss:6.232360510807181e-05\n",
            "train loss:0.00012992749363107707\n",
            "train loss:0.0028868624278469324\n",
            "train loss:0.0015070796478117018\n",
            "train loss:0.007966755379437983\n",
            "train loss:0.0005386203526190938\n",
            "train loss:2.811756873425249e-05\n",
            "train loss:0.0011629360258555805\n",
            "train loss:0.0005785637091660821\n",
            "train loss:0.003275093925314028\n",
            "train loss:0.00281126982753621\n",
            "train loss:0.0010874442313895227\n",
            "train loss:0.004235708977376867\n",
            "train loss:0.000808695511888145\n",
            "train loss:0.001013601024695928\n",
            "train loss:0.0007021605489624124\n",
            "train loss:0.0009841901214994217\n",
            "train loss:0.00025796997360972044\n",
            "train loss:0.0013289241788313377\n",
            "train loss:0.002143293680255842\n",
            "train loss:0.001716919275948904\n",
            "train loss:0.0013983648560801452\n",
            "train loss:0.00042066706752572097\n",
            "train loss:0.0034439657065082722\n",
            "train loss:0.00018795949397669196\n",
            "train loss:5.404165049410333e-05\n",
            "train loss:0.0028390720002570338\n",
            "train loss:0.0026348602904009938\n",
            "train loss:0.0004371112650714271\n",
            "train loss:0.002619612411660197\n",
            "train loss:0.001598049026074617\n",
            "train loss:0.0010686724750069692\n",
            "train loss:0.0009118543334193977\n",
            "train loss:0.001466826310760924\n",
            "train loss:0.001137526701846616\n",
            "train loss:0.0007282044108029305\n",
            "train loss:0.002306743239611723\n",
            "train loss:0.0019018766901481984\n",
            "train loss:0.001526758869486911\n",
            "train loss:0.001160742710350603\n",
            "train loss:0.00030092566266173626\n",
            "train loss:0.0002273636687324283\n",
            "train loss:0.00010298636644473107\n",
            "train loss:6.59129436230845e-05\n",
            "train loss:8.657322194937717e-05\n",
            "train loss:0.0006654230828387277\n",
            "train loss:0.00019121209824934753\n",
            "train loss:0.0012291385516516127\n",
            "train loss:0.0012453926635337008\n",
            "train loss:0.00014279462363639916\n",
            "train loss:0.00077132238657262\n",
            "train loss:0.00011270730781074085\n",
            "train loss:0.0003226832799165214\n",
            "train loss:0.00018119837543821162\n",
            "train loss:0.0012591630965803904\n",
            "train loss:0.0008452040350743434\n",
            "train loss:0.003027895268442119\n",
            "train loss:0.0005991941995001116\n",
            "train loss:0.00011887585284227189\n",
            "train loss:0.0024572304466051755\n",
            "train loss:0.0006903699707545871\n",
            "train loss:0.0015647756748891363\n",
            "train loss:0.00039311266086040415\n",
            "train loss:0.0010721909516016352\n",
            "train loss:3.146289752383544e-05\n",
            "train loss:0.0014225379890857794\n",
            "train loss:0.0002496968813492413\n",
            "train loss:0.0002117600216756943\n",
            "train loss:0.0017022357734140913\n",
            "train loss:0.0011631795191563304\n",
            "train loss:0.002905160847615444\n",
            "train loss:0.0003155136183063032\n",
            "train loss:0.00036905301121517727\n",
            "train loss:0.0003326595225597002\n",
            "train loss:0.0006184205291981299\n",
            "train loss:0.00015745433732242446\n",
            "train loss:0.0006534259831869649\n",
            "train loss:0.000743817392285461\n",
            "train loss:0.0004782387503511135\n",
            "train loss:0.0005305706062188771\n",
            "train loss:0.0007492831555305932\n",
            "train loss:7.891998579064216e-05\n",
            "train loss:0.004383725832619851\n",
            "train loss:0.001137824565182671\n",
            "train loss:0.0019995318950511275\n",
            "train loss:0.0005920870361226268\n",
            "train loss:0.00011443622874555918\n",
            "train loss:0.0006213912710003004\n",
            "train loss:0.00033063199600554655\n",
            "train loss:0.00030070091548457264\n",
            "train loss:0.0005650281509853365\n",
            "train loss:0.000849458790042318\n",
            "train loss:0.0012544304992669678\n",
            "train loss:0.0011460053999961678\n",
            "train loss:3.7231257073706794e-05\n",
            "train loss:0.0002473498692245654\n",
            "train loss:9.807885308365785e-05\n",
            "train loss:0.0014016274675642995\n",
            "train loss:0.0015970133462671545\n",
            "train loss:0.0004401164047661356\n",
            "train loss:0.00017380909283340974\n",
            "train loss:0.0004861259111943362\n",
            "train loss:3.520197781622924e-05\n",
            "train loss:0.0006283298166542402\n",
            "train loss:0.00017552566827695054\n",
            "train loss:0.0010269025157127571\n",
            "train loss:0.0003110295797787696\n",
            "train loss:0.00025409828826429033\n",
            "train loss:2.6865313440434202e-05\n",
            "train loss:0.0021395099518679074\n",
            "train loss:0.0007891947862868347\n",
            "train loss:0.001942964180096698\n",
            "train loss:0.0004525999364757276\n",
            "train loss:0.0004113527416897562\n",
            "train loss:0.0023961868268033014\n",
            "train loss:0.00015972142073353908\n",
            "train loss:0.005475204705400718\n",
            "train loss:0.00036473158634631235\n",
            "train loss:0.0003184228720280223\n",
            "train loss:0.00033436734873132086\n",
            "train loss:0.0004013760348671458\n",
            "train loss:0.0011471394955627503\n",
            "train loss:0.0017699374608992227\n",
            "train loss:0.001235047794819636\n",
            "train loss:0.0003061735313288582\n",
            "train loss:0.000823178386157634\n",
            "train loss:0.0007509952149764473\n",
            "train loss:0.0002930869117178869\n",
            "train loss:0.0005919022490620429\n",
            "train loss:0.0003708119655377325\n",
            "train loss:0.0010338013114056873\n",
            "train loss:0.00015518212389536296\n",
            "train loss:7.58029370329042e-05\n",
            "train loss:0.0019762366836749805\n",
            "train loss:0.00021115419297757944\n",
            "train loss:0.0005867299412979158\n",
            "train loss:0.0006493368734662433\n",
            "train loss:0.0007642439870407833\n",
            "train loss:5.684450617620548e-05\n",
            "train loss:6.423487483756584e-05\n",
            "train loss:0.0003388036882879508\n",
            "train loss:0.0002487998193266497\n",
            "train loss:0.001692975805493359\n",
            "train loss:0.0028945566907805464\n",
            "train loss:0.0023483164577663028\n",
            "train loss:1.6040419941421256e-05\n",
            "train loss:0.0006728624618733943\n",
            "train loss:0.0002523557210739703\n",
            "train loss:0.0009725548212847052\n",
            "train loss:0.00027044600643278056\n",
            "train loss:0.0008858225893450212\n",
            "train loss:0.000124384573343156\n",
            "train loss:3.928714361099944e-05\n",
            "train loss:0.0013726813569388946\n",
            "train loss:0.0007204593951918358\n",
            "train loss:0.0015592930043104105\n",
            "train loss:0.00024390136764752937\n",
            "train loss:0.0032953350684149425\n",
            "train loss:0.00017971237664749236\n",
            "train loss:2.3553974210632927e-05\n",
            "train loss:0.0015412844293609404\n",
            "train loss:0.001008783618172917\n",
            "train loss:0.00014804046650584597\n",
            "train loss:0.0016136253803465434\n",
            "train loss:0.00029347666586773815\n",
            "train loss:0.00010586652777950806\n",
            "train loss:6.254597831180809e-05\n",
            "train loss:6.1001080103484874e-05\n",
            "train loss:0.0013043725431224843\n",
            "train loss:0.0013883519554202877\n",
            "train loss:0.0001967829104792708\n",
            "train loss:0.0006485926873744672\n",
            "train loss:0.0015365022722556685\n",
            "train loss:7.84745394002378e-05\n",
            "train loss:0.001394250678910883\n",
            "train loss:0.00028993757133008414\n",
            "train loss:0.0011194189263719902\n",
            "train loss:0.0016356622056654011\n",
            "train loss:0.00043529381472512196\n",
            "train loss:0.0008018374486552553\n",
            "train loss:0.0003230836474864059\n",
            "train loss:0.0001834142032417948\n",
            "train loss:0.001224144671891114\n",
            "train loss:8.72365862891324e-05\n",
            "train loss:0.0006038004098468989\n",
            "train loss:0.0004673112007072506\n",
            "train loss:0.00024057719915981856\n",
            "train loss:0.0006214790393121189\n",
            "train loss:0.00044367722691713625\n",
            "train loss:0.001461951817238764\n",
            "train loss:0.00015524212065624844\n",
            "train loss:0.0009128961608740065\n",
            "train loss:0.00026130172123000494\n",
            "train loss:0.002030657438793712\n",
            "train loss:0.002554087348031226\n",
            "train loss:0.0005841786501114457\n",
            "train loss:4.595548295405177e-05\n",
            "train loss:0.0021555881533531517\n",
            "train loss:7.990831320200026e-05\n",
            "train loss:0.0001400373104630214\n",
            "train loss:0.0008243928303887565\n",
            "train loss:0.0020408370206321256\n",
            "train loss:0.00026300605395968105\n",
            "train loss:0.0027366314939310894\n",
            "train loss:0.0002558786983926604\n",
            "train loss:0.00019138636075305938\n",
            "train loss:0.0008091381963998938\n",
            "train loss:0.0011086527804982185\n",
            "train loss:0.00010314139891753752\n",
            "train loss:0.001154343155200841\n",
            "train loss:0.0012546977355216437\n",
            "train loss:7.175891359661798e-05\n",
            "train loss:0.00030179386768810525\n",
            "train loss:0.00010219114542586949\n",
            "train loss:0.001845847868628815\n",
            "train loss:0.0005797994301987086\n",
            "train loss:0.0010709446065237155\n",
            "train loss:0.0031792027585812845\n",
            "train loss:0.001717498386901954\n",
            "train loss:0.00019790514037416825\n",
            "train loss:0.0002692602894650844\n",
            "train loss:0.0001575096095715905\n",
            "train loss:0.00011140802889729157\n",
            "train loss:0.0001432727199028666\n",
            "train loss:0.00021524544910978254\n",
            "train loss:0.0001675335442804006\n",
            "train loss:0.0006121665492850424\n",
            "train loss:7.690620653267506e-05\n",
            "train loss:0.00025505937616591333\n",
            "train loss:0.00013065740789851234\n",
            "train loss:0.00010935672602795613\n",
            "train loss:0.00017434773652957885\n",
            "train loss:0.010061161377672843\n",
            "train loss:5.177827604413729e-05\n",
            "train loss:0.0015363037876129878\n",
            "train loss:0.00045866002183243473\n",
            "train loss:0.00017964736267477718\n",
            "train loss:0.00021935870727006297\n",
            "train loss:0.0015995175987147494\n",
            "train loss:0.0008193577736969714\n",
            "train loss:0.0001601018213935502\n",
            "train loss:0.05765006932543196\n",
            "train loss:0.0021562880380581605\n",
            "train loss:0.0004009049382884867\n",
            "train loss:0.00634879351162647\n",
            "train loss:0.0038871129520821044\n",
            "train loss:8.152229244330399e-05\n",
            "train loss:6.708360326293485e-05\n",
            "train loss:0.00019377933237595796\n",
            "train loss:0.0007150888009598601\n",
            "train loss:0.000857402243698664\n",
            "train loss:0.00023766315872340203\n",
            "train loss:0.0003626481252786487\n",
            "train loss:0.0016682274723592307\n",
            "train loss:0.0013507084876794502\n",
            "train loss:0.001681427169448434\n",
            "train loss:0.005447920176308599\n",
            "train loss:0.004087248606422736\n",
            "train loss:0.00015859260836306605\n",
            "train loss:0.00210428598730353\n",
            "train loss:0.0022980596352901806\n",
            "train loss:0.0009730805495179131\n",
            "train loss:0.0012354880127340806\n",
            "train loss:0.0002081553052177696\n",
            "train loss:0.0005455684575814348\n",
            "train loss:0.0010081243010157687\n",
            "train loss:0.0016445842463082571\n",
            "train loss:0.00035127882236726826\n",
            "train loss:0.004619259130309201\n",
            "train loss:0.002530519883068996\n",
            "train loss:0.0005270797106843994\n",
            "train loss:0.0007847177831026917\n",
            "train loss:0.0004027330359442407\n",
            "train loss:0.00045394804997072284\n",
            "train loss:0.000990329552316829\n",
            "train loss:0.0009468508148305059\n",
            "train loss:0.0003632261172520148\n",
            "train loss:0.004059360155733033\n",
            "train loss:0.010029779106917736\n",
            "train loss:0.0008297434757634512\n",
            "train loss:0.0022358767176395623\n",
            "train loss:0.021512718822306338\n",
            "train loss:0.002949448442483254\n",
            "train loss:0.0025835144750158993\n",
            "train loss:0.0028840369146884404\n",
            "train loss:0.0004928077462252999\n",
            "train loss:0.00656477171494759\n",
            "train loss:0.006709146294978657\n",
            "train loss:0.00047223237745625153\n",
            "train loss:0.0032668137042091527\n",
            "train loss:0.0015692781584020724\n",
            "train loss:0.0007587766259501942\n",
            "train loss:0.0015480818721479985\n",
            "train loss:0.00610398535774045\n",
            "train loss:0.0010482498278250514\n",
            "train loss:0.004309955669367491\n",
            "train loss:0.0007384668308414622\n",
            "train loss:0.0005495063883167753\n",
            "train loss:0.016468262270403114\n",
            "train loss:0.0026198254087701094\n",
            "train loss:0.0032307208409932393\n",
            "train loss:0.0009214074668185064\n",
            "train loss:0.03677058004199648\n",
            "train loss:0.00030636849180598955\n",
            "train loss:0.00030706237264815765\n",
            "train loss:0.0011935714489763874\n",
            "train loss:0.010057336915351982\n",
            "train loss:0.0019961253904067842\n",
            "train loss:0.00026173953132906157\n",
            "train loss:0.00028751120135440814\n",
            "train loss:0.003224827073636616\n",
            "train loss:0.0043458853391230194\n",
            "train loss:0.0008164612603458292\n",
            "train loss:0.006883111754355893\n",
            "train loss:0.0019155704817190463\n",
            "train loss:0.018888341534798455\n",
            "train loss:0.0005624801148288502\n",
            "train loss:0.0038480045786968998\n",
            "train loss:0.0003999559875024024\n",
            "train loss:0.0028417745157223073\n",
            "train loss:0.005075908637835595\n",
            "train loss:0.0012873736111115547\n",
            "train loss:0.0025291968299770216\n",
            "train loss:0.0030011462352757478\n",
            "train loss:0.0019241282634037307\n",
            "train loss:0.0016965706069425727\n",
            "train loss:0.0011573231553105977\n",
            "train loss:0.00012749550819199926\n",
            "train loss:0.0009404277958563563\n",
            "train loss:0.0009593542604016131\n",
            "train loss:0.0007566489761903959\n",
            "train loss:0.002166602092897116\n",
            "train loss:0.002057965723434181\n",
            "train loss:0.007585149809160689\n",
            "train loss:0.0006093905386285866\n",
            "train loss:0.0038514079891812693\n",
            "train loss:0.0010068360842035407\n",
            "train loss:0.004500834762655298\n",
            "train loss:0.0011743469576591323\n",
            "train loss:0.003507802824125794\n",
            "train loss:0.01718726033169871\n",
            "train loss:0.0004913660885661385\n",
            "train loss:0.0010395563493091346\n",
            "train loss:0.00501281362176354\n",
            "train loss:0.0012964852581052152\n",
            "train loss:0.0035579196893163996\n",
            "train loss:0.0019261706230980808\n",
            "train loss:0.0585925749462259\n",
            "train loss:0.00010057303046398638\n",
            "train loss:0.0005570951892064925\n",
            "train loss:0.0016360496309993022\n",
            "train loss:0.00015731765145952765\n",
            "train loss:0.003202978121979572\n",
            "train loss:0.0048030943412038654\n",
            "train loss:0.0008699479017018484\n",
            "train loss:0.0004132911791948593\n",
            "train loss:0.0017740258895426145\n",
            "train loss:0.00014564058875004727\n",
            "train loss:0.0016971454010006794\n",
            "train loss:0.0011885979786371975\n",
            "train loss:0.0041617967239647\n",
            "train loss:0.0011055742032004099\n",
            "train loss:0.0020058879807235406\n",
            "train loss:0.002613094221844692\n",
            "train loss:0.0005023168809545561\n",
            "train loss:0.0025238306131870757\n",
            "train loss:0.0017464648729160056\n",
            "train loss:0.0006112213952311276\n",
            "train loss:6.703132845979138e-06\n",
            "train loss:0.0026980058925849476\n",
            "train loss:0.00010789670470926631\n",
            "train loss:0.001761389520790094\n",
            "train loss:0.0013860276281643777\n",
            "train loss:0.0017962296371184336\n",
            "train loss:0.0006120806911960613\n",
            "train loss:0.00025791936912911433\n",
            "train loss:0.0009657518415263971\n",
            "train loss:0.00021181512261318782\n",
            "train loss:0.004584561661384489\n",
            "train loss:0.0007978151638639527\n",
            "train loss:0.003948162892121575\n",
            "train loss:0.007925591445287017\n",
            "train loss:0.0013903964872243842\n",
            "train loss:0.004855663274254212\n",
            "train loss:8.545739434144385e-05\n",
            "train loss:0.0020179238517397903\n",
            "train loss:0.0012322093720035245\n",
            "train loss:0.003221294092026267\n",
            "train loss:0.0016881631133162716\n",
            "train loss:0.0020946084920661044\n",
            "train loss:0.0004949667339541681\n",
            "train loss:0.0015317886570184892\n",
            "train loss:0.0005757411533265517\n",
            "train loss:0.0065982827016369495\n",
            "train loss:0.0009142648506106335\n",
            "train loss:4.975282812253877e-05\n",
            "train loss:0.00023044974058605402\n",
            "train loss:0.0006560299346999696\n",
            "train loss:0.00013697400352163518\n",
            "train loss:0.0001597031799658375\n",
            "train loss:0.001507955515598274\n",
            "train loss:0.003495134635817306\n",
            "train loss:0.001705537512357949\n",
            "train loss:0.0003937847943503877\n",
            "train loss:3.105575380052512e-05\n",
            "train loss:0.001429604070380066\n",
            "train loss:0.0004445867724746989\n",
            "train loss:0.000164021033748099\n",
            "train loss:0.0006197799558120843\n",
            "train loss:0.0030022479043547456\n",
            "train loss:7.385674863817462e-05\n",
            "train loss:0.005333766390462647\n",
            "train loss:0.003399342389976374\n",
            "train loss:0.00011811076580976057\n",
            "train loss:0.0019367454479557128\n",
            "train loss:0.00014310936261583175\n",
            "train loss:0.0019240661400511754\n",
            "train loss:0.0001610901828931365\n",
            "train loss:0.00011358012192115909\n",
            "train loss:0.002051711027994563\n",
            "train loss:0.0022050692110806404\n",
            "train loss:0.0010351395032417094\n",
            "train loss:0.0001667198196069169\n",
            "train loss:0.0032872284662739983\n",
            "train loss:0.0021943673935341163\n",
            "train loss:0.0008761190477089092\n",
            "train loss:0.0012709623295722716\n",
            "train loss:0.003479193666281523\n",
            "train loss:0.0005451146584947242\n",
            "train loss:0.0013415259458972961\n",
            "train loss:0.005071371271246846\n",
            "train loss:0.0008799317463192988\n",
            "train loss:0.00017713105152005238\n",
            "train loss:0.0010935979283909609\n",
            "train loss:0.00025519196471248876\n",
            "train loss:0.0014244566338082615\n",
            "train loss:0.0017773952370016219\n",
            "train loss:0.00024296342493348652\n",
            "train loss:0.0017993028716700226\n",
            "train loss:0.001788098769101881\n",
            "train loss:0.00109479969332132\n",
            "train loss:0.0005590167278827544\n",
            "train loss:0.0012148143016498998\n",
            "train loss:8.097470313002174e-05\n",
            "train loss:0.000513599820470704\n",
            "train loss:0.003216513035382556\n",
            "train loss:0.0016414265393660352\n",
            "train loss:0.0011623503138716489\n",
            "train loss:0.0010760290052145912\n",
            "train loss:0.004828665626059528\n",
            "train loss:0.0003503954597567152\n",
            "train loss:0.004518833361156707\n",
            "train loss:0.0007062104913962727\n",
            "train loss:0.0002791578634566737\n",
            "train loss:0.0021447852423348217\n",
            "train loss:0.0011779632152998265\n",
            "train loss:0.000684790096882308\n",
            "train loss:0.0002194656335274266\n",
            "train loss:9.851678289884759e-05\n",
            "train loss:0.004859792017340955\n",
            "train loss:0.0015784686641891086\n",
            "train loss:0.000758421755793656\n",
            "train loss:0.003140747018957357\n",
            "train loss:0.0001765217120486485\n",
            "train loss:0.0008971044061699906\n",
            "train loss:0.00016741848505529366\n",
            "train loss:0.00037269243814801997\n",
            "train loss:0.0009051073852423594\n",
            "train loss:0.001556060651188699\n",
            "train loss:9.928863145862705e-05\n",
            "train loss:0.0025340233583063666\n",
            "train loss:0.0005301524591457479\n",
            "train loss:0.00033514094456190834\n",
            "train loss:0.0015465746884488528\n",
            "train loss:0.00014729084090087997\n",
            "train loss:0.036514667093533244\n",
            "train loss:0.0005911385775825274\n",
            "train loss:0.0011062319907599314\n",
            "train loss:0.0011319705463631393\n",
            "train loss:8.52594138608219e-05\n",
            "train loss:0.0008467496826051944\n",
            "train loss:0.0007634620682106787\n",
            "train loss:0.0007629696851144864\n",
            "train loss:0.00026029482533081996\n",
            "train loss:0.004296753092490261\n",
            "train loss:0.001460300200017793\n",
            "train loss:0.0004183284565148767\n",
            "train loss:0.025478012586322166\n",
            "=== epoch:19, train acc:0.997, test acc:0.986 ===\n",
            "train loss:0.0014941951641229757\n",
            "train loss:0.00041242352486628024\n",
            "train loss:0.0017632100783088545\n",
            "train loss:0.000751401771293208\n",
            "train loss:0.007889167604584666\n",
            "train loss:0.00021898911450872417\n",
            "train loss:0.0012517640247365783\n",
            "train loss:0.00030865426823811255\n",
            "train loss:6.0679269645739845e-05\n",
            "train loss:0.005032980185640572\n",
            "train loss:0.000434886199369462\n",
            "train loss:0.0016513988560810345\n",
            "train loss:0.00029809959902388406\n",
            "train loss:0.000121492176596992\n",
            "train loss:0.001352661283856942\n",
            "train loss:0.001292857942807771\n",
            "train loss:0.0018076089358974653\n",
            "train loss:0.0005472264605018007\n",
            "train loss:0.002452901815536518\n",
            "train loss:0.00011594500550810153\n",
            "train loss:0.0016271672462832229\n",
            "train loss:0.0020914055620577594\n",
            "train loss:0.0013068051630564913\n",
            "train loss:0.0003382033329962964\n",
            "train loss:0.002877004676530151\n",
            "train loss:0.00021531238838235652\n",
            "train loss:0.00016539464901620227\n",
            "train loss:0.0035720434302429055\n",
            "train loss:0.0025807391269426234\n",
            "train loss:0.000457103906251144\n",
            "train loss:0.0013993581747625801\n",
            "train loss:0.0005256906310748101\n",
            "train loss:0.001596620552537676\n",
            "train loss:0.0009391008953117724\n",
            "train loss:8.413272047989615e-05\n",
            "train loss:0.00010809436135796345\n",
            "train loss:0.000602174818135599\n",
            "train loss:0.00019639970938331447\n",
            "train loss:0.00013237969683677103\n",
            "train loss:0.0001118154940851976\n",
            "train loss:0.0007502398467847525\n",
            "train loss:0.00041114711153168206\n",
            "train loss:0.0009697119946177242\n",
            "train loss:0.0026287778335053313\n",
            "train loss:0.0008800637520549331\n",
            "train loss:0.0019361859278050838\n",
            "train loss:0.000651936491136434\n",
            "train loss:0.0004973862342315824\n",
            "train loss:0.00018864676347575757\n",
            "train loss:0.0024709776040339982\n",
            "train loss:0.001832291459653314\n",
            "train loss:0.0007426407868024553\n",
            "train loss:0.002673023697691521\n",
            "train loss:0.0013069958668198017\n",
            "train loss:0.0004088701613952808\n",
            "train loss:7.214535666261425e-05\n",
            "train loss:0.008338704505371837\n",
            "train loss:0.0003750256504707956\n",
            "train loss:8.872424595449787e-05\n",
            "train loss:0.0032471281520891865\n",
            "train loss:0.0007613631625486819\n",
            "train loss:0.00038623857485521715\n",
            "train loss:0.0002156859613449549\n",
            "train loss:7.561442909970927e-05\n",
            "train loss:0.0009065339013556653\n",
            "train loss:0.0018249851752013544\n",
            "train loss:0.0008580054924079677\n",
            "train loss:0.0001435591831920176\n",
            "train loss:0.0031948105843344655\n",
            "train loss:0.00046047155687596174\n",
            "train loss:0.00027403582998504277\n",
            "train loss:0.0010983200025454893\n",
            "train loss:0.0011980206719053958\n",
            "train loss:0.0006302905199419142\n",
            "train loss:0.00019929170616344417\n",
            "train loss:0.0005240102635870106\n",
            "train loss:0.0016040994685482465\n",
            "train loss:0.0010707122461221794\n",
            "train loss:0.00036730218525075816\n",
            "train loss:0.0006915422705978921\n",
            "train loss:0.001198407395048891\n",
            "train loss:0.0009221073588280682\n",
            "train loss:0.016393190726819645\n",
            "train loss:0.0005594647219313304\n",
            "train loss:0.0022814852947623246\n",
            "train loss:0.0008841900198209176\n",
            "train loss:0.000508047090668337\n",
            "train loss:0.0006044505693744066\n",
            "train loss:6.610102321029345e-05\n",
            "train loss:0.002487902340073504\n",
            "train loss:0.0004986640493125087\n",
            "train loss:0.002027952704100063\n",
            "train loss:0.0002865906109099089\n",
            "train loss:0.001039011209526043\n",
            "train loss:0.0036373415870090287\n",
            "train loss:0.0003472524276444963\n",
            "train loss:0.0022681399377344237\n",
            "train loss:0.0005090446834932241\n",
            "train loss:0.0015696067981103025\n",
            "train loss:0.0026482847379631622\n",
            "train loss:0.0030403279264693966\n",
            "train loss:4.538905567307333e-05\n",
            "train loss:9.758503228780068e-05\n",
            "train loss:0.00017823942768942756\n",
            "train loss:0.0007398571615213049\n",
            "train loss:0.00071782578067064\n",
            "train loss:0.00026830099941304534\n",
            "train loss:0.0007421184370947338\n",
            "train loss:0.0006293438409331427\n",
            "train loss:0.0003969872975596106\n",
            "train loss:0.0028364224959023265\n",
            "train loss:0.0018839103496911319\n",
            "train loss:4.6382743840932484e-05\n",
            "train loss:0.000530265315900508\n",
            "train loss:0.004438031523095925\n",
            "train loss:0.0005860383729756524\n",
            "train loss:0.0005083001987448733\n",
            "train loss:0.00010926774181047899\n",
            "train loss:0.0004885761798874361\n",
            "train loss:0.0012333145779720542\n",
            "train loss:0.004514527201351103\n",
            "train loss:0.0006693121905204047\n",
            "train loss:0.0002896789140178973\n",
            "train loss:0.0023813401315700504\n",
            "train loss:0.0014364453818187385\n",
            "train loss:0.00189656020606985\n",
            "train loss:0.0006908826318840902\n",
            "train loss:0.0009240756523485495\n",
            "train loss:5.050432629566092e-05\n",
            "train loss:0.0001324468270264616\n",
            "train loss:0.00030725404691989426\n",
            "train loss:0.0019861648030596764\n",
            "train loss:2.1654076086403928e-05\n",
            "train loss:0.0006927457687070336\n",
            "train loss:0.002243303316429827\n",
            "train loss:0.0003720642251133539\n",
            "train loss:0.0014998733143881276\n",
            "train loss:0.001609810439988208\n",
            "train loss:0.00235141075758048\n",
            "train loss:0.000472189392363255\n",
            "train loss:0.008968550602857233\n",
            "train loss:0.00010648533513836398\n",
            "train loss:0.00012384298262836015\n",
            "train loss:5.0210225526048413e-05\n",
            "train loss:0.00045298652011515684\n",
            "train loss:0.0006066506876007268\n",
            "train loss:0.005767278288118874\n",
            "train loss:0.0013435847042371982\n",
            "train loss:0.00018685703392605643\n",
            "train loss:0.0002715236996901366\n",
            "train loss:0.007783506794611332\n",
            "train loss:0.012106510017026893\n",
            "train loss:0.0016580710593912637\n",
            "train loss:0.0004792048469990952\n",
            "train loss:0.004058114923816121\n",
            "train loss:0.0015107470570661957\n",
            "train loss:0.0006050412890528885\n",
            "train loss:0.004004600489931379\n",
            "train loss:0.0043034317755327334\n",
            "train loss:0.0008012506282584135\n",
            "train loss:0.003408035923265113\n",
            "train loss:0.00017606788945103338\n",
            "train loss:0.0005906072034672259\n",
            "train loss:0.0003987349424933211\n",
            "train loss:0.0003587423096117157\n",
            "train loss:0.0009352646726696725\n",
            "train loss:0.001590261539331219\n",
            "train loss:0.0028742344120223653\n",
            "train loss:0.0018850871484754719\n",
            "train loss:0.00038135834661749365\n",
            "train loss:0.00863643670152862\n",
            "train loss:0.0034688378265413503\n",
            "train loss:0.0006462424919850159\n",
            "train loss:0.006476767924261432\n",
            "train loss:0.0018345637246751057\n",
            "train loss:0.0014186279137888243\n",
            "train loss:0.001050143493421962\n",
            "train loss:0.0011827758198560058\n",
            "train loss:0.0005716887296905128\n",
            "train loss:0.0002813792420202493\n",
            "train loss:0.00039852730923882604\n",
            "train loss:0.005762278580263351\n",
            "train loss:0.0025357448731901616\n",
            "train loss:0.001483209799730542\n",
            "train loss:0.00021093181628093822\n",
            "train loss:0.00010322526230403497\n",
            "train loss:0.00084223395173364\n",
            "train loss:0.0010251014550792276\n",
            "train loss:0.0005164692555964561\n",
            "train loss:0.0018073226578150573\n",
            "train loss:0.0007583204461669632\n",
            "train loss:0.0005394404875822891\n",
            "train loss:0.0005671969819227023\n",
            "train loss:0.004348779900170861\n",
            "train loss:0.0003332432895527745\n",
            "train loss:0.0005917375490750201\n",
            "train loss:0.0012911121739705195\n",
            "train loss:0.0019301216252490814\n",
            "train loss:0.0010770162773511716\n",
            "train loss:0.0006332567488324187\n",
            "train loss:0.0013439877041662115\n",
            "train loss:0.00012361522283870302\n",
            "train loss:0.0009084391598726879\n",
            "train loss:0.002144599517946417\n",
            "train loss:0.0006051285833646533\n",
            "train loss:0.0010295016316005203\n",
            "train loss:0.0001250832855910101\n",
            "train loss:0.00014961909377295967\n",
            "train loss:0.0021000060999759873\n",
            "train loss:9.310054118098079e-05\n",
            "train loss:0.0006115244862991204\n",
            "train loss:0.0009194476094932118\n",
            "train loss:0.002599014294760053\n",
            "train loss:0.00033259423646608144\n",
            "train loss:0.00018694662211020678\n",
            "train loss:0.0008519333097220228\n",
            "train loss:0.00030846787496713085\n",
            "train loss:0.002818323460934305\n",
            "train loss:0.0011035156694541538\n",
            "train loss:0.00015088360824656985\n",
            "train loss:0.0003754669581855102\n",
            "train loss:0.001273022275610253\n",
            "train loss:0.0016746542212315433\n",
            "train loss:0.000944307992134826\n",
            "train loss:0.0018538630020915412\n",
            "train loss:3.1555081504794936e-05\n",
            "train loss:0.029636959897482953\n",
            "train loss:0.0032137199852055776\n",
            "train loss:0.006085664967173817\n",
            "train loss:0.003658260828410046\n",
            "train loss:0.00030766943154649336\n",
            "train loss:0.0001572855116406363\n",
            "train loss:0.00045585769826202713\n",
            "train loss:0.00036463525248960445\n",
            "train loss:0.00018147498252299815\n",
            "train loss:0.00024436457042471187\n",
            "train loss:0.0010004534008574784\n",
            "train loss:0.00022143440739381146\n",
            "train loss:0.0018599296496002921\n",
            "train loss:0.0005527228203675651\n",
            "train loss:0.0003099622056861906\n",
            "train loss:0.0017999688467116278\n",
            "train loss:0.00013609513943558645\n",
            "train loss:0.0009526610652030717\n",
            "train loss:0.0011882315179503342\n",
            "train loss:6.145793015172058e-05\n",
            "train loss:0.00010060605416964382\n",
            "train loss:0.0008760183800168909\n",
            "train loss:0.00024755836178078495\n",
            "train loss:0.0008853200585756195\n",
            "train loss:0.00098672514119886\n",
            "train loss:0.0012004539845485022\n",
            "train loss:0.005743873635249964\n",
            "train loss:0.00011140146811260787\n",
            "train loss:0.0021867320997739133\n",
            "train loss:4.4345843993714025e-05\n",
            "train loss:0.0007507508405404883\n",
            "train loss:0.0024560310967002073\n",
            "train loss:0.007706348250982769\n",
            "train loss:8.102627678874002e-05\n",
            "train loss:0.028298628812120034\n",
            "train loss:0.005014422822198796\n",
            "train loss:0.00034764935344476245\n",
            "train loss:4.632309531619262e-05\n",
            "train loss:0.001239273861068361\n",
            "train loss:7.577419153074435e-05\n",
            "train loss:0.0003200893517936307\n",
            "train loss:0.00010802753117887802\n",
            "train loss:0.014196526827549981\n",
            "train loss:0.002908795281811036\n",
            "train loss:0.0007323486805797564\n",
            "train loss:0.0009997802012091305\n",
            "train loss:0.004608406794270409\n",
            "train loss:0.0005252958921067083\n",
            "train loss:0.0005832257439932625\n",
            "train loss:5.972095581951672e-05\n",
            "train loss:0.0006369469700466435\n",
            "train loss:0.000283946581541897\n",
            "train loss:9.10813717116871e-05\n",
            "train loss:0.003081759601944777\n",
            "train loss:0.0013582258930903866\n",
            "train loss:0.0015919674322368562\n",
            "train loss:0.00022934761036264592\n",
            "train loss:0.0007451356391886639\n",
            "train loss:0.01870801114934636\n",
            "train loss:0.0012761605386875992\n",
            "train loss:4.4189629822354745e-05\n",
            "train loss:0.0033043049901700355\n",
            "train loss:7.342295882695315e-05\n",
            "train loss:0.00015358011995727675\n",
            "train loss:0.0034992735949818526\n",
            "train loss:0.0015990324868151245\n",
            "train loss:0.003100302294392335\n",
            "train loss:0.0013050955544128106\n",
            "train loss:0.005294802657275585\n",
            "train loss:0.004000435475491761\n",
            "train loss:0.010716664654830503\n",
            "train loss:0.0019060438946347797\n",
            "train loss:0.0008639739236805878\n",
            "train loss:0.0008310755188472677\n",
            "train loss:0.000919245827637174\n",
            "train loss:0.005411698626489558\n",
            "train loss:0.0008106507357749719\n",
            "train loss:0.0030983443799541864\n",
            "train loss:0.0042355199521577295\n",
            "train loss:0.0007152410307898678\n",
            "train loss:0.007932873163971869\n",
            "train loss:0.003735899757092625\n",
            "train loss:0.0007880082916515589\n",
            "train loss:0.0029668459343025278\n",
            "train loss:0.006151287675903682\n",
            "train loss:0.0006622648207442653\n",
            "train loss:0.000592144153075704\n",
            "train loss:0.001518544808013874\n",
            "train loss:0.022553746949277382\n",
            "train loss:0.0014953676145938544\n",
            "train loss:0.0011646033484080376\n",
            "train loss:0.0018867503358358769\n",
            "train loss:0.00023399919269372738\n",
            "train loss:0.00020321712391876078\n",
            "train loss:0.0008703244211586195\n",
            "train loss:0.0007827183859542816\n",
            "train loss:0.001430288436820627\n",
            "train loss:0.0027879682289247087\n",
            "train loss:0.0005344720896126957\n",
            "train loss:0.0003650013795391281\n",
            "train loss:0.0017893396551029988\n",
            "train loss:0.0018792619902169069\n",
            "train loss:8.708243230240647e-05\n",
            "train loss:0.00037422933048520903\n",
            "train loss:0.002553773722930366\n",
            "train loss:0.0032349406092780364\n",
            "train loss:0.0002632485774346272\n",
            "train loss:0.00206423909916674\n",
            "train loss:0.0038988424426466676\n",
            "train loss:0.0003392275703069762\n",
            "train loss:0.0002993259974955563\n",
            "train loss:0.0020908259084669372\n",
            "train loss:0.0010616558746459578\n",
            "train loss:0.0007722470310005319\n",
            "train loss:0.00011544728468193209\n",
            "train loss:0.0002781691285621245\n",
            "train loss:0.00036903015856357656\n",
            "train loss:0.0036054180066022994\n",
            "train loss:0.000745084198660605\n",
            "train loss:0.0003632453819284038\n",
            "train loss:0.0005607664544740355\n",
            "train loss:0.0018694006366146156\n",
            "train loss:0.0007071059843911236\n",
            "train loss:0.00014444224461401475\n",
            "train loss:0.004659179803688696\n",
            "train loss:0.017650447956314333\n",
            "train loss:0.003742752409179747\n",
            "train loss:0.00017555724774987198\n",
            "train loss:0.0005146865793385929\n",
            "train loss:0.000393333644335178\n",
            "train loss:0.0011328509709413522\n",
            "train loss:0.0013772149899143452\n",
            "train loss:0.0003860877557087837\n",
            "train loss:0.007094493373246311\n",
            "train loss:0.0006808661691255179\n",
            "train loss:0.0003256882480698361\n",
            "train loss:0.0026924338560439355\n",
            "train loss:0.0016872739745179368\n",
            "train loss:0.0012013166087233846\n",
            "train loss:0.0038776381059779717\n",
            "train loss:0.00030211482136617345\n",
            "train loss:0.003898325070357298\n",
            "train loss:0.0003332845143194297\n",
            "train loss:0.0011465295642981458\n",
            "train loss:0.00040032073176408565\n",
            "train loss:0.003209918600378769\n",
            "train loss:0.003093662210544422\n",
            "train loss:0.005404341858408638\n",
            "train loss:0.002139419912346178\n",
            "train loss:0.0008518188938032597\n",
            "train loss:0.00010037499374514741\n",
            "train loss:0.0022963276220173854\n",
            "train loss:0.0013057054315743421\n",
            "train loss:0.001694671673940538\n",
            "train loss:0.0007313538247099449\n",
            "train loss:0.0002957944442721098\n",
            "train loss:0.001536362643715164\n",
            "train loss:0.0009986835683874426\n",
            "train loss:0.0009167966188735606\n",
            "train loss:0.0009923117444068225\n",
            "train loss:2.661578192841955e-05\n",
            "train loss:0.0021187511273194504\n",
            "train loss:3.5507993642452434e-05\n",
            "train loss:0.0005286176337385526\n",
            "train loss:7.932467211511788e-05\n",
            "train loss:3.3505798523572945e-06\n",
            "train loss:0.0008036368518295052\n",
            "train loss:0.0005071893506641679\n",
            "train loss:0.0009005762912944377\n",
            "train loss:9.838636217350694e-05\n",
            "train loss:0.0005502495788427994\n",
            "train loss:0.00039788463373797544\n",
            "train loss:0.0009550371525421424\n",
            "train loss:0.001606146405776501\n",
            "train loss:0.0016329661792944868\n",
            "train loss:5.167931372287222e-05\n",
            "train loss:0.0017104947211940933\n",
            "train loss:0.0006464328079960181\n",
            "train loss:0.016153755228098764\n",
            "train loss:0.012693049742432794\n",
            "train loss:0.002028874051467081\n",
            "train loss:6.951247554847594e-05\n",
            "train loss:0.00015604094761034106\n",
            "train loss:0.00021073038368770347\n",
            "train loss:0.00028727259521797217\n",
            "train loss:0.003766365762366517\n",
            "train loss:0.00019803401651918078\n",
            "train loss:0.00016476492071602515\n",
            "train loss:0.00032334260511658673\n",
            "train loss:0.012103841358075337\n",
            "train loss:0.00023459306454376365\n",
            "train loss:0.002459620822364522\n",
            "train loss:0.0017044336257010504\n",
            "train loss:0.000693818678525681\n",
            "train loss:0.00023073620289235585\n",
            "train loss:0.0015963053509470276\n",
            "train loss:0.00044583856188510815\n",
            "train loss:0.0009378690922256703\n",
            "train loss:0.0011223540625829133\n",
            "train loss:0.00011538399871877115\n",
            "train loss:0.0011414973300353382\n",
            "train loss:0.0002310344846338027\n",
            "train loss:7.87064985172352e-05\n",
            "train loss:0.001708207194784957\n",
            "train loss:0.00019543569905828287\n",
            "train loss:0.0013985968655243776\n",
            "train loss:0.00017547134053374298\n",
            "train loss:0.0022848916830020076\n",
            "train loss:0.0009868269136274147\n",
            "train loss:0.00045348876645478527\n",
            "train loss:0.0003797473811769196\n",
            "train loss:0.0018437140876321574\n",
            "train loss:0.0021899737982999812\n",
            "train loss:0.0013070156131100947\n",
            "train loss:0.0003124840836041132\n",
            "train loss:0.005828163826305905\n",
            "train loss:0.00016552052826538985\n",
            "train loss:0.0025330998299378034\n",
            "train loss:0.00496672870548117\n",
            "train loss:0.004116680834238456\n",
            "train loss:0.000181149249765794\n",
            "train loss:0.00015879617513527114\n",
            "train loss:0.0032311561393651624\n",
            "train loss:0.0005659722521883813\n",
            "train loss:0.0023058256165512255\n",
            "train loss:0.0003984904576941103\n",
            "train loss:0.0013248654086085137\n",
            "train loss:0.001351466919162327\n",
            "train loss:0.00011164043997517331\n",
            "train loss:0.0022453112066740024\n",
            "train loss:0.0007949177150874158\n",
            "train loss:0.0005491623268256572\n",
            "train loss:0.0003370217751625005\n",
            "train loss:0.00010042851190121721\n",
            "train loss:0.0014116144176125574\n",
            "train loss:0.0025782081302579123\n",
            "train loss:0.0013466015621502673\n",
            "train loss:0.004444400895606885\n",
            "train loss:0.00038251939631956625\n",
            "train loss:0.002008814545332464\n",
            "train loss:0.002446329327306601\n",
            "train loss:0.0002173294556747658\n",
            "train loss:0.0007392707141117899\n",
            "train loss:0.0003056199484144201\n",
            "train loss:0.0005155278590084929\n",
            "train loss:0.00012723125903838975\n",
            "train loss:0.002063367791863825\n",
            "train loss:0.00043633864417398876\n",
            "train loss:0.0003421511451402538\n",
            "train loss:0.0002107886392061227\n",
            "train loss:7.593329329169646e-05\n",
            "train loss:0.0005841667551886151\n",
            "train loss:0.005267478807738233\n",
            "train loss:0.0005339696779822612\n",
            "train loss:0.0013414978477589894\n",
            "train loss:0.00046259959501323\n",
            "train loss:0.000826546961983302\n",
            "train loss:0.00029856988677952844\n",
            "train loss:0.012754317208000831\n",
            "train loss:0.013932717788665252\n",
            "train loss:0.003459572864268058\n",
            "train loss:0.0005983309362487945\n",
            "train loss:0.00020513109844905923\n",
            "train loss:0.0002491111603428653\n",
            "train loss:0.0006466147064930511\n",
            "train loss:0.000780252660588966\n",
            "train loss:0.008767028997796715\n",
            "train loss:0.0005087490616430307\n",
            "train loss:0.0009009880818256734\n",
            "train loss:5.0374969912240816e-05\n",
            "train loss:0.0016791365158711916\n",
            "train loss:7.298659517172395e-05\n",
            "train loss:0.0010310403996491018\n",
            "train loss:9.058387927403217e-05\n",
            "train loss:0.009185576688232808\n",
            "train loss:0.001050039710173289\n",
            "train loss:0.0020763867252360188\n",
            "train loss:0.0011566374751673\n",
            "train loss:0.0002510161483913684\n",
            "train loss:0.0004555766732092862\n",
            "train loss:0.00041382809650369396\n",
            "train loss:0.0024142113061629295\n",
            "train loss:0.007465462049913172\n",
            "train loss:0.0021609306841657836\n",
            "train loss:0.0005574974707742746\n",
            "train loss:0.0008782005786292251\n",
            "train loss:0.00010466735014885505\n",
            "train loss:0.0002705369942373188\n",
            "train loss:0.002849659636511358\n",
            "train loss:0.0005650180994427495\n",
            "train loss:0.0011882649849379546\n",
            "train loss:9.38097170306248e-05\n",
            "train loss:0.00045465319176250617\n",
            "train loss:0.0020205393215970645\n",
            "train loss:0.0016014101938647217\n",
            "train loss:0.0005140295719734593\n",
            "train loss:0.00029777980470349006\n",
            "train loss:0.0013106632870573257\n",
            "train loss:0.00022547688066206476\n",
            "train loss:0.0003253984191727445\n",
            "train loss:0.001535187539250401\n",
            "train loss:0.0017549723340297536\n",
            "train loss:0.0017160498132685047\n",
            "train loss:5.5508643330457304e-05\n",
            "train loss:0.0008702645670819143\n",
            "train loss:0.0013524169219288847\n",
            "train loss:0.00991947664742577\n",
            "train loss:0.0006096462236076152\n",
            "train loss:0.0036914655396631333\n",
            "train loss:0.00022653417924320597\n",
            "train loss:0.0008327896302495194\n",
            "train loss:9.715039805311478e-05\n",
            "train loss:0.000656354442634874\n",
            "train loss:0.001642762073340168\n",
            "train loss:0.00033259142130668905\n",
            "train loss:0.0016912104669985868\n",
            "train loss:0.006497228649833305\n",
            "train loss:0.0006153106189834442\n",
            "train loss:0.00010646171664893317\n",
            "train loss:0.002051121995055537\n",
            "train loss:0.0005361729498994184\n",
            "train loss:0.0006499207693979785\n",
            "train loss:0.0006043073517836815\n",
            "train loss:0.0012521056110342652\n",
            "train loss:0.0007806313384199511\n",
            "train loss:0.0003097854861140525\n",
            "train loss:0.0005439168480725298\n",
            "train loss:0.0009390645817393782\n",
            "train loss:0.0011680687938285465\n",
            "train loss:2.5857741654848216e-05\n",
            "train loss:0.0003121817668104725\n",
            "train loss:0.00028182499001028243\n",
            "train loss:0.00012203142931776295\n",
            "train loss:0.007078252572252804\n",
            "train loss:0.00018050515835755555\n",
            "train loss:0.0016528407649592197\n",
            "train loss:0.0003938591288533764\n",
            "train loss:0.013790150408882951\n",
            "train loss:0.00026421198099412185\n",
            "train loss:0.0003010506541517213\n",
            "train loss:0.000484954439043944\n",
            "train loss:0.0005306868351375227\n",
            "train loss:0.0015963857922216568\n",
            "train loss:0.0011928738410292023\n",
            "train loss:0.001176552491628274\n",
            "train loss:0.00013002757493298952\n",
            "train loss:0.002768298893329812\n",
            "train loss:9.372403639391864e-05\n",
            "train loss:0.0002727215246123321\n",
            "train loss:0.0001764271397885487\n",
            "train loss:0.0003004908073076045\n",
            "train loss:0.009419018358629879\n",
            "train loss:0.00014351573082879184\n",
            "train loss:0.004325350704276421\n",
            "train loss:0.0007113698138801066\n",
            "train loss:0.0010362944025066475\n",
            "train loss:0.0007198658305481484\n",
            "train loss:0.00598529306052663\n",
            "train loss:0.0012185796398151193\n",
            "train loss:0.0009309614851956178\n",
            "train loss:0.0007346606724123433\n",
            "train loss:0.0010346316597354159\n",
            "train loss:0.002638349132117006\n",
            "train loss:0.00033078262413482467\n",
            "train loss:0.01609505340138887\n",
            "train loss:0.002154565954780947\n",
            "train loss:7.829119531775013e-05\n",
            "train loss:0.004915085622188575\n",
            "train loss:0.0002728464611743184\n",
            "train loss:0.0013493877270824262\n",
            "train loss:0.00043889081559816815\n",
            "train loss:0.002665437532196604\n",
            "train loss:0.0013667807314407807\n",
            "train loss:0.0025672478822819427\n",
            "=== epoch:20, train acc:0.999, test acc:0.991 ===\n",
            "train loss:0.000490727348713268\n",
            "train loss:0.000920929886968788\n",
            "train loss:0.004819170978368504\n",
            "train loss:0.004909446435150082\n",
            "train loss:0.0005781768003851091\n",
            "train loss:0.00022644085016385957\n",
            "train loss:0.0027627494261661433\n",
            "train loss:5.8667626997468206e-05\n",
            "train loss:0.001620888231725363\n",
            "train loss:0.0029877520427385573\n",
            "train loss:0.0012336757536498854\n",
            "train loss:0.0014164290293425966\n",
            "train loss:0.0007411466120066694\n",
            "train loss:0.0012050804480706369\n",
            "train loss:0.0025273965223044873\n",
            "train loss:0.001717020775796607\n",
            "train loss:0.0023638910306540434\n",
            "train loss:0.0014021585899334093\n",
            "train loss:0.008577295074373208\n",
            "train loss:0.001981436256754176\n",
            "train loss:0.00016347323739659378\n",
            "train loss:0.0006277325269312184\n",
            "train loss:0.0005375308455595762\n",
            "train loss:0.00027042069517595007\n",
            "train loss:0.0029297343084315973\n",
            "train loss:0.0009332543835678258\n",
            "train loss:0.0003333452397870977\n",
            "train loss:0.0012142441933283776\n",
            "train loss:0.0015712237556291331\n",
            "train loss:0.0032411434603582734\n",
            "train loss:0.00011115944558511524\n",
            "train loss:0.0007299542600469104\n",
            "train loss:0.0005206911308541135\n",
            "train loss:0.0008428146165854418\n",
            "train loss:0.0019997659526754924\n",
            "train loss:0.0006652115414595562\n",
            "train loss:0.0032549559147646433\n",
            "train loss:0.0003471347883562294\n",
            "train loss:0.0003471311634216212\n",
            "train loss:3.0716428761018404e-05\n",
            "train loss:0.000156752996538509\n",
            "train loss:0.0006847268980456966\n",
            "train loss:4.83036621588623e-05\n",
            "train loss:0.004203473277237527\n",
            "train loss:0.00016899415063048866\n",
            "train loss:0.00012592764726664744\n",
            "train loss:0.008164854775232586\n",
            "train loss:0.002429056931344291\n",
            "train loss:0.0006381080236041865\n",
            "train loss:0.00016046754279522535\n",
            "train loss:0.0006516720769916718\n",
            "train loss:0.00014153186740579937\n",
            "train loss:0.0006610007976941744\n",
            "train loss:0.00015206978564277272\n",
            "train loss:0.0014047187370345299\n",
            "train loss:0.0008240489800878595\n",
            "train loss:0.003738710605114822\n",
            "train loss:0.000995322725939828\n",
            "train loss:7.399685392011019e-05\n",
            "train loss:0.0012368561297751452\n",
            "train loss:0.0002686489696331363\n",
            "train loss:0.0002433171358862702\n",
            "train loss:0.0005819227353225807\n",
            "train loss:0.003000774452189766\n",
            "train loss:0.005860108227820197\n",
            "train loss:0.0011701019596063711\n",
            "train loss:0.0017656821252662643\n",
            "train loss:0.002287148174024814\n",
            "train loss:0.003870236168273587\n",
            "train loss:0.00016537188061606367\n",
            "train loss:0.0023340537594845456\n",
            "train loss:0.0034794247680159816\n",
            "train loss:5.9063177787234926e-05\n",
            "train loss:0.001708332962832392\n",
            "train loss:0.0009451919680377541\n",
            "train loss:0.023980402562701068\n",
            "train loss:0.00046624044513872897\n",
            "train loss:0.0002472063961889973\n",
            "train loss:0.001799474187444884\n",
            "train loss:0.0004579314331738666\n",
            "train loss:0.0005964992798153658\n",
            "train loss:0.0003843115953931423\n",
            "train loss:2.6681980652982204e-05\n",
            "train loss:0.00010592622189586263\n",
            "train loss:0.0001694260291077354\n",
            "train loss:0.0005261398602284891\n",
            "train loss:0.003009345551223303\n",
            "train loss:0.0015689901385035393\n",
            "train loss:0.0013088841034307036\n",
            "train loss:0.00034651541490207557\n",
            "train loss:0.001115316968759403\n",
            "train loss:4.997616415135291e-05\n",
            "train loss:0.00028486793283779026\n",
            "train loss:0.0014688769556746217\n",
            "train loss:0.0019187637602397032\n",
            "train loss:0.002101978790115858\n",
            "train loss:0.00033706375518204143\n",
            "train loss:9.5044373963647e-05\n",
            "train loss:0.00751334872597392\n",
            "train loss:2.2673517748341034e-05\n",
            "train loss:1.9626047412267595e-05\n",
            "train loss:0.0016252876745858052\n",
            "train loss:0.00025531149289277584\n",
            "train loss:3.3337080119339016e-05\n",
            "train loss:0.00020250888211317036\n",
            "train loss:0.0008010265049656747\n",
            "train loss:0.001356665687635594\n",
            "train loss:0.0023007851856759877\n",
            "train loss:0.0015893637469199234\n",
            "train loss:0.0009005381255953538\n",
            "train loss:0.004215578693726487\n",
            "train loss:6.25449407165929e-05\n",
            "train loss:0.00447548963955333\n",
            "train loss:0.00012791369053328296\n",
            "train loss:0.0004788182554620282\n",
            "train loss:0.010161196196905977\n",
            "train loss:0.0010974750139245675\n",
            "train loss:0.0009381942471850104\n",
            "train loss:0.0003415019221989992\n",
            "train loss:0.007386427076505256\n",
            "train loss:0.0002140938013835205\n",
            "train loss:0.00018708835886439103\n",
            "train loss:0.000802376761342786\n",
            "train loss:0.0005703626185639604\n",
            "train loss:0.0021019905203040635\n",
            "train loss:0.00031086784963085123\n",
            "train loss:0.00016742264279510387\n",
            "train loss:0.00012047448519997773\n",
            "train loss:0.00018965664765771103\n",
            "train loss:0.0004166783285591228\n",
            "train loss:0.002343431919853252\n",
            "train loss:0.0004260913457511501\n",
            "train loss:0.00215662964022961\n",
            "train loss:0.00033997667454505054\n",
            "train loss:7.335340821282137e-05\n",
            "train loss:9.942834990106045e-05\n",
            "train loss:0.00010564470428522865\n",
            "train loss:0.0005785362923096447\n",
            "train loss:5.387506031793453e-05\n",
            "train loss:0.00012950385108440632\n",
            "train loss:0.00043277943637918517\n",
            "train loss:0.001025340520307012\n",
            "train loss:0.0013303694122259848\n",
            "train loss:0.0011821772296690429\n",
            "train loss:0.0007868697972682747\n",
            "train loss:0.005433637985724758\n",
            "train loss:9.946724718285512e-05\n",
            "train loss:0.00032603079619516134\n",
            "train loss:0.0013021084176606302\n",
            "train loss:0.0009581019197418734\n",
            "train loss:0.0015235178280576251\n",
            "train loss:0.0001264236982481192\n",
            "train loss:0.003833665809065119\n",
            "train loss:0.001096313854166071\n",
            "train loss:0.000635462602589615\n",
            "train loss:0.0008158323844657542\n",
            "train loss:0.026643056216065383\n",
            "train loss:0.0004830418291356768\n",
            "train loss:5.741026679970508e-05\n",
            "train loss:0.00015331704228760818\n",
            "train loss:0.0010235303913557418\n",
            "train loss:0.0010796411157544432\n",
            "train loss:0.0053213170585392836\n",
            "train loss:0.0007365683316210476\n",
            "train loss:0.004166742022180331\n",
            "train loss:0.00073096415224458\n",
            "train loss:0.00015297132037203628\n",
            "train loss:0.0017630870295214176\n",
            "train loss:0.0016392030980596473\n",
            "train loss:0.000518916068854441\n",
            "train loss:0.00015921860210104465\n",
            "train loss:0.018409250660541564\n",
            "train loss:3.238326179020943e-05\n",
            "train loss:0.0012025552666280349\n",
            "train loss:0.001992813671813068\n",
            "train loss:0.0034318538450556226\n",
            "train loss:0.0003318154170960209\n",
            "train loss:0.0032280821229590923\n",
            "train loss:0.00038114648696021414\n",
            "train loss:0.0013917972318111538\n",
            "train loss:0.0022268127491326204\n",
            "train loss:0.0012957760452852653\n",
            "train loss:0.00018712536844956037\n",
            "train loss:0.0016620692181619418\n",
            "train loss:0.0013185457639044676\n",
            "train loss:0.00572304997138634\n",
            "train loss:0.0008702782978726334\n",
            "train loss:0.0013156246358553994\n",
            "train loss:0.00199803094237053\n",
            "train loss:0.0005996969115279998\n",
            "train loss:0.0028339232753304775\n",
            "train loss:0.0004530830772824654\n",
            "train loss:0.0008508470902593242\n",
            "train loss:0.0005770902927790301\n",
            "train loss:0.003062296695536709\n",
            "train loss:0.0007731876168533863\n",
            "train loss:0.00019486285057041685\n",
            "train loss:0.0001062108465266016\n",
            "train loss:0.0005925725837061325\n",
            "train loss:0.0035830445457441453\n",
            "train loss:0.0004982326735871914\n",
            "train loss:0.0004590000440199596\n",
            "train loss:0.002550090887266275\n",
            "train loss:0.0011608244971514914\n",
            "train loss:0.002713855267527557\n",
            "train loss:0.004759737555465514\n",
            "train loss:0.01085707390366504\n",
            "train loss:0.0005312307994666453\n",
            "train loss:0.00011339765359867954\n",
            "train loss:0.0010257245183773384\n",
            "train loss:0.0014431095401767696\n",
            "train loss:0.0033743988047265604\n",
            "train loss:0.0001270054578274334\n",
            "train loss:0.004249438251842268\n",
            "train loss:0.001527384595122304\n",
            "train loss:0.0005600966740433429\n",
            "train loss:0.00017846591896410026\n",
            "train loss:7.592999947893754e-05\n",
            "train loss:0.0029927737298091352\n",
            "train loss:0.0011649518507590067\n",
            "train loss:0.001939017567846873\n",
            "train loss:0.0005482407871054105\n",
            "train loss:8.165150356800307e-05\n",
            "train loss:0.00043391112936527515\n",
            "train loss:8.301613468484064e-05\n",
            "train loss:0.02373885143007436\n",
            "train loss:0.000498332295819396\n",
            "train loss:0.00097236211520705\n",
            "train loss:0.001888832579212339\n",
            "train loss:0.002017663925256838\n",
            "train loss:0.0002521472266897052\n",
            "train loss:0.00040297200155813673\n",
            "train loss:0.00027515433059533236\n",
            "train loss:0.0004135390893111831\n",
            "train loss:0.005180858806192842\n",
            "train loss:0.002944327789060015\n",
            "train loss:1.2711356262094973e-05\n",
            "train loss:0.0016122465492713566\n",
            "train loss:0.00014418012016843035\n",
            "train loss:0.0002804670955082177\n",
            "train loss:5.501500002240838e-05\n",
            "train loss:0.0011939819780517641\n",
            "train loss:0.004765969642392467\n",
            "train loss:0.00017204417007169422\n",
            "train loss:6.307812239716762e-05\n",
            "train loss:0.0002472882856973427\n",
            "train loss:1.7774189534117567e-05\n",
            "train loss:0.0046831711064288465\n",
            "train loss:0.00037528631838035296\n",
            "train loss:0.00016587878659099885\n",
            "train loss:0.00027525305751830006\n",
            "train loss:0.0016624407269502798\n",
            "train loss:0.0010665315266605018\n",
            "train loss:0.0037667435054962216\n",
            "train loss:3.559738217485842e-05\n",
            "train loss:0.0008906113820107582\n",
            "train loss:0.00047698851048008065\n",
            "train loss:0.000868776855244194\n",
            "train loss:0.0011004456612818108\n",
            "train loss:0.0010668726606398679\n",
            "train loss:0.00052309503048684\n",
            "train loss:0.002377398980425635\n",
            "train loss:0.002595888360010544\n",
            "train loss:0.0075629460083425385\n",
            "train loss:0.005378002200226283\n",
            "train loss:0.0006106352734742826\n",
            "train loss:0.001747729180420854\n",
            "train loss:0.0030477308599766643\n",
            "train loss:0.002163628710031452\n",
            "train loss:0.0007555221451291891\n",
            "train loss:0.002882481783681969\n",
            "train loss:0.00013735661909210412\n",
            "train loss:0.0049721544868367665\n",
            "train loss:0.00021309649488759967\n",
            "train loss:0.0012025809361107449\n",
            "train loss:0.001539657705591116\n",
            "train loss:0.000527866016512913\n",
            "train loss:0.0011602432522899253\n",
            "train loss:0.0015867905879867292\n",
            "train loss:0.00010725091191858142\n",
            "train loss:0.0002344373677837008\n",
            "train loss:0.0013672021852703636\n",
            "train loss:0.00038282248495796506\n",
            "train loss:0.002260396551834339\n",
            "train loss:0.03476454576706877\n",
            "train loss:2.826529844142563e-05\n",
            "train loss:0.0002837185591218096\n",
            "train loss:0.0007506896798482481\n",
            "train loss:0.0003832514173611901\n",
            "train loss:0.001319951219083646\n",
            "train loss:0.0004309873262370828\n",
            "train loss:0.005623579884937982\n",
            "train loss:7.943284171071496e-05\n",
            "train loss:0.0010689872504723364\n",
            "train loss:0.00025188687055062876\n",
            "train loss:0.001110248706659516\n",
            "train loss:0.00016237407138409586\n",
            "train loss:0.00425363222830288\n",
            "train loss:0.001676576225090611\n",
            "train loss:0.0011494041861734913\n",
            "train loss:0.00023641621055174356\n",
            "train loss:0.0030895754747190273\n",
            "train loss:0.00204019810615989\n",
            "train loss:0.0010115617322345837\n",
            "train loss:0.01727741430662882\n",
            "train loss:0.0020791415656491023\n",
            "train loss:0.001860082252334362\n",
            "train loss:0.0022835520763380118\n",
            "train loss:0.00047146329965643176\n",
            "train loss:0.0017064415782438841\n",
            "train loss:0.0009760616954852174\n",
            "train loss:0.001855873179745248\n",
            "train loss:0.003085779843750457\n",
            "train loss:0.00472050654544219\n",
            "train loss:0.00036426007252129175\n",
            "train loss:0.00021265509350794142\n",
            "train loss:0.0028192574717421474\n",
            "train loss:0.00028220995742350366\n",
            "train loss:0.00037369492893243384\n",
            "train loss:0.00016938283013764839\n",
            "train loss:0.00013423475386620828\n",
            "train loss:0.0005448896692957127\n",
            "train loss:0.0005705156118830702\n",
            "train loss:0.0005611067838928934\n",
            "train loss:0.00028558620196192987\n",
            "train loss:1.723295941886468e-05\n",
            "train loss:9.88200253447072e-05\n",
            "train loss:0.0004467642187618418\n",
            "train loss:0.0005022496393687655\n",
            "train loss:0.00041521395258213795\n",
            "train loss:0.00027803367709246804\n",
            "train loss:0.0008985087449351019\n",
            "train loss:0.0023123033566330793\n",
            "train loss:0.010011864024119\n",
            "train loss:0.002517169985993426\n",
            "train loss:0.0008901552598476287\n",
            "train loss:8.507916161153064e-05\n",
            "train loss:0.0008346362774810119\n",
            "train loss:0.0007874580038388215\n",
            "train loss:0.0002851444781927663\n",
            "train loss:0.002175012994980742\n",
            "train loss:0.0009101656727151852\n",
            "train loss:0.0028819228532265396\n",
            "train loss:0.0003173088472394371\n",
            "train loss:0.0033099874594472996\n",
            "train loss:0.00038074217574171425\n",
            "train loss:3.2303773872745195e-05\n",
            "train loss:0.00044177072859103307\n",
            "train loss:0.0007031836913714389\n",
            "train loss:0.0015065352146258304\n",
            "train loss:0.004696773313049631\n",
            "train loss:0.0003219940970761007\n",
            "train loss:0.0076747105654856014\n",
            "train loss:0.00016214766023135306\n",
            "train loss:0.009721352787785372\n",
            "train loss:0.00020194403524123435\n",
            "train loss:0.002663869176375266\n",
            "train loss:0.001613008759480565\n",
            "train loss:8.942974515671342e-05\n",
            "train loss:0.002052805359798525\n",
            "train loss:0.00016750035067512154\n",
            "train loss:0.0026638752733692283\n",
            "train loss:0.0004102712865446791\n",
            "train loss:0.0011001195467604406\n",
            "train loss:1.636093700570781e-05\n",
            "train loss:0.00022839783225041068\n",
            "train loss:0.0013658188047890286\n",
            "train loss:0.0007622863504736112\n",
            "train loss:0.0005284325446303635\n",
            "train loss:0.0024760662870706132\n",
            "train loss:0.0016980171349945824\n",
            "train loss:0.010990606552413183\n",
            "train loss:0.0008500181982698428\n",
            "train loss:0.0013075065608201813\n",
            "train loss:0.0010059245549684402\n",
            "train loss:0.0006509784516973599\n",
            "train loss:0.0054899784727010835\n",
            "train loss:0.0011349063976635555\n",
            "train loss:0.0013990771353945388\n",
            "train loss:0.0013416545096194513\n",
            "train loss:0.000205806981606944\n",
            "train loss:0.001309901249798896\n",
            "train loss:4.2568282518676866e-05\n",
            "train loss:0.00012528285530769904\n",
            "train loss:0.005749723075403264\n",
            "train loss:0.002443311457098193\n",
            "train loss:0.00019085762961270597\n",
            "train loss:0.000242080398016227\n",
            "train loss:0.004493645173345628\n",
            "train loss:0.0029867890201627817\n",
            "train loss:0.0017693213614833913\n",
            "train loss:0.0001600089140279359\n",
            "train loss:0.0013331158540741662\n",
            "train loss:0.001156860477259008\n",
            "train loss:0.0013186634544947728\n",
            "train loss:0.0010281398890361413\n",
            "train loss:0.00013247442943995256\n",
            "train loss:0.00037433518302359384\n",
            "train loss:0.002385920908009541\n",
            "train loss:0.0016716777806578343\n",
            "train loss:0.004967457042084381\n",
            "train loss:0.00467846934892944\n",
            "train loss:0.00022235330753631402\n",
            "train loss:0.0010826035837637766\n",
            "train loss:0.00016548454064914275\n",
            "train loss:0.00012031928884055825\n",
            "train loss:0.0036677521918849825\n",
            "train loss:0.002279406985845953\n",
            "train loss:4.220787271593235e-05\n",
            "train loss:0.003677522316697181\n",
            "train loss:0.0012652010834088739\n",
            "train loss:0.00692673882948972\n",
            "train loss:0.00030780571615760276\n",
            "train loss:0.0013720139723043957\n",
            "train loss:0.0008258832582493763\n",
            "train loss:0.0023797750302198477\n",
            "train loss:0.0001201193222631713\n",
            "train loss:0.00023642053251230404\n",
            "train loss:0.0008533235571894901\n",
            "train loss:0.00038873969716123925\n",
            "train loss:0.002718661920932034\n",
            "train loss:0.0002811359359518836\n",
            "train loss:0.0012359979003300756\n",
            "train loss:0.0029716649637470975\n",
            "train loss:0.004273972918562186\n",
            "train loss:0.0008456749149811358\n",
            "train loss:0.0012950389056628822\n",
            "train loss:0.0008014104137082153\n",
            "train loss:0.00030727046413359994\n",
            "train loss:0.000541244982992286\n",
            "train loss:7.222586029302925e-05\n",
            "train loss:0.0004513814415360157\n",
            "train loss:0.0012623552364574569\n",
            "train loss:0.002336691251251965\n",
            "train loss:0.0027046501935498175\n",
            "train loss:0.00019130148440204005\n",
            "train loss:0.0014333600771457072\n",
            "train loss:0.0015135902939451174\n",
            "train loss:0.0009152442309831105\n",
            "train loss:0.0020589244882026274\n",
            "train loss:0.0010299171436415494\n",
            "train loss:6.241254584320816e-05\n",
            "train loss:0.00011388500878150092\n",
            "train loss:0.002002432977978135\n",
            "train loss:6.692314902266901e-05\n",
            "train loss:0.0007107397755421665\n",
            "train loss:0.0158772437699083\n",
            "train loss:0.002643680993428382\n",
            "train loss:0.00017040100378028846\n",
            "train loss:0.000558697981872359\n",
            "train loss:0.0005854625982195835\n",
            "train loss:0.001960149762334741\n",
            "train loss:0.0027432180552914125\n",
            "train loss:0.0003917949121368798\n",
            "train loss:0.0010583973296617872\n",
            "train loss:0.00039172843370637885\n",
            "train loss:0.0012563992867263454\n",
            "train loss:0.003357537062135621\n",
            "train loss:0.00021879952417860633\n",
            "train loss:0.0021146401457222703\n",
            "train loss:0.0022673468240404786\n",
            "train loss:0.0004072599269212145\n",
            "train loss:7.682244486220938e-05\n",
            "train loss:0.00032533693808043155\n",
            "train loss:0.002807159700154311\n",
            "train loss:0.0007114350137007171\n",
            "train loss:9.474586525745099e-05\n",
            "train loss:0.0012358480520018374\n",
            "train loss:0.001679775901267341\n",
            "train loss:0.00020735031426712098\n",
            "train loss:0.000571063358867771\n",
            "train loss:0.0005550220157047972\n",
            "train loss:0.0016714179212793635\n",
            "train loss:0.0025483743892250692\n",
            "train loss:0.0013420429386352991\n",
            "train loss:1.4122243361637306e-05\n",
            "train loss:6.73536171682478e-05\n",
            "train loss:0.0001585437380780947\n",
            "train loss:0.002820645364008992\n",
            "train loss:0.0009549290074192415\n",
            "train loss:0.000987982678374255\n",
            "train loss:0.0031976518872168826\n",
            "train loss:0.0001793466793149142\n",
            "train loss:0.0020705240037501528\n",
            "train loss:0.005694517716232047\n",
            "train loss:5.640256896681489e-05\n",
            "train loss:0.0012873606393854484\n",
            "train loss:0.0012857606383249892\n",
            "train loss:0.0006595982590814407\n",
            "train loss:0.004058115574177796\n",
            "train loss:0.006632288952584623\n",
            "train loss:0.0007726903642940149\n",
            "train loss:0.0038160027985383394\n",
            "train loss:0.00019053940822057744\n",
            "train loss:8.236198137786488e-05\n",
            "train loss:0.005207605102203282\n",
            "train loss:0.0015066044675980093\n",
            "train loss:0.0013253005393509553\n",
            "train loss:2.7574130441710687e-05\n",
            "train loss:0.0004108478537142663\n",
            "train loss:0.0025522417850808955\n",
            "train loss:0.0012714222388022502\n",
            "train loss:0.0008492202508063019\n",
            "train loss:0.0002803862765539993\n",
            "train loss:0.002142907313304447\n",
            "train loss:0.00021279096196975773\n",
            "train loss:0.00025154044628075526\n",
            "train loss:0.0009204694449562601\n",
            "train loss:0.001323594476327218\n",
            "train loss:0.00023546496376182442\n",
            "train loss:0.0013445693363543776\n",
            "train loss:0.0005694340682798799\n",
            "train loss:0.00027505796386100977\n",
            "train loss:0.0013924540893686683\n",
            "train loss:0.0005169373839006386\n",
            "train loss:4.2129362598293645e-05\n",
            "train loss:0.0005253706390345362\n",
            "train loss:0.00025999560794342987\n",
            "train loss:0.00031009286476966753\n",
            "train loss:0.0027556088957657347\n",
            "train loss:0.002007263529252011\n",
            "train loss:0.0005616680757073006\n",
            "train loss:0.001436864216300886\n",
            "train loss:0.0012657908254670358\n",
            "train loss:0.0008283187532399664\n",
            "train loss:0.0004955350748159794\n",
            "train loss:0.00046155722709661796\n",
            "train loss:0.0007375768601342993\n",
            "train loss:0.00036321121149513906\n",
            "train loss:0.0012417967988348432\n",
            "train loss:0.002908462704189654\n",
            "train loss:0.0002407257170915489\n",
            "train loss:0.0007388740312906934\n",
            "train loss:0.0004439526252414177\n",
            "train loss:0.00039137568267817306\n",
            "train loss:0.001391755872471154\n",
            "train loss:0.0008455375468376401\n",
            "train loss:0.0008118419345722634\n",
            "train loss:6.811718795048119e-05\n",
            "train loss:0.0001690869911751703\n",
            "train loss:0.00035154593346947726\n",
            "train loss:0.0017567663308749133\n",
            "train loss:0.00014572458948692178\n",
            "train loss:0.00021773903759624222\n",
            "train loss:0.018509059662027712\n",
            "train loss:0.00016591431507981467\n",
            "train loss:0.001655698631232132\n",
            "train loss:0.00033944887932939706\n",
            "train loss:0.00854813672830538\n",
            "train loss:0.00023948724711659008\n",
            "train loss:0.00013553164061914284\n",
            "train loss:0.0013420205797246146\n",
            "train loss:3.938396673592211e-05\n",
            "train loss:0.0004393429094445852\n",
            "train loss:0.003135369314929792\n",
            "train loss:0.0011215708966872504\n",
            "train loss:0.0007018886496664812\n",
            "train loss:0.00010297610050248619\n",
            "train loss:1.5802966792746878e-05\n",
            "train loss:0.0023685986582459625\n",
            "train loss:0.0008075105914285538\n",
            "train loss:0.0008318983560222301\n",
            "train loss:2.2012550148545035e-05\n",
            "train loss:0.0018872099146566921\n",
            "train loss:0.0012190682917826367\n",
            "train loss:0.00022355161318391746\n",
            "train loss:0.0017850751411687443\n",
            "train loss:0.0003797341038254\n",
            "train loss:0.00016607452262143565\n",
            "train loss:0.0017170364127417385\n",
            "train loss:0.00018402367857869395\n",
            "train loss:9.91790098560619e-05\n",
            "train loss:0.0017596741392974019\n",
            "train loss:3.9848025406231435e-05\n",
            "train loss:0.002270149264303244\n",
            "train loss:0.0008250422027716775\n",
            "train loss:0.00010744215334091317\n",
            "train loss:9.182742253529279e-05\n",
            "train loss:0.0011436735694599451\n",
            "train loss:7.690883338753265e-05\n",
            "train loss:0.00010802774111510646\n",
            "train loss:0.0003778173918268493\n",
            "train loss:0.002250333920490786\n",
            "train loss:0.0005645136301876392\n",
            "train loss:0.003007332768044774\n",
            "train loss:0.0001335425839835351\n",
            "train loss:0.0008004848231807785\n",
            "train loss:8.947143773180543e-05\n",
            "train loss:0.003131127608236586\n",
            "train loss:0.0009695569741210188\n",
            "train loss:0.0011478736172991022\n",
            "train loss:0.0018132298029458272\n",
            "train loss:0.0009036465692148924\n",
            "train loss:7.619389389115258e-05\n",
            "train loss:0.0017307101595520524\n",
            "train loss:0.0009611990904025037\n",
            "train loss:0.0002901504145765121\n",
            "train loss:0.0030600205246732615\n",
            "train loss:2.1619318002234155e-05\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9879\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from mnist import load_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100, # 배치 크기 100개\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1번째 층의 가중치 시각화하기"
      ],
      "metadata": {
        "id": "ZKNUglxWCtNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ccUIjvrQKL7-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "919a8ca4-74fd-4e98-a3a2-be894877ca2d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXqO39q1IoJadzeNkRHAQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}